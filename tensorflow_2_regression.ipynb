{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression example using TensorFlow 2.0\n",
    "Auto MPG regression task (target attribute is 'mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current version of tensorflow: 2.0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCurrent version of tensorflow: {0}\\n\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"auto_mpg-processed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get dimension of dataset-\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in dataset-\n",
    "data.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values-\n",
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin</th>\n",
       "      <th>car_name_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>4341.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "0  15.0        8.0         350.0       165.0  3693.0          11.5   \n",
       "1  18.0        8.0         318.0       150.0  3436.0          11.0   \n",
       "2  16.0        8.0         304.0       150.0  3433.0          12.0   \n",
       "3  17.0        8.0         302.0       140.0  3449.0          10.5   \n",
       "4  15.0        8.0         429.0       198.0  4341.0          10.0   \n",
       "\n",
       "   model_year  origin  car_name_encoded  \n",
       "0        70.0     1.0              36.0  \n",
       "1        70.0     1.0             231.0  \n",
       "2        70.0     1.0              14.0  \n",
       "3        70.0     1.0             161.0  \n",
       "4        70.0     1.0             141.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print first 8 data points of dataset-\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'car_name_encoded' attribute-\n",
    "data.drop('car_name_encoded', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data normalization for numeric attributes-\n",
    "\n",
    "# Get names of numeric attributes-\n",
    "cols = data.columns.tolist()\n",
    "cols.remove('origin')\n",
    "cols.remove('model_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a standard scaler-\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# Perform standard scaling for numeric attributes-\n",
    "data_scaled = std_scaler.fit_transform(data.loc[:, cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from 'numpy.ndarray' to Pandas DataFrame-\n",
    "data_scaled = pd.DataFrame(data_scaled, columns=cols)\n",
    "\n",
    "# Add  attributes to 'data_scaled'-\n",
    "data_scaled['origin'] = data['origin']\n",
    "data_scaled['model_year'] = data['model_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one hot encoding for 'origin' attribute-\n",
    "one_hot_encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 'origin' attribute-\n",
    "origin = data_scaled['origin']\n",
    "\n",
    "origin_one_hot_encoded = pd.get_dummies(data_scaled['origin'], prefix = 'origin', prefix_sep = '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete 'origin' attribute from dataset-\n",
    "data_scaled.drop('origin', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add attributes of one hot encoded attribute to 'data'-\n",
    "for attr in origin_one_hot_encoded.columns.tolist():\n",
    "    data_scaled[attr] = origin_one_hot_encoded[attr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin_1.0</th>\n",
       "      <th>origin_2.0</th>\n",
       "      <th>origin_3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.091843</td>\n",
       "      <td>1.504336</td>\n",
       "      <td>1.506627</td>\n",
       "      <td>1.590532</td>\n",
       "      <td>0.855275</td>\n",
       "      <td>-1.481575</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.707773</td>\n",
       "      <td>1.504336</td>\n",
       "      <td>1.199270</td>\n",
       "      <td>1.199049</td>\n",
       "      <td>0.551642</td>\n",
       "      <td>-1.663271</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.963820</td>\n",
       "      <td>1.504336</td>\n",
       "      <td>1.064801</td>\n",
       "      <td>1.199049</td>\n",
       "      <td>0.548098</td>\n",
       "      <td>-1.299879</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.835796</td>\n",
       "      <td>1.504336</td>\n",
       "      <td>1.045591</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.567001</td>\n",
       "      <td>-1.844967</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.091843</td>\n",
       "      <td>1.504336</td>\n",
       "      <td>2.265414</td>\n",
       "      <td>2.451796</td>\n",
       "      <td>1.620855</td>\n",
       "      <td>-2.026662</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.219867</td>\n",
       "      <td>1.504336</td>\n",
       "      <td>2.505536</td>\n",
       "      <td>3.025971</td>\n",
       "      <td>1.636214</td>\n",
       "      <td>-2.390054</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.219867</td>\n",
       "      <td>1.504336</td>\n",
       "      <td>2.371068</td>\n",
       "      <td>2.895477</td>\n",
       "      <td>1.586593</td>\n",
       "      <td>-2.571749</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.219867</td>\n",
       "      <td>1.504336</td>\n",
       "      <td>2.515141</td>\n",
       "      <td>3.156466</td>\n",
       "      <td>1.720097</td>\n",
       "      <td>-2.026662</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.091843</td>\n",
       "      <td>1.504336</td>\n",
       "      <td>1.890823</td>\n",
       "      <td>2.243005</td>\n",
       "      <td>1.040763</td>\n",
       "      <td>-2.571749</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.091843</td>\n",
       "      <td>1.504336</td>\n",
       "      <td>1.823588</td>\n",
       "      <td>1.721027</td>\n",
       "      <td>0.701686</td>\n",
       "      <td>-2.026662</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mpg  cylinders  displacement  horsepower    weight  acceleration  \\\n",
       "0 -1.091843   1.504336      1.506627    1.590532  0.855275     -1.481575   \n",
       "1 -0.707773   1.504336      1.199270    1.199049  0.551642     -1.663271   \n",
       "2 -0.963820   1.504336      1.064801    1.199049  0.548098     -1.299879   \n",
       "3 -0.835796   1.504336      1.045591    0.938060  0.567001     -1.844967   \n",
       "4 -1.091843   1.504336      2.265414    2.451796  1.620855     -2.026662   \n",
       "5 -1.219867   1.504336      2.505536    3.025971  1.636214     -2.390054   \n",
       "6 -1.219867   1.504336      2.371068    2.895477  1.586593     -2.571749   \n",
       "7 -1.219867   1.504336      2.515141    3.156466  1.720097     -2.026662   \n",
       "8 -1.091843   1.504336      1.890823    2.243005  1.040763     -2.571749   \n",
       "9 -1.091843   1.504336      1.823588    1.721027  0.701686     -2.026662   \n",
       "\n",
       "   model_year  origin_1.0  origin_2.0  origin_3.0  \n",
       "0        70.0           1           0           0  \n",
       "1        70.0           1           0           0  \n",
       "2        70.0           1           0           0  \n",
       "3        70.0           1           0           0  \n",
       "4        70.0           1           0           0  \n",
       "5        70.0           1           0           0  \n",
       "6        70.0           1           0           0  \n",
       "7        70.0           1           0           0  \n",
       "8        70.0           1           0           0  \n",
       "9        70.0           1           0           0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into features (X) and target (y)-\n",
    "X = data_scaled.drop('mpg', axis = 1)\n",
    "y = data_scaled['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and testing shapes are:\n",
      "X_train = (265, 9), y_train = (265,), X_test = (132, 9) and y_test = (132,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split features & target into training and testing sets-\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(\"\\nTraining and testing shapes are:\")\n",
    "print(\"X_train = {0}, y_train = {1}, X_test = {2} and y_test = {3}\\n\".format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make numpy.ndarray-\n",
    "X_train_n = X_train.values\n",
    "X_test_n = X_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct neural network for Supervised Regression task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    '''\n",
    "    Function to create/build the NN model which will be used for training\n",
    "    on the available dataset\n",
    "    '''\n",
    "    model = keras.Sequential([\n",
    "        # 9 neurons for 9 input plus 1 for bias-\n",
    "        layers.Dense(10, activation = 'relu', input_shape = (X_train.shape[1], )),\n",
    "        # layers.Dense(64, activation = 'relu', input_shape = (X_train.shape[1], )),\n",
    "        # layers.Dense(64, activation = 'relu'),\n",
    "        # layers.Dense(64, activation = 'relu'),\n",
    "        layers.Dense(10, activation = 'relu'),\n",
    "        layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "    # optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) \n",
    "\n",
    "    model.compile(loss = 'mse', optimizer = optimizer,\n",
    "            metrics = ['mae', 'mse'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model-\n",
    "nn_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# To get information/summary about the constructed NN model-\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check- See if the constructed model works on a toy data sample-\n",
    "# data sample size of 25 examples-\n",
    "example_batch = X_train_n[:25]\n",
    "example_result = nn_model.predict(example_batch)\n",
    "\n",
    "example_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'patience' parameter is the number of epochs to check for improvement-\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 265 samples, validate on 132 samples\n",
      "Epoch 1/1000\n",
      "265/265 [==============================] - 1s 4ms/sample - loss: 448.3530 - mae: 21.1319 - mse: 448.3530 - val_loss: 360.3085 - val_mae: 18.9630 - val_mse: 360.3085\n",
      "Epoch 2/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 304.0106 - mae: 17.3915 - mse: 304.0107 - val_loss: 238.0701 - val_mae: 15.4076 - val_mse: 238.0701\n",
      "Epoch 3/1000\n",
      "265/265 [==============================] - 0s 197us/sample - loss: 196.8171 - mae: 13.9743 - mse: 196.8171 - val_loss: 143.6419 - val_mae: 11.9552 - val_mse: 143.6418\n",
      "Epoch 4/1000\n",
      "265/265 [==============================] - 0s 189us/sample - loss: 108.2794 - mae: 10.3191 - mse: 108.2794 - val_loss: 65.8268 - val_mae: 8.0642 - val_mse: 65.8268\n",
      "Epoch 5/1000\n",
      "265/265 [==============================] - 0s 195us/sample - loss: 45.0813 - mae: 6.5822 - mse: 45.0813 - val_loss: 22.0408 - val_mae: 4.5977 - val_mse: 22.0408\n",
      "Epoch 6/1000\n",
      "265/265 [==============================] - 0s 194us/sample - loss: 13.3746 - mae: 3.4465 - mse: 13.3746 - val_loss: 4.7855 - val_mae: 1.9754 - val_mse: 4.7855\n",
      "Epoch 7/1000\n",
      "265/265 [==============================] - 0s 254us/sample - loss: 2.7277 - mae: 1.4522 - mse: 2.7277 - val_loss: 1.1345 - val_mae: 0.8932 - val_mse: 1.1345\n",
      "Epoch 8/1000\n",
      "265/265 [==============================] - 0s 188us/sample - loss: 1.1654 - mae: 0.8373 - mse: 1.1654 - val_loss: 1.3171 - val_mae: 0.8804 - val_mse: 1.3171\n",
      "Epoch 9/1000\n",
      "265/265 [==============================] - 0s 212us/sample - loss: 1.4239 - mae: 0.8775 - mse: 1.4239 - val_loss: 1.3978 - val_mae: 0.8964 - val_mse: 1.3978\n",
      "Epoch 10/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 1.3424 - mae: 0.8438 - mse: 1.3424 - val_loss: 1.1448 - val_mae: 0.8145 - val_mse: 1.1448\n",
      "Epoch 11/1000\n",
      "265/265 [==============================] - 0s 205us/sample - loss: 1.0798 - mae: 0.7618 - mse: 1.0798 - val_loss: 0.9141 - val_mae: 0.7468 - val_mse: 0.9141\n",
      "Epoch 12/1000\n",
      "265/265 [==============================] - 0s 199us/sample - loss: 0.9125 - mae: 0.7329 - mse: 0.9125 - val_loss: 0.8180 - val_mae: 0.7319 - val_mse: 0.8180\n",
      "Epoch 13/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.8323 - mae: 0.7282 - mse: 0.8323 - val_loss: 0.7741 - val_mae: 0.7258 - val_mse: 0.7741\n",
      "Epoch 14/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.7900 - mae: 0.7195 - mse: 0.7900 - val_loss: 0.7354 - val_mae: 0.7127 - val_mse: 0.7354\n",
      "Epoch 15/1000\n",
      "265/265 [==============================] - 0s 236us/sample - loss: 0.7491 - mae: 0.7037 - mse: 0.7491 - val_loss: 0.6971 - val_mae: 0.6948 - val_mse: 0.6971\n",
      "Epoch 16/1000\n",
      "265/265 [==============================] - 0s 192us/sample - loss: 0.7106 - mae: 0.6809 - mse: 0.7106 - val_loss: 0.6583 - val_mae: 0.6700 - val_mse: 0.6583\n",
      "Epoch 17/1000\n",
      "265/265 [==============================] - 0s 223us/sample - loss: 0.6740 - mae: 0.6585 - mse: 0.6740 - val_loss: 0.6256 - val_mae: 0.6530 - val_mse: 0.6256\n",
      "Epoch 18/1000\n",
      "265/265 [==============================] - 0s 206us/sample - loss: 0.6423 - mae: 0.6377 - mse: 0.6423 - val_loss: 0.5944 - val_mae: 0.6320 - val_mse: 0.5944\n",
      "Epoch 19/1000\n",
      "265/265 [==============================] - 0s 182us/sample - loss: 0.6134 - mae: 0.6179 - mse: 0.6134 - val_loss: 0.5689 - val_mae: 0.6149 - val_mse: 0.5689\n",
      "Epoch 20/1000\n",
      "265/265 [==============================] - 0s 187us/sample - loss: 0.5882 - mae: 0.6008 - mse: 0.5882 - val_loss: 0.5456 - val_mae: 0.5975 - val_mse: 0.5456\n",
      "Epoch 21/1000\n",
      "265/265 [==============================] - 0s 183us/sample - loss: 0.5678 - mae: 0.5828 - mse: 0.5678 - val_loss: 0.5253 - val_mae: 0.5789 - val_mse: 0.5253\n",
      "Epoch 22/1000\n",
      "265/265 [==============================] - 0s 178us/sample - loss: 0.5475 - mae: 0.5698 - mse: 0.5475 - val_loss: 0.5076 - val_mae: 0.5712 - val_mse: 0.5076\n",
      "Epoch 23/1000\n",
      "265/265 [==============================] - 0s 188us/sample - loss: 0.5293 - mae: 0.5614 - mse: 0.5293 - val_loss: 0.4914 - val_mae: 0.5620 - val_mse: 0.4914\n",
      "Epoch 24/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.5133 - mae: 0.5517 - mse: 0.5133 - val_loss: 0.4775 - val_mae: 0.5530 - val_mse: 0.4775\n",
      "Epoch 25/1000\n",
      "265/265 [==============================] - 0s 191us/sample - loss: 0.5020 - mae: 0.5505 - mse: 0.5020 - val_loss: 0.4683 - val_mae: 0.5527 - val_mse: 0.4683\n",
      "Epoch 26/1000\n",
      "265/265 [==============================] - 0s 190us/sample - loss: 0.4883 - mae: 0.5420 - mse: 0.4883 - val_loss: 0.4553 - val_mae: 0.5399 - val_mse: 0.4553\n",
      "Epoch 27/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.4773 - mae: 0.5294 - mse: 0.4773 - val_loss: 0.4438 - val_mae: 0.5267 - val_mse: 0.4438\n",
      "Epoch 28/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.4697 - mae: 0.5199 - mse: 0.4697 - val_loss: 0.4344 - val_mae: 0.5173 - val_mse: 0.4344\n",
      "Epoch 29/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.4611 - mae: 0.5116 - mse: 0.4611 - val_loss: 0.4264 - val_mae: 0.5105 - val_mse: 0.4264\n",
      "Epoch 30/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.4536 - mae: 0.5093 - mse: 0.4536 - val_loss: 0.4217 - val_mae: 0.5122 - val_mse: 0.4217\n",
      "Epoch 31/1000\n",
      "265/265 [==============================] - 0s 178us/sample - loss: 0.4456 - mae: 0.5080 - mse: 0.4456 - val_loss: 0.4176 - val_mae: 0.5112 - val_mse: 0.4176\n",
      "Epoch 32/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.4409 - mae: 0.5072 - mse: 0.4409 - val_loss: 0.4130 - val_mae: 0.5081 - val_mse: 0.4130\n",
      "Epoch 33/1000\n",
      "265/265 [==============================] - 0s 176us/sample - loss: 0.4372 - mae: 0.5067 - mse: 0.4372 - val_loss: 0.4093 - val_mae: 0.5055 - val_mse: 0.4093\n",
      "Epoch 34/1000\n",
      "265/265 [==============================] - 0s 177us/sample - loss: 0.4295 - mae: 0.4967 - mse: 0.4295 - val_loss: 0.4008 - val_mae: 0.4908 - val_mse: 0.4008\n",
      "Epoch 35/1000\n",
      "265/265 [==============================] - 0s 178us/sample - loss: 0.4280 - mae: 0.4880 - mse: 0.4280 - val_loss: 0.3962 - val_mae: 0.4840 - val_mse: 0.3962\n",
      "Epoch 36/1000\n",
      "265/265 [==============================] - 0s 189us/sample - loss: 0.4241 - mae: 0.4858 - mse: 0.4241 - val_loss: 0.3947 - val_mae: 0.4861 - val_mse: 0.3947\n",
      "Epoch 37/1000\n",
      "265/265 [==============================] - 0s 199us/sample - loss: 0.4207 - mae: 0.4903 - mse: 0.4207 - val_loss: 0.3955 - val_mae: 0.4911 - val_mse: 0.3955\n",
      "Epoch 38/1000\n",
      "265/265 [==============================] - 0s 179us/sample - loss: 0.4178 - mae: 0.4913 - mse: 0.4178 - val_loss: 0.3933 - val_mae: 0.4895 - val_mse: 0.3933\n",
      "Epoch 39/1000\n",
      "265/265 [==============================] - 0s 178us/sample - loss: 0.4139 - mae: 0.4879 - mse: 0.4139 - val_loss: 0.3876 - val_mae: 0.4809 - val_mse: 0.3876\n",
      "Epoch 40/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.4120 - mae: 0.4831 - mse: 0.4120 - val_loss: 0.3848 - val_mae: 0.4774 - val_mse: 0.3848\n",
      "Epoch 41/1000\n",
      "265/265 [==============================] - 0s 193us/sample - loss: 0.4100 - mae: 0.4840 - mse: 0.4100 - val_loss: 0.3856 - val_mae: 0.4804 - val_mse: 0.3856\n",
      "Epoch 42/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.4076 - mae: 0.4831 - mse: 0.4076 - val_loss: 0.3831 - val_mae: 0.4778 - val_mse: 0.3831\n",
      "Epoch 43/1000\n",
      "265/265 [==============================] - 0s 178us/sample - loss: 0.4060 - mae: 0.4811 - mse: 0.4060 - val_loss: 0.3813 - val_mae: 0.4764 - val_mse: 0.3813\n",
      "Epoch 44/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.4050 - mae: 0.4836 - mse: 0.4050 - val_loss: 0.3841 - val_mae: 0.4820 - val_mse: 0.3841\n",
      "Epoch 45/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.4025 - mae: 0.4830 - mse: 0.4025 - val_loss: 0.3793 - val_mae: 0.4756 - val_mse: 0.3793\n",
      "Epoch 46/1000\n",
      "265/265 [==============================] - 0s 222us/sample - loss: 0.4010 - mae: 0.4809 - mse: 0.4010 - val_loss: 0.3787 - val_mae: 0.4759 - val_mse: 0.3787\n",
      "Epoch 47/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.3998 - mae: 0.4797 - mse: 0.3998 - val_loss: 0.3750 - val_mae: 0.4713 - val_mse: 0.3750\n",
      "Epoch 48/1000\n",
      "265/265 [==============================] - 0s 185us/sample - loss: 0.3980 - mae: 0.4788 - mse: 0.3980 - val_loss: 0.3732 - val_mae: 0.4697 - val_mse: 0.3732\n",
      "Epoch 49/1000\n",
      "265/265 [==============================] - 0s 197us/sample - loss: 0.3967 - mae: 0.4758 - mse: 0.3967 - val_loss: 0.3701 - val_mae: 0.4656 - val_mse: 0.3701\n",
      "Epoch 50/1000\n",
      "265/265 [==============================] - 0s 185us/sample - loss: 0.3953 - mae: 0.4761 - mse: 0.3953 - val_loss: 0.3743 - val_mae: 0.4726 - val_mse: 0.3743\n",
      "Epoch 51/1000\n",
      "265/265 [==============================] - 0s 179us/sample - loss: 0.3931 - mae: 0.4785 - mse: 0.3931 - val_loss: 0.3738 - val_mae: 0.4724 - val_mse: 0.3738\n",
      "Epoch 52/1000\n",
      "265/265 [==============================] - 0s 184us/sample - loss: 0.3918 - mae: 0.4783 - mse: 0.3918 - val_loss: 0.3729 - val_mae: 0.4719 - val_mse: 0.3729\n",
      "Epoch 53/1000\n",
      "265/265 [==============================] - 0s 197us/sample - loss: 0.3919 - mae: 0.4765 - mse: 0.3919 - val_loss: 0.3681 - val_mae: 0.4656 - val_mse: 0.3681\n",
      "Epoch 54/1000\n",
      "265/265 [==============================] - 0s 179us/sample - loss: 0.3892 - mae: 0.4744 - mse: 0.3892 - val_loss: 0.3673 - val_mae: 0.4653 - val_mse: 0.3673\n",
      "Epoch 55/1000\n",
      "265/265 [==============================] - 0s 191us/sample - loss: 0.3887 - mae: 0.4742 - mse: 0.3887 - val_loss: 0.3667 - val_mae: 0.4652 - val_mse: 0.3667\n",
      "Epoch 56/1000\n",
      "265/265 [==============================] - 0s 178us/sample - loss: 0.3870 - mae: 0.4749 - mse: 0.3870 - val_loss: 0.3697 - val_mae: 0.4698 - val_mse: 0.3697\n",
      "Epoch 57/1000\n",
      "265/265 [==============================] - 0s 187us/sample - loss: 0.3851 - mae: 0.4741 - mse: 0.3851 - val_loss: 0.3678 - val_mae: 0.4679 - val_mse: 0.3678\n",
      "Epoch 58/1000\n",
      "265/265 [==============================] - 0s 188us/sample - loss: 0.3841 - mae: 0.4734 - mse: 0.3841 - val_loss: 0.3660 - val_mae: 0.4661 - val_mse: 0.3660\n",
      "Epoch 59/1000\n",
      "265/265 [==============================] - 0s 170us/sample - loss: 0.3824 - mae: 0.4717 - mse: 0.3824 - val_loss: 0.3617 - val_mae: 0.4603 - val_mse: 0.3617\n",
      "Epoch 60/1000\n",
      "265/265 [==============================] - 0s 190us/sample - loss: 0.3850 - mae: 0.4691 - mse: 0.3850 - val_loss: 0.3585 - val_mae: 0.4558 - val_mse: 0.3585\n",
      "Epoch 61/1000\n",
      "265/265 [==============================] - 0s 166us/sample - loss: 0.3811 - mae: 0.4686 - mse: 0.3811 - val_loss: 0.3630 - val_mae: 0.4636 - val_mse: 0.3630\n",
      "Epoch 62/1000\n",
      "265/265 [==============================] - 0s 225us/sample - loss: 0.3784 - mae: 0.4698 - mse: 0.3784 - val_loss: 0.3655 - val_mae: 0.4672 - val_mse: 0.3655\n",
      "Epoch 63/1000\n",
      "265/265 [==============================] - 0s 182us/sample - loss: 0.3777 - mae: 0.4703 - mse: 0.3777 - val_loss: 0.3618 - val_mae: 0.4629 - val_mse: 0.3618\n",
      "Epoch 64/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 0.3763 - mae: 0.4682 - mse: 0.3763 - val_loss: 0.3594 - val_mae: 0.4600 - val_mse: 0.3594\n",
      "Epoch 65/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.3754 - mae: 0.4671 - mse: 0.3754 - val_loss: 0.3593 - val_mae: 0.4603 - val_mse: 0.3593\n",
      "Epoch 66/1000\n",
      "265/265 [==============================] - 0s 186us/sample - loss: 0.3750 - mae: 0.4661 - mse: 0.3750 - val_loss: 0.3560 - val_mae: 0.4559 - val_mse: 0.3560\n",
      "Epoch 67/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.3745 - mae: 0.4671 - mse: 0.3745 - val_loss: 0.3595 - val_mae: 0.4615 - val_mse: 0.3595\n",
      "Epoch 68/1000\n",
      "265/265 [==============================] - 0s 192us/sample - loss: 0.3719 - mae: 0.4662 - mse: 0.3719 - val_loss: 0.3583 - val_mae: 0.4602 - val_mse: 0.3583\n",
      "Epoch 69/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.3717 - mae: 0.4674 - mse: 0.3717 - val_loss: 0.3601 - val_mae: 0.4629 - val_mse: 0.3601\n",
      "Epoch 70/1000\n",
      "265/265 [==============================] - 0s 191us/sample - loss: 0.3695 - mae: 0.4642 - mse: 0.3695 - val_loss: 0.3549 - val_mae: 0.4564 - val_mse: 0.3549\n",
      "Epoch 71/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.3682 - mae: 0.4633 - mse: 0.3682 - val_loss: 0.3555 - val_mae: 0.4577 - val_mse: 0.3555\n",
      "Epoch 72/1000\n",
      "265/265 [==============================] - 0s 180us/sample - loss: 0.3679 - mae: 0.4635 - mse: 0.3679 - val_loss: 0.3549 - val_mae: 0.4573 - val_mse: 0.3549\n",
      "Epoch 73/1000\n",
      "265/265 [==============================] - 0s 246us/sample - loss: 0.3663 - mae: 0.4632 - mse: 0.3663 - val_loss: 0.3565 - val_mae: 0.4596 - val_mse: 0.3565\n",
      "Epoch 74/1000\n",
      "265/265 [==============================] - 0s 219us/sample - loss: 0.3653 - mae: 0.4629 - mse: 0.3653 - val_loss: 0.3551 - val_mae: 0.4582 - val_mse: 0.3551\n",
      "Epoch 75/1000\n",
      "265/265 [==============================] - 0s 183us/sample - loss: 0.3644 - mae: 0.4614 - mse: 0.3644 - val_loss: 0.3520 - val_mae: 0.4545 - val_mse: 0.3520\n",
      "Epoch 76/1000\n",
      "265/265 [==============================] - 0s 205us/sample - loss: 0.3636 - mae: 0.4612 - mse: 0.3636 - val_loss: 0.3513 - val_mae: 0.4538 - val_mse: 0.3513\n",
      "Epoch 77/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 0.3624 - mae: 0.4585 - mse: 0.3624 - val_loss: 0.3454 - val_mae: 0.4465 - val_mse: 0.3454\n",
      "Epoch 78/1000\n",
      "265/265 [==============================] - 0s 186us/sample - loss: 0.3629 - mae: 0.4562 - mse: 0.3629 - val_loss: 0.3450 - val_mae: 0.4463 - val_mse: 0.3450\n",
      "Epoch 79/1000\n",
      "265/265 [==============================] - 0s 181us/sample - loss: 0.3611 - mae: 0.4576 - mse: 0.3611 - val_loss: 0.3513 - val_mae: 0.4548 - val_mse: 0.3513\n",
      "Epoch 80/1000\n",
      "265/265 [==============================] - 0s 199us/sample - loss: 0.3594 - mae: 0.4573 - mse: 0.3594 - val_loss: 0.3473 - val_mae: 0.4501 - val_mse: 0.3473\n",
      "Epoch 81/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.3576 - mae: 0.4553 - mse: 0.3576 - val_loss: 0.3470 - val_mae: 0.4501 - val_mse: 0.3470\n",
      "Epoch 82/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.3572 - mae: 0.4548 - mse: 0.3572 - val_loss: 0.3462 - val_mae: 0.4494 - val_mse: 0.3462\n",
      "Epoch 83/1000\n",
      "265/265 [==============================] - 0s 191us/sample - loss: 0.3574 - mae: 0.4568 - mse: 0.3574 - val_loss: 0.3542 - val_mae: 0.4591 - val_mse: 0.3542\n",
      "Epoch 84/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.3542 - mae: 0.4552 - mse: 0.3542 - val_loss: 0.3441 - val_mae: 0.4473 - val_mse: 0.3441\n",
      "Epoch 85/1000\n",
      "265/265 [==============================] - 0s 190us/sample - loss: 0.3570 - mae: 0.4533 - mse: 0.3570 - val_loss: 0.3388 - val_mae: 0.4419 - val_mse: 0.3388\n",
      "Epoch 86/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.3536 - mae: 0.4519 - mse: 0.3536 - val_loss: 0.3529 - val_mae: 0.4579 - val_mse: 0.3529\n",
      "Epoch 87/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.3544 - mae: 0.4574 - mse: 0.3544 - val_loss: 0.3500 - val_mae: 0.4548 - val_mse: 0.3500\n",
      "Epoch 88/1000\n",
      "265/265 [==============================] - 0s 235us/sample - loss: 0.3504 - mae: 0.4513 - mse: 0.3504 - val_loss: 0.3415 - val_mae: 0.4450 - val_mse: 0.3415\n",
      "Epoch 89/1000\n",
      "265/265 [==============================] - 0s 206us/sample - loss: 0.3520 - mae: 0.4499 - mse: 0.3520 - val_loss: 0.3367 - val_mae: 0.4404 - val_mse: 0.3367\n",
      "Epoch 90/1000\n",
      "265/265 [==============================] - 0s 184us/sample - loss: 0.3495 - mae: 0.4476 - mse: 0.3495 - val_loss: 0.3398 - val_mae: 0.4433 - val_mse: 0.3398\n",
      "Epoch 91/1000\n",
      "265/265 [==============================] - 0s 219us/sample - loss: 0.3473 - mae: 0.4468 - mse: 0.3473 - val_loss: 0.3396 - val_mae: 0.4434 - val_mse: 0.3396\n",
      "Epoch 92/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.3468 - mae: 0.4469 - mse: 0.3468 - val_loss: 0.3422 - val_mae: 0.4466 - val_mse: 0.3422\n",
      "Epoch 93/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.3460 - mae: 0.4479 - mse: 0.3460 - val_loss: 0.3406 - val_mae: 0.4451 - val_mse: 0.3406\n",
      "Epoch 94/1000\n",
      "265/265 [==============================] - 0s 200us/sample - loss: 0.3450 - mae: 0.4441 - mse: 0.3450 - val_loss: 0.3336 - val_mae: 0.4376 - val_mse: 0.3336\n",
      "Epoch 95/1000\n",
      "265/265 [==============================] - 0s 235us/sample - loss: 0.3456 - mae: 0.4436 - mse: 0.3456 - val_loss: 0.3356 - val_mae: 0.4401 - val_mse: 0.3356\n",
      "Epoch 96/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.3425 - mae: 0.4428 - mse: 0.3425 - val_loss: 0.3366 - val_mae: 0.4413 - val_mse: 0.3366\n",
      "Epoch 97/1000\n",
      "265/265 [==============================] - 0s 205us/sample - loss: 0.3416 - mae: 0.4421 - mse: 0.3416 - val_loss: 0.3349 - val_mae: 0.4396 - val_mse: 0.3349\n",
      "Epoch 98/1000\n",
      "265/265 [==============================] - 0s 185us/sample - loss: 0.3413 - mae: 0.4429 - mse: 0.3413 - val_loss: 0.3386 - val_mae: 0.4433 - val_mse: 0.3386\n",
      "Epoch 99/1000\n",
      "265/265 [==============================] - 0s 184us/sample - loss: 0.3406 - mae: 0.4424 - mse: 0.3406 - val_loss: 0.3337 - val_mae: 0.4378 - val_mse: 0.3337\n",
      "Epoch 100/1000\n",
      "265/265 [==============================] - 0s 215us/sample - loss: 0.3405 - mae: 0.4439 - mse: 0.3405 - val_loss: 0.3359 - val_mae: 0.4400 - val_mse: 0.3359\n",
      "Epoch 101/1000\n",
      "265/265 [==============================] - 0s 184us/sample - loss: 0.3393 - mae: 0.4410 - mse: 0.3393 - val_loss: 0.3276 - val_mae: 0.4323 - val_mse: 0.3276\n",
      "Epoch 102/1000\n",
      "265/265 [==============================] - 0s 195us/sample - loss: 0.3375 - mae: 0.4375 - mse: 0.3375 - val_loss: 0.3277 - val_mae: 0.4326 - val_mse: 0.3277\n",
      "Epoch 103/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.3366 - mae: 0.4370 - mse: 0.3366 - val_loss: 0.3301 - val_mae: 0.4350 - val_mse: 0.3301\n",
      "Epoch 104/1000\n",
      "265/265 [==============================] - 0s 187us/sample - loss: 0.3344 - mae: 0.4381 - mse: 0.3344 - val_loss: 0.3402 - val_mae: 0.4452 - val_mse: 0.3402\n",
      "Epoch 105/1000\n",
      "265/265 [==============================] - 0s 185us/sample - loss: 0.3357 - mae: 0.4422 - mse: 0.3357 - val_loss: 0.3330 - val_mae: 0.4383 - val_mse: 0.3330\n",
      "Epoch 106/1000\n",
      "265/265 [==============================] - 0s 205us/sample - loss: 0.3338 - mae: 0.4368 - mse: 0.3338 - val_loss: 0.3261 - val_mae: 0.4316 - val_mse: 0.3261\n",
      "Epoch 107/1000\n",
      "265/265 [==============================] - 0s 193us/sample - loss: 0.3332 - mae: 0.4343 - mse: 0.3332 - val_loss: 0.3273 - val_mae: 0.4330 - val_mse: 0.3273\n",
      "Epoch 108/1000\n",
      "265/265 [==============================] - 0s 188us/sample - loss: 0.3321 - mae: 0.4316 - mse: 0.3321 - val_loss: 0.3243 - val_mae: 0.4303 - val_mse: 0.3243\n",
      "Epoch 109/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.3310 - mae: 0.4317 - mse: 0.3310 - val_loss: 0.3281 - val_mae: 0.4339 - val_mse: 0.3281\n",
      "Epoch 110/1000\n",
      "265/265 [==============================] - 0s 187us/sample - loss: 0.3304 - mae: 0.4316 - mse: 0.3304 - val_loss: 0.3244 - val_mae: 0.4309 - val_mse: 0.3244\n",
      "Epoch 111/1000\n",
      "265/265 [==============================] - 0s 185us/sample - loss: 0.3298 - mae: 0.4320 - mse: 0.3298 - val_loss: 0.3294 - val_mae: 0.4347 - val_mse: 0.3294\n",
      "Epoch 112/1000\n",
      "265/265 [==============================] - 0s 225us/sample - loss: 0.3282 - mae: 0.4306 - mse: 0.3282 - val_loss: 0.3223 - val_mae: 0.4286 - val_mse: 0.3223\n",
      "Epoch 113/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.3280 - mae: 0.4305 - mse: 0.3280 - val_loss: 0.3259 - val_mae: 0.4312 - val_mse: 0.3259\n",
      "Epoch 114/1000\n",
      "265/265 [==============================] - 0s 176us/sample - loss: 0.3262 - mae: 0.4307 - mse: 0.3262 - val_loss: 0.3221 - val_mae: 0.4277 - val_mse: 0.3221\n",
      "Epoch 115/1000\n",
      "265/265 [==============================] - 0s 225us/sample - loss: 0.3280 - mae: 0.4293 - mse: 0.3280 - val_loss: 0.3229 - val_mae: 0.4282 - val_mse: 0.3229\n",
      "Epoch 116/1000\n",
      "265/265 [==============================] - 0s 212us/sample - loss: 0.3248 - mae: 0.4288 - mse: 0.3248 - val_loss: 0.3255 - val_mae: 0.4305 - val_mse: 0.3255\n",
      "Epoch 117/1000\n",
      "265/265 [==============================] - 0s 194us/sample - loss: 0.3238 - mae: 0.4291 - mse: 0.3238 - val_loss: 0.3219 - val_mae: 0.4274 - val_mse: 0.3219\n",
      "Epoch 118/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.3231 - mae: 0.4292 - mse: 0.3231 - val_loss: 0.3247 - val_mae: 0.4296 - val_mse: 0.3247\n",
      "Epoch 119/1000\n",
      "265/265 [==============================] - 0s 196us/sample - loss: 0.3217 - mae: 0.4270 - mse: 0.3217 - val_loss: 0.3187 - val_mae: 0.4244 - val_mse: 0.3187\n",
      "Epoch 120/1000\n",
      "265/265 [==============================] - 0s 225us/sample - loss: 0.3217 - mae: 0.4236 - mse: 0.3217 - val_loss: 0.3168 - val_mae: 0.4226 - val_mse: 0.3168\n",
      "Epoch 121/1000\n",
      "265/265 [==============================] - 0s 213us/sample - loss: 0.3212 - mae: 0.4258 - mse: 0.3212 - val_loss: 0.3229 - val_mae: 0.4278 - val_mse: 0.3229\n",
      "Epoch 122/1000\n",
      "265/265 [==============================] - 0s 242us/sample - loss: 0.3205 - mae: 0.4283 - mse: 0.3205 - val_loss: 0.3213 - val_mae: 0.4264 - val_mse: 0.3213\n",
      "Epoch 123/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.3186 - mae: 0.4229 - mse: 0.3186 - val_loss: 0.3132 - val_mae: 0.4198 - val_mse: 0.3132\n",
      "Epoch 124/1000\n",
      "265/265 [==============================] - 0s 199us/sample - loss: 0.3194 - mae: 0.4216 - mse: 0.3194 - val_loss: 0.3177 - val_mae: 0.4245 - val_mse: 0.3177\n",
      "Epoch 125/1000\n",
      "265/265 [==============================] - 0s 182us/sample - loss: 0.3181 - mae: 0.4259 - mse: 0.3181 - val_loss: 0.3264 - val_mae: 0.4321 - val_mse: 0.3264\n",
      "Epoch 126/1000\n",
      "265/265 [==============================] - 0s 180us/sample - loss: 0.3184 - mae: 0.4254 - mse: 0.3184 - val_loss: 0.3170 - val_mae: 0.4234 - val_mse: 0.3170\n",
      "Epoch 127/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.3152 - mae: 0.4222 - mse: 0.3152 - val_loss: 0.3185 - val_mae: 0.4241 - val_mse: 0.3185\n",
      "Epoch 128/1000\n",
      "265/265 [==============================] - 0s 182us/sample - loss: 0.3158 - mae: 0.4216 - mse: 0.3158 - val_loss: 0.3149 - val_mae: 0.4213 - val_mse: 0.3149\n",
      "Epoch 129/1000\n",
      "265/265 [==============================] - 0s 196us/sample - loss: 0.3153 - mae: 0.4235 - mse: 0.3153 - val_loss: 0.3165 - val_mae: 0.4222 - val_mse: 0.3165\n",
      "Epoch 130/1000\n",
      "265/265 [==============================] - 0s 190us/sample - loss: 0.3127 - mae: 0.4188 - mse: 0.3127 - val_loss: 0.3093 - val_mae: 0.4157 - val_mse: 0.3093\n",
      "Epoch 131/1000\n",
      "265/265 [==============================] - 0s 195us/sample - loss: 0.3148 - mae: 0.4177 - mse: 0.3148 - val_loss: 0.3100 - val_mae: 0.4170 - val_mse: 0.3100\n",
      "Epoch 132/1000\n",
      "265/265 [==============================] - 0s 179us/sample - loss: 0.3125 - mae: 0.4170 - mse: 0.3125 - val_loss: 0.3122 - val_mae: 0.4194 - val_mse: 0.3122\n",
      "Epoch 133/1000\n",
      "265/265 [==============================] - 0s 200us/sample - loss: 0.3118 - mae: 0.4186 - mse: 0.3118 - val_loss: 0.3126 - val_mae: 0.4194 - val_mse: 0.3126\n",
      "Epoch 134/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.3117 - mae: 0.4172 - mse: 0.3117 - val_loss: 0.3108 - val_mae: 0.4179 - val_mse: 0.3108\n",
      "Epoch 135/1000\n",
      "265/265 [==============================] - 0s 172us/sample - loss: 0.3105 - mae: 0.4195 - mse: 0.3105 - val_loss: 0.3195 - val_mae: 0.4266 - val_mse: 0.3195\n",
      "Epoch 136/1000\n",
      "265/265 [==============================] - 0s 193us/sample - loss: 0.3097 - mae: 0.4212 - mse: 0.3097 - val_loss: 0.3139 - val_mae: 0.4215 - val_mse: 0.3139\n",
      "Epoch 137/1000\n",
      "265/265 [==============================] - 0s 213us/sample - loss: 0.3119 - mae: 0.4145 - mse: 0.3119 - val_loss: 0.3057 - val_mae: 0.4131 - val_mse: 0.3057\n",
      "Epoch 138/1000\n",
      "265/265 [==============================] - 0s 179us/sample - loss: 0.3083 - mae: 0.4116 - mse: 0.3083 - val_loss: 0.3119 - val_mae: 0.4196 - val_mse: 0.3119\n",
      "Epoch 139/1000\n",
      "265/265 [==============================] - 0s 166us/sample - loss: 0.3099 - mae: 0.4204 - mse: 0.3099 - val_loss: 0.3252 - val_mae: 0.4327 - val_mse: 0.3252\n",
      "Epoch 140/1000\n",
      "265/265 [==============================] - 0s 226us/sample - loss: 0.3080 - mae: 0.4200 - mse: 0.3080 - val_loss: 0.3106 - val_mae: 0.4181 - val_mse: 0.3106\n",
      "Epoch 141/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.3052 - mae: 0.4141 - mse: 0.3052 - val_loss: 0.3075 - val_mae: 0.4149 - val_mse: 0.3075\n",
      "Epoch 142/1000\n",
      "265/265 [==============================] - 0s 215us/sample - loss: 0.3049 - mae: 0.4123 - mse: 0.3049 - val_loss: 0.3064 - val_mae: 0.4137 - val_mse: 0.3064\n",
      "Epoch 143/1000\n",
      "265/265 [==============================] - 0s 194us/sample - loss: 0.3057 - mae: 0.4120 - mse: 0.3057 - val_loss: 0.3079 - val_mae: 0.4152 - val_mse: 0.3079\n",
      "Epoch 144/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.3129 - mae: 0.4254 - mse: 0.3129 - val_loss: 0.3281 - val_mae: 0.4343 - val_mse: 0.3281\n",
      "Epoch 145/1000\n",
      "265/265 [==============================] - 0s 187us/sample - loss: 0.3050 - mae: 0.4217 - mse: 0.3050 - val_loss: 0.3073 - val_mae: 0.4143 - val_mse: 0.3073\n",
      "Epoch 146/1000\n",
      "265/265 [==============================] - 0s 218us/sample - loss: 0.3046 - mae: 0.4115 - mse: 0.3046 - val_loss: 0.3045 - val_mae: 0.4116 - val_mse: 0.3045\n",
      "Epoch 147/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.3019 - mae: 0.4107 - mse: 0.3019 - val_loss: 0.3070 - val_mae: 0.4143 - val_mse: 0.3070\n",
      "Epoch 148/1000\n",
      "265/265 [==============================] - 0s 215us/sample - loss: 0.3015 - mae: 0.4129 - mse: 0.3015 - val_loss: 0.3090 - val_mae: 0.4163 - val_mse: 0.3090\n",
      "Epoch 149/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.3004 - mae: 0.4108 - mse: 0.3004 - val_loss: 0.3049 - val_mae: 0.4123 - val_mse: 0.3049\n",
      "Epoch 150/1000\n",
      "265/265 [==============================] - 0s 194us/sample - loss: 0.3002 - mae: 0.4097 - mse: 0.3002 - val_loss: 0.3050 - val_mae: 0.4127 - val_mse: 0.3050\n",
      "Epoch 151/1000\n",
      "265/265 [==============================] - 0s 190us/sample - loss: 0.2992 - mae: 0.4090 - mse: 0.2992 - val_loss: 0.3057 - val_mae: 0.4134 - val_mse: 0.3057\n",
      "Epoch 152/1000\n",
      "265/265 [==============================] - 0s 194us/sample - loss: 0.2993 - mae: 0.4074 - mse: 0.2993 - val_loss: 0.3030 - val_mae: 0.4109 - val_mse: 0.3030\n",
      "Epoch 153/1000\n",
      "265/265 [==============================] - 0s 193us/sample - loss: 0.2994 - mae: 0.4103 - mse: 0.2994 - val_loss: 0.3070 - val_mae: 0.4151 - val_mse: 0.3070\n",
      "Epoch 154/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2976 - mae: 0.4072 - mse: 0.2976 - val_loss: 0.3011 - val_mae: 0.4092 - val_mse: 0.3011\n",
      "Epoch 155/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.2990 - mae: 0.4036 - mse: 0.2990 - val_loss: 0.3017 - val_mae: 0.4101 - val_mse: 0.3017\n",
      "Epoch 156/1000\n",
      "265/265 [==============================] - 0s 183us/sample - loss: 0.2960 - mae: 0.4062 - mse: 0.2960 - val_loss: 0.3117 - val_mae: 0.4202 - val_mse: 0.3117\n",
      "Epoch 157/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.2976 - mae: 0.4098 - mse: 0.2976 - val_loss: 0.3038 - val_mae: 0.4120 - val_mse: 0.3038\n",
      "Epoch 158/1000\n",
      "265/265 [==============================] - 0s 179us/sample - loss: 0.2995 - mae: 0.4050 - mse: 0.2995 - val_loss: 0.2992 - val_mae: 0.4075 - val_mse: 0.2992\n",
      "Epoch 159/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.2939 - mae: 0.4028 - mse: 0.2939 - val_loss: 0.3078 - val_mae: 0.4165 - val_mse: 0.3078\n",
      "Epoch 160/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.2965 - mae: 0.4124 - mse: 0.2965 - val_loss: 0.3125 - val_mae: 0.4209 - val_mse: 0.3125\n",
      "Epoch 161/1000\n",
      "265/265 [==============================] - 0s 194us/sample - loss: 0.2974 - mae: 0.4081 - mse: 0.2974 - val_loss: 0.2976 - val_mae: 0.4062 - val_mse: 0.2976\n",
      "Epoch 162/1000\n",
      "265/265 [==============================] - 0s 169us/sample - loss: 0.2936 - mae: 0.4023 - mse: 0.2936 - val_loss: 0.3049 - val_mae: 0.4131 - val_mse: 0.3049\n",
      "Epoch 163/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.2937 - mae: 0.4076 - mse: 0.2937 - val_loss: 0.3037 - val_mae: 0.4117 - val_mse: 0.3037\n",
      "Epoch 164/1000\n",
      "265/265 [==============================] - 0s 196us/sample - loss: 0.2923 - mae: 0.4041 - mse: 0.2923 - val_loss: 0.3012 - val_mae: 0.4092 - val_mse: 0.3012\n",
      "Epoch 165/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.2941 - mae: 0.4099 - mse: 0.2941 - val_loss: 0.3075 - val_mae: 0.4135 - val_mse: 0.3075\n",
      "Epoch 166/1000\n",
      "265/265 [==============================] - 0s 222us/sample - loss: 0.2923 - mae: 0.4062 - mse: 0.2923 - val_loss: 0.2986 - val_mae: 0.4057 - val_mse: 0.2986\n",
      "Epoch 167/1000\n",
      "265/265 [==============================] - 0s 227us/sample - loss: 0.2910 - mae: 0.4056 - mse: 0.2910 - val_loss: 0.3023 - val_mae: 0.4086 - val_mse: 0.3023\n",
      "Epoch 168/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.2910 - mae: 0.4072 - mse: 0.2910 - val_loss: 0.3031 - val_mae: 0.4105 - val_mse: 0.3031\n",
      "Epoch 169/1000\n",
      "265/265 [==============================] - 0s 193us/sample - loss: 0.2895 - mae: 0.4050 - mse: 0.2895 - val_loss: 0.3024 - val_mae: 0.4092 - val_mse: 0.3024\n",
      "Epoch 170/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.2892 - mae: 0.4046 - mse: 0.2892 - val_loss: 0.3019 - val_mae: 0.4089 - val_mse: 0.3019\n",
      "Epoch 171/1000\n",
      "265/265 [==============================] - 0s 206us/sample - loss: 0.2889 - mae: 0.4028 - mse: 0.2889 - val_loss: 0.3011 - val_mae: 0.4092 - val_mse: 0.3011\n",
      "Epoch 172/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2887 - mae: 0.4053 - mse: 0.2887 - val_loss: 0.3052 - val_mae: 0.4128 - val_mse: 0.3052\n",
      "Epoch 173/1000\n",
      "265/265 [==============================] - 0s 187us/sample - loss: 0.2876 - mae: 0.4030 - mse: 0.2876 - val_loss: 0.2983 - val_mae: 0.4059 - val_mse: 0.2983\n",
      "Epoch 174/1000\n",
      "265/265 [==============================] - 0s 175us/sample - loss: 0.2866 - mae: 0.4009 - mse: 0.2866 - val_loss: 0.3014 - val_mae: 0.4087 - val_mse: 0.3014\n",
      "Epoch 175/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.2870 - mae: 0.4044 - mse: 0.2870 - val_loss: 0.3041 - val_mae: 0.4116 - val_mse: 0.3041\n",
      "Epoch 176/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 0.2879 - mae: 0.4062 - mse: 0.2879 - val_loss: 0.3038 - val_mae: 0.4114 - val_mse: 0.3038\n",
      "Epoch 177/1000\n",
      "265/265 [==============================] - 0s 226us/sample - loss: 0.2871 - mae: 0.4013 - mse: 0.2871 - val_loss: 0.3001 - val_mae: 0.4079 - val_mse: 0.3001\n",
      "Epoch 178/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.2861 - mae: 0.4040 - mse: 0.2861 - val_loss: 0.3072 - val_mae: 0.4148 - val_mse: 0.3072\n",
      "Epoch 179/1000\n",
      "265/265 [==============================] - 0s 186us/sample - loss: 0.2836 - mae: 0.3982 - mse: 0.2836 - val_loss: 0.2952 - val_mae: 0.4036 - val_mse: 0.2952\n",
      "Epoch 180/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.2861 - mae: 0.3974 - mse: 0.2861 - val_loss: 0.2950 - val_mae: 0.4031 - val_mse: 0.2950\n",
      "Epoch 181/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.2838 - mae: 0.3982 - mse: 0.2838 - val_loss: 0.3027 - val_mae: 0.4101 - val_mse: 0.3027\n",
      "Epoch 182/1000\n",
      "265/265 [==============================] - 0s 197us/sample - loss: 0.2834 - mae: 0.4020 - mse: 0.2834 - val_loss: 0.3000 - val_mae: 0.4078 - val_mse: 0.3000\n",
      "Epoch 183/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2934 - mae: 0.4028 - mse: 0.2934 - val_loss: 0.2954 - val_mae: 0.4045 - val_mse: 0.2954\n",
      "Epoch 184/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.2822 - mae: 0.3992 - mse: 0.2822 - val_loss: 0.3123 - val_mae: 0.4218 - val_mse: 0.3123\n",
      "Epoch 185/1000\n",
      "265/265 [==============================] - 0s 222us/sample - loss: 0.2833 - mae: 0.4015 - mse: 0.2833 - val_loss: 0.2963 - val_mae: 0.4053 - val_mse: 0.2963\n",
      "Epoch 186/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.2835 - mae: 0.3957 - mse: 0.2835 - val_loss: 0.2955 - val_mae: 0.4044 - val_mse: 0.2955\n",
      "Epoch 187/1000\n",
      "265/265 [==============================] - 0s 194us/sample - loss: 0.2848 - mae: 0.4060 - mse: 0.2848 - val_loss: 0.3082 - val_mae: 0.4167 - val_mse: 0.3082\n",
      "Epoch 188/1000\n",
      "265/265 [==============================] - 0s 177us/sample - loss: 0.2838 - mae: 0.4006 - mse: 0.2838 - val_loss: 0.2920 - val_mae: 0.3997 - val_mse: 0.2920\n",
      "Epoch 189/1000\n",
      "265/265 [==============================] - 0s 187us/sample - loss: 0.2803 - mae: 0.3959 - mse: 0.2803 - val_loss: 0.2960 - val_mae: 0.4036 - val_mse: 0.2960\n",
      "Epoch 190/1000\n",
      "265/265 [==============================] - 0s 177us/sample - loss: 0.2805 - mae: 0.3959 - mse: 0.2805 - val_loss: 0.2955 - val_mae: 0.4036 - val_mse: 0.2955\n",
      "Epoch 191/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.2781 - mae: 0.3977 - mse: 0.2781 - val_loss: 0.3046 - val_mae: 0.4137 - val_mse: 0.3046\n",
      "Epoch 192/1000\n",
      "265/265 [==============================] - 0s 185us/sample - loss: 0.2796 - mae: 0.4005 - mse: 0.2796 - val_loss: 0.2969 - val_mae: 0.4058 - val_mse: 0.2969\n",
      "Epoch 193/1000\n",
      "265/265 [==============================] - 0s 173us/sample - loss: 0.2801 - mae: 0.3950 - mse: 0.2801 - val_loss: 0.2901 - val_mae: 0.3994 - val_mse: 0.2901\n",
      "Epoch 194/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.2803 - mae: 0.3962 - mse: 0.2803 - val_loss: 0.3029 - val_mae: 0.4131 - val_mse: 0.3029\n",
      "Epoch 195/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.2771 - mae: 0.3959 - mse: 0.2771 - val_loss: 0.2941 - val_mae: 0.4039 - val_mse: 0.2941\n",
      "Epoch 196/1000\n",
      "265/265 [==============================] - 0s 177us/sample - loss: 0.2764 - mae: 0.3925 - mse: 0.2764 - val_loss: 0.2921 - val_mae: 0.4018 - val_mse: 0.2921\n",
      "Epoch 197/1000\n",
      "265/265 [==============================] - 0s 193us/sample - loss: 0.2771 - mae: 0.3915 - mse: 0.2771 - val_loss: 0.2916 - val_mae: 0.4011 - val_mse: 0.2916\n",
      "Epoch 198/1000\n",
      "265/265 [==============================] - 0s 219us/sample - loss: 0.2782 - mae: 0.3960 - mse: 0.2782 - val_loss: 0.3009 - val_mae: 0.4112 - val_mse: 0.3009\n",
      "Epoch 199/1000\n",
      "265/265 [==============================] - 0s 188us/sample - loss: 0.2768 - mae: 0.3940 - mse: 0.2768 - val_loss: 0.2902 - val_mae: 0.4000 - val_mse: 0.2902\n",
      "Epoch 200/1000\n",
      "265/265 [==============================] - 0s 221us/sample - loss: 0.2837 - mae: 0.4037 - mse: 0.2837 - val_loss: 0.3096 - val_mae: 0.4192 - val_mse: 0.3096\n",
      "Epoch 201/1000\n",
      "265/265 [==============================] - 0s 190us/sample - loss: 0.2779 - mae: 0.4010 - mse: 0.2779 - val_loss: 0.2891 - val_mae: 0.3985 - val_mse: 0.2891\n",
      "Epoch 202/1000\n",
      "265/265 [==============================] - 0s 233us/sample - loss: 0.2759 - mae: 0.3917 - mse: 0.2759 - val_loss: 0.2917 - val_mae: 0.4007 - val_mse: 0.2917\n",
      "Epoch 203/1000\n",
      "265/265 [==============================] - 0s 195us/sample - loss: 0.2765 - mae: 0.4035 - mse: 0.2765 - val_loss: 0.3094 - val_mae: 0.4183 - val_mse: 0.3094\n",
      "Epoch 204/1000\n",
      "265/265 [==============================] - 0s 213us/sample - loss: 0.2740 - mae: 0.3980 - mse: 0.2740 - val_loss: 0.2901 - val_mae: 0.4001 - val_mse: 0.2901\n",
      "Epoch 205/1000\n",
      "265/265 [==============================] - 0s 194us/sample - loss: 0.2739 - mae: 0.3930 - mse: 0.2739 - val_loss: 0.2939 - val_mae: 0.4036 - val_mse: 0.2939\n",
      "Epoch 206/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2724 - mae: 0.3939 - mse: 0.2724 - val_loss: 0.2906 - val_mae: 0.3999 - val_mse: 0.2906\n",
      "Epoch 207/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 0.2708 - mae: 0.3924 - mse: 0.2708 - val_loss: 0.2960 - val_mae: 0.4066 - val_mse: 0.2960\n",
      "Epoch 208/1000\n",
      "265/265 [==============================] - 0s 187us/sample - loss: 0.2735 - mae: 0.3972 - mse: 0.2735 - val_loss: 0.2959 - val_mae: 0.4070 - val_mse: 0.2959\n",
      "Epoch 209/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.2721 - mae: 0.3897 - mse: 0.2721 - val_loss: 0.2854 - val_mae: 0.3967 - val_mse: 0.2854\n",
      "Epoch 210/1000\n",
      "265/265 [==============================] - 0s 182us/sample - loss: 0.2712 - mae: 0.3911 - mse: 0.2712 - val_loss: 0.2976 - val_mae: 0.4088 - val_mse: 0.2976\n",
      "Epoch 211/1000\n",
      "265/265 [==============================] - 0s 200us/sample - loss: 0.2718 - mae: 0.3926 - mse: 0.2718 - val_loss: 0.2863 - val_mae: 0.3967 - val_mse: 0.2863\n",
      "Epoch 212/1000\n",
      "265/265 [==============================] - 0s 195us/sample - loss: 0.2694 - mae: 0.3932 - mse: 0.2694 - val_loss: 0.3032 - val_mae: 0.4121 - val_mse: 0.3032\n",
      "Epoch 213/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.2728 - mae: 0.3998 - mse: 0.2728 - val_loss: 0.2912 - val_mae: 0.4004 - val_mse: 0.2912\n",
      "Epoch 214/1000\n",
      "265/265 [==============================] - 0s 197us/sample - loss: 0.2709 - mae: 0.3914 - mse: 0.2709 - val_loss: 0.2830 - val_mae: 0.3941 - val_mse: 0.2830\n",
      "Epoch 215/1000\n",
      "265/265 [==============================] - 0s 213us/sample - loss: 0.2761 - mae: 0.3965 - mse: 0.2761 - val_loss: 0.2942 - val_mae: 0.4050 - val_mse: 0.2942\n",
      "Epoch 216/1000\n",
      "265/265 [==============================] - 0s 182us/sample - loss: 0.2689 - mae: 0.3899 - mse: 0.2689 - val_loss: 0.2850 - val_mae: 0.3969 - val_mse: 0.2850\n",
      "Epoch 217/1000\n",
      "265/265 [==============================] - 0s 245us/sample - loss: 0.2712 - mae: 0.3942 - mse: 0.2712 - val_loss: 0.2981 - val_mae: 0.4096 - val_mse: 0.2981\n",
      "Epoch 218/1000\n",
      "265/265 [==============================] - 0s 219us/sample - loss: 0.2701 - mae: 0.3963 - mse: 0.2701 - val_loss: 0.2920 - val_mae: 0.4049 - val_mse: 0.2920\n",
      "Epoch 219/1000\n",
      "265/265 [==============================] - 0s 193us/sample - loss: 0.2677 - mae: 0.3859 - mse: 0.2677 - val_loss: 0.2821 - val_mae: 0.3956 - val_mse: 0.2821\n",
      "Epoch 220/1000\n",
      "265/265 [==============================] - 0s 183us/sample - loss: 0.2679 - mae: 0.3866 - mse: 0.2679 - val_loss: 0.2876 - val_mae: 0.4006 - val_mse: 0.2876\n",
      "Epoch 221/1000\n",
      "265/265 [==============================] - 0s 196us/sample - loss: 0.2674 - mae: 0.3871 - mse: 0.2674 - val_loss: 0.2832 - val_mae: 0.3960 - val_mse: 0.2832\n",
      "Epoch 222/1000\n",
      "265/265 [==============================] - 0s 179us/sample - loss: 0.2689 - mae: 0.3929 - mse: 0.2689 - val_loss: 0.2942 - val_mae: 0.4066 - val_mse: 0.2942\n",
      "Epoch 223/1000\n",
      "265/265 [==============================] - 0s 185us/sample - loss: 0.2709 - mae: 0.3892 - mse: 0.2709 - val_loss: 0.2817 - val_mae: 0.3942 - val_mse: 0.2817\n",
      "Epoch 224/1000\n",
      "265/265 [==============================] - 0s 194us/sample - loss: 0.2648 - mae: 0.3855 - mse: 0.2648 - val_loss: 0.2846 - val_mae: 0.3970 - val_mse: 0.2846\n",
      "Epoch 225/1000\n",
      "265/265 [==============================] - 0s 222us/sample - loss: 0.2669 - mae: 0.3918 - mse: 0.2669 - val_loss: 0.2861 - val_mae: 0.3985 - val_mse: 0.2861\n",
      "Epoch 226/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.2639 - mae: 0.3860 - mse: 0.2639 - val_loss: 0.2796 - val_mae: 0.3926 - val_mse: 0.2796\n",
      "Epoch 227/1000\n",
      "265/265 [==============================] - 0s 246us/sample - loss: 0.2655 - mae: 0.3898 - mse: 0.2655 - val_loss: 0.2873 - val_mae: 0.4003 - val_mse: 0.2873\n",
      "Epoch 228/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.2628 - mae: 0.3876 - mse: 0.2628 - val_loss: 0.2812 - val_mae: 0.3948 - val_mse: 0.2812\n",
      "Epoch 229/1000\n",
      "265/265 [==============================] - 0s 230us/sample - loss: 0.2638 - mae: 0.3872 - mse: 0.2638 - val_loss: 0.2810 - val_mae: 0.3953 - val_mse: 0.2810\n",
      "Epoch 230/1000\n",
      "265/265 [==============================] - 0s 255us/sample - loss: 0.2660 - mae: 0.3845 - mse: 0.2660 - val_loss: 0.2793 - val_mae: 0.3936 - val_mse: 0.2793\n",
      "Epoch 231/1000\n",
      "265/265 [==============================] - 0s 224us/sample - loss: 0.2621 - mae: 0.3892 - mse: 0.2621 - val_loss: 0.2979 - val_mae: 0.4088 - val_mse: 0.2979\n",
      "Epoch 232/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2645 - mae: 0.3894 - mse: 0.2645 - val_loss: 0.2778 - val_mae: 0.3913 - val_mse: 0.2778\n",
      "Epoch 233/1000\n",
      "265/265 [==============================] - 0s 232us/sample - loss: 0.2636 - mae: 0.3829 - mse: 0.2636 - val_loss: 0.2787 - val_mae: 0.3929 - val_mse: 0.2787\n",
      "Epoch 234/1000\n",
      "265/265 [==============================] - 0s 235us/sample - loss: 0.2610 - mae: 0.3845 - mse: 0.2610 - val_loss: 0.2852 - val_mae: 0.3993 - val_mse: 0.2852\n",
      "Epoch 235/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.2624 - mae: 0.3859 - mse: 0.2624 - val_loss: 0.2825 - val_mae: 0.3976 - val_mse: 0.2825\n",
      "Epoch 236/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.2604 - mae: 0.3845 - mse: 0.2604 - val_loss: 0.2803 - val_mae: 0.3954 - val_mse: 0.2803\n",
      "Epoch 237/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.2606 - mae: 0.3820 - mse: 0.2606 - val_loss: 0.2798 - val_mae: 0.3951 - val_mse: 0.2798\n",
      "Epoch 238/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2606 - mae: 0.3846 - mse: 0.2606 - val_loss: 0.2832 - val_mae: 0.3978 - val_mse: 0.2832\n",
      "Epoch 239/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.2604 - mae: 0.3844 - mse: 0.2604 - val_loss: 0.2824 - val_mae: 0.3968 - val_mse: 0.2824\n",
      "Epoch 240/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.2594 - mae: 0.3841 - mse: 0.2594 - val_loss: 0.2813 - val_mae: 0.3959 - val_mse: 0.2813\n",
      "Epoch 241/1000\n",
      "265/265 [==============================] - 0s 219us/sample - loss: 0.2591 - mae: 0.3836 - mse: 0.2591 - val_loss: 0.2792 - val_mae: 0.3943 - val_mse: 0.2792\n",
      "Epoch 242/1000\n",
      "265/265 [==============================] - 0s 244us/sample - loss: 0.2590 - mae: 0.3843 - mse: 0.2590 - val_loss: 0.2796 - val_mae: 0.3940 - val_mse: 0.2796\n",
      "Epoch 243/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2588 - mae: 0.3826 - mse: 0.2588 - val_loss: 0.2766 - val_mae: 0.3917 - val_mse: 0.2766\n",
      "Epoch 244/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.2623 - mae: 0.3813 - mse: 0.2623 - val_loss: 0.2754 - val_mae: 0.3924 - val_mse: 0.2754\n",
      "Epoch 245/1000\n",
      "265/265 [==============================] - 0s 222us/sample - loss: 0.2598 - mae: 0.3833 - mse: 0.2598 - val_loss: 0.2821 - val_mae: 0.3978 - val_mse: 0.2821\n",
      "Epoch 246/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.2580 - mae: 0.3813 - mse: 0.2580 - val_loss: 0.2732 - val_mae: 0.3911 - val_mse: 0.2732\n",
      "Epoch 247/1000\n",
      "265/265 [==============================] - 0s 255us/sample - loss: 0.2591 - mae: 0.3822 - mse: 0.2591 - val_loss: 0.2769 - val_mae: 0.3930 - val_mse: 0.2769\n",
      "Epoch 248/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.2571 - mae: 0.3809 - mse: 0.2571 - val_loss: 0.2759 - val_mae: 0.3932 - val_mse: 0.2759\n",
      "Epoch 249/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.2565 - mae: 0.3795 - mse: 0.2565 - val_loss: 0.2756 - val_mae: 0.3930 - val_mse: 0.2756\n",
      "Epoch 250/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.2564 - mae: 0.3793 - mse: 0.2564 - val_loss: 0.2769 - val_mae: 0.3944 - val_mse: 0.2769\n",
      "Epoch 251/1000\n",
      "265/265 [==============================] - 0s 231us/sample - loss: 0.2561 - mae: 0.3811 - mse: 0.2561 - val_loss: 0.2805 - val_mae: 0.3960 - val_mse: 0.2805\n",
      "Epoch 252/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.2583 - mae: 0.3855 - mse: 0.2583 - val_loss: 0.2756 - val_mae: 0.3927 - val_mse: 0.2756\n",
      "Epoch 253/1000\n",
      "265/265 [==============================] - 0s 218us/sample - loss: 0.2791 - mae: 0.3912 - mse: 0.2791 - val_loss: 0.2690 - val_mae: 0.3884 - val_mse: 0.2690\n",
      "Epoch 254/1000\n",
      "265/265 [==============================] - 0s 191us/sample - loss: 0.2690 - mae: 0.3986 - mse: 0.2690 - val_loss: 0.3040 - val_mae: 0.4167 - val_mse: 0.3040\n",
      "Epoch 255/1000\n",
      "265/265 [==============================] - 0s 191us/sample - loss: 0.2554 - mae: 0.3823 - mse: 0.2554 - val_loss: 0.2674 - val_mae: 0.3860 - val_mse: 0.2674\n",
      "Epoch 256/1000\n",
      "265/265 [==============================] - 0s 212us/sample - loss: 0.2621 - mae: 0.3768 - mse: 0.2621 - val_loss: 0.2782 - val_mae: 0.3960 - val_mse: 0.2782\n",
      "Epoch 257/1000\n",
      "265/265 [==============================] - 0s 245us/sample - loss: 0.2572 - mae: 0.3828 - mse: 0.2572 - val_loss: 0.2783 - val_mae: 0.3955 - val_mse: 0.2783\n",
      "Epoch 258/1000\n",
      "265/265 [==============================] - 0s 223us/sample - loss: 0.2555 - mae: 0.3799 - mse: 0.2555 - val_loss: 0.2715 - val_mae: 0.3909 - val_mse: 0.2715\n",
      "Epoch 259/1000\n",
      "265/265 [==============================] - 0s 183us/sample - loss: 0.2537 - mae: 0.3789 - mse: 0.2537 - val_loss: 0.2797 - val_mae: 0.3970 - val_mse: 0.2797\n",
      "Epoch 260/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.2586 - mae: 0.3808 - mse: 0.2586 - val_loss: 0.2703 - val_mae: 0.3894 - val_mse: 0.2703\n",
      "Epoch 261/1000\n",
      "265/265 [==============================] - 0s 231us/sample - loss: 0.2557 - mae: 0.3805 - mse: 0.2557 - val_loss: 0.2767 - val_mae: 0.3936 - val_mse: 0.2767\n",
      "Epoch 262/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.2535 - mae: 0.3803 - mse: 0.2535 - val_loss: 0.2716 - val_mae: 0.3901 - val_mse: 0.2716\n",
      "Epoch 263/1000\n",
      "265/265 [==============================] - 0s 250us/sample - loss: 0.2550 - mae: 0.3767 - mse: 0.2550 - val_loss: 0.2660 - val_mae: 0.3861 - val_mse: 0.2660\n",
      "Epoch 264/1000\n",
      "265/265 [==============================] - 0s 189us/sample - loss: 0.2534 - mae: 0.3748 - mse: 0.2534 - val_loss: 0.2763 - val_mae: 0.3938 - val_mse: 0.2763\n",
      "Epoch 265/1000\n",
      "265/265 [==============================] - 0s 239us/sample - loss: 0.2557 - mae: 0.3856 - mse: 0.2557 - val_loss: 0.2852 - val_mae: 0.4021 - val_mse: 0.2852\n",
      "Epoch 266/1000\n",
      "265/265 [==============================] - 0s 230us/sample - loss: 0.2519 - mae: 0.3768 - mse: 0.2519 - val_loss: 0.2653 - val_mae: 0.3860 - val_mse: 0.2653\n",
      "Epoch 267/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.2555 - mae: 0.3737 - mse: 0.2555 - val_loss: 0.2721 - val_mae: 0.3923 - val_mse: 0.2721\n",
      "Epoch 268/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.2510 - mae: 0.3759 - mse: 0.2510 - val_loss: 0.2778 - val_mae: 0.3958 - val_mse: 0.2778\n",
      "Epoch 269/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.2537 - mae: 0.3760 - mse: 0.2537 - val_loss: 0.2670 - val_mae: 0.3895 - val_mse: 0.2670\n",
      "Epoch 270/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2530 - mae: 0.3728 - mse: 0.2530 - val_loss: 0.2729 - val_mae: 0.3947 - val_mse: 0.2729\n",
      "Epoch 271/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2556 - mae: 0.3822 - mse: 0.2556 - val_loss: 0.2771 - val_mae: 0.3955 - val_mse: 0.2771\n",
      "Epoch 272/1000\n",
      "265/265 [==============================] - 0s 200us/sample - loss: 0.2500 - mae: 0.3728 - mse: 0.2500 - val_loss: 0.2644 - val_mae: 0.3857 - val_mse: 0.2644\n",
      "Epoch 273/1000\n",
      "265/265 [==============================] - 0s 233us/sample - loss: 0.2530 - mae: 0.3717 - mse: 0.2530 - val_loss: 0.2710 - val_mae: 0.3900 - val_mse: 0.2710\n",
      "Epoch 274/1000\n",
      "265/265 [==============================] - 0s 228us/sample - loss: 0.2519 - mae: 0.3742 - mse: 0.2519 - val_loss: 0.2696 - val_mae: 0.3899 - val_mse: 0.2696\n",
      "Epoch 275/1000\n",
      "265/265 [==============================] - 0s 255us/sample - loss: 0.2509 - mae: 0.3758 - mse: 0.2509 - val_loss: 0.2634 - val_mae: 0.3850 - val_mse: 0.2634\n",
      "Epoch 276/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.2514 - mae: 0.3704 - mse: 0.2514 - val_loss: 0.2637 - val_mae: 0.3851 - val_mse: 0.2637\n",
      "Epoch 277/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.2508 - mae: 0.3761 - mse: 0.2508 - val_loss: 0.2746 - val_mae: 0.3925 - val_mse: 0.2746\n",
      "Epoch 278/1000\n",
      "265/265 [==============================] - 0s 212us/sample - loss: 0.2529 - mae: 0.3797 - mse: 0.2529 - val_loss: 0.2670 - val_mae: 0.3881 - val_mse: 0.2670\n",
      "Epoch 279/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.2493 - mae: 0.3721 - mse: 0.2493 - val_loss: 0.2660 - val_mae: 0.3879 - val_mse: 0.2660\n",
      "Epoch 280/1000\n",
      "265/265 [==============================] - 0s 233us/sample - loss: 0.2489 - mae: 0.3748 - mse: 0.2489 - val_loss: 0.2733 - val_mae: 0.3919 - val_mse: 0.2733\n",
      "Epoch 281/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.2511 - mae: 0.3740 - mse: 0.2511 - val_loss: 0.2611 - val_mae: 0.3835 - val_mse: 0.2611\n",
      "Epoch 282/1000\n",
      "265/265 [==============================] - 0s 221us/sample - loss: 0.2479 - mae: 0.3708 - mse: 0.2479 - val_loss: 0.2761 - val_mae: 0.3947 - val_mse: 0.2761\n",
      "Epoch 283/1000\n",
      "265/265 [==============================] - 0s 206us/sample - loss: 0.2510 - mae: 0.3779 - mse: 0.2510 - val_loss: 0.2722 - val_mae: 0.3911 - val_mse: 0.2722\n",
      "Epoch 284/1000\n",
      "265/265 [==============================] - 0s 221us/sample - loss: 0.2491 - mae: 0.3767 - mse: 0.2491 - val_loss: 0.2660 - val_mae: 0.3865 - val_mse: 0.2660\n",
      "Epoch 285/1000\n",
      "265/265 [==============================] - 0s 281us/sample - loss: 0.2493 - mae: 0.3691 - mse: 0.2493 - val_loss: 0.2619 - val_mae: 0.3850 - val_mse: 0.2619\n",
      "Epoch 286/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.2500 - mae: 0.3734 - mse: 0.2500 - val_loss: 0.2778 - val_mae: 0.3962 - val_mse: 0.2778\n",
      "Epoch 287/1000\n",
      "265/265 [==============================] - 0s 232us/sample - loss: 0.2474 - mae: 0.3736 - mse: 0.2474 - val_loss: 0.2638 - val_mae: 0.3865 - val_mse: 0.2638\n",
      "Epoch 288/1000\n",
      "265/265 [==============================] - 0s 215us/sample - loss: 0.2486 - mae: 0.3735 - mse: 0.2486 - val_loss: 0.2693 - val_mae: 0.3910 - val_mse: 0.2693\n",
      "Epoch 289/1000\n",
      "265/265 [==============================] - 0s 241us/sample - loss: 0.2492 - mae: 0.3757 - mse: 0.2492 - val_loss: 0.2659 - val_mae: 0.3879 - val_mse: 0.2659\n",
      "Epoch 290/1000\n",
      "265/265 [==============================] - 0s 303us/sample - loss: 0.2478 - mae: 0.3697 - mse: 0.2478 - val_loss: 0.2627 - val_mae: 0.3856 - val_mse: 0.2627\n",
      "Epoch 291/1000\n",
      "265/265 [==============================] - 0s 243us/sample - loss: 0.2502 - mae: 0.3759 - mse: 0.2502 - val_loss: 0.2717 - val_mae: 0.3907 - val_mse: 0.2717\n",
      "Epoch 292/1000\n",
      "265/265 [==============================] - 0s 223us/sample - loss: 0.2484 - mae: 0.3686 - mse: 0.2484 - val_loss: 0.2608 - val_mae: 0.3840 - val_mse: 0.2608\n",
      "Epoch 293/1000\n",
      "265/265 [==============================] - 0s 215us/sample - loss: 0.2470 - mae: 0.3713 - mse: 0.2470 - val_loss: 0.2732 - val_mae: 0.3937 - val_mse: 0.2732\n",
      "Epoch 294/1000\n",
      "265/265 [==============================] - 0s 218us/sample - loss: 0.2472 - mae: 0.3745 - mse: 0.2472 - val_loss: 0.2653 - val_mae: 0.3877 - val_mse: 0.2653\n",
      "Epoch 295/1000\n",
      "265/265 [==============================] - 0s 181us/sample - loss: 0.2469 - mae: 0.3686 - mse: 0.2469 - val_loss: 0.2609 - val_mae: 0.3847 - val_mse: 0.2609\n",
      "Epoch 296/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2448 - mae: 0.3680 - mse: 0.2448 - val_loss: 0.2739 - val_mae: 0.3952 - val_mse: 0.2739\n",
      "Epoch 297/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.2490 - mae: 0.3753 - mse: 0.2490 - val_loss: 0.2711 - val_mae: 0.3915 - val_mse: 0.2711\n",
      "Epoch 298/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.2491 - mae: 0.3771 - mse: 0.2491 - val_loss: 0.2648 - val_mae: 0.3863 - val_mse: 0.2648\n",
      "Epoch 299/1000\n",
      "265/265 [==============================] - 0s 183us/sample - loss: 0.2465 - mae: 0.3727 - mse: 0.2465 - val_loss: 0.2637 - val_mae: 0.3865 - val_mse: 0.2637\n",
      "Epoch 300/1000\n",
      "265/265 [==============================] - 0s 223us/sample - loss: 0.2476 - mae: 0.3679 - mse: 0.2476 - val_loss: 0.2634 - val_mae: 0.3856 - val_mse: 0.2634\n",
      "Epoch 301/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.2444 - mae: 0.3703 - mse: 0.2444 - val_loss: 0.2691 - val_mae: 0.3895 - val_mse: 0.2691\n",
      "Epoch 302/1000\n",
      "265/265 [==============================] - 0s 260us/sample - loss: 0.2448 - mae: 0.3699 - mse: 0.2448 - val_loss: 0.2617 - val_mae: 0.3832 - val_mse: 0.2617\n",
      "Epoch 303/1000\n",
      "265/265 [==============================] - 0s 230us/sample - loss: 0.2458 - mae: 0.3723 - mse: 0.2458 - val_loss: 0.2633 - val_mae: 0.3846 - val_mse: 0.2633\n",
      "Epoch 304/1000\n",
      "265/265 [==============================] - 0s 188us/sample - loss: 0.2478 - mae: 0.3699 - mse: 0.2478 - val_loss: 0.2639 - val_mae: 0.3866 - val_mse: 0.2639\n",
      "Epoch 305/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.2449 - mae: 0.3695 - mse: 0.2449 - val_loss: 0.2670 - val_mae: 0.3884 - val_mse: 0.2670\n",
      "Epoch 306/1000\n",
      "265/265 [==============================] - 0s 199us/sample - loss: 0.2472 - mae: 0.3752 - mse: 0.2472 - val_loss: 0.2711 - val_mae: 0.3900 - val_mse: 0.2711\n",
      "Epoch 307/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2446 - mae: 0.3716 - mse: 0.2446 - val_loss: 0.2669 - val_mae: 0.3855 - val_mse: 0.2669\n",
      "Epoch 308/1000\n",
      "265/265 [==============================] - 0s 224us/sample - loss: 0.2435 - mae: 0.3711 - mse: 0.2435 - val_loss: 0.2679 - val_mae: 0.3852 - val_mse: 0.2679\n",
      "Epoch 309/1000\n",
      "265/265 [==============================] - 0s 195us/sample - loss: 0.2478 - mae: 0.3699 - mse: 0.2478 - val_loss: 0.2626 - val_mae: 0.3832 - val_mse: 0.2626\n",
      "Epoch 310/1000\n",
      "265/265 [==============================] - 0s 227us/sample - loss: 0.2448 - mae: 0.3707 - mse: 0.2448 - val_loss: 0.2638 - val_mae: 0.3857 - val_mse: 0.2638\n",
      "Epoch 311/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.2429 - mae: 0.3671 - mse: 0.2429 - val_loss: 0.2604 - val_mae: 0.3842 - val_mse: 0.2604\n",
      "Epoch 312/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.2441 - mae: 0.3652 - mse: 0.2441 - val_loss: 0.2663 - val_mae: 0.3894 - val_mse: 0.2663\n",
      "Epoch 313/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.2435 - mae: 0.3692 - mse: 0.2435 - val_loss: 0.2647 - val_mae: 0.3859 - val_mse: 0.2647\n",
      "Epoch 314/1000\n",
      "265/265 [==============================] - 0s 206us/sample - loss: 0.2466 - mae: 0.3747 - mse: 0.2466 - val_loss: 0.2598 - val_mae: 0.3806 - val_mse: 0.2598\n",
      "Epoch 315/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2468 - mae: 0.3668 - mse: 0.2468 - val_loss: 0.2593 - val_mae: 0.3808 - val_mse: 0.2593\n",
      "Epoch 316/1000\n",
      "265/265 [==============================] - 0s 189us/sample - loss: 0.2424 - mae: 0.3709 - mse: 0.2424 - val_loss: 0.2655 - val_mae: 0.3867 - val_mse: 0.2655\n",
      "Epoch 317/1000\n",
      "265/265 [==============================] - 0s 200us/sample - loss: 0.2413 - mae: 0.3662 - mse: 0.2413 - val_loss: 0.2582 - val_mae: 0.3824 - val_mse: 0.2582\n",
      "Epoch 318/1000\n",
      "265/265 [==============================] - 0s 227us/sample - loss: 0.2428 - mae: 0.3639 - mse: 0.2428 - val_loss: 0.2597 - val_mae: 0.3838 - val_mse: 0.2597\n",
      "Epoch 319/1000\n",
      "265/265 [==============================] - 0s 185us/sample - loss: 0.2410 - mae: 0.3666 - mse: 0.2410 - val_loss: 0.2671 - val_mae: 0.3897 - val_mse: 0.2671\n",
      "Epoch 320/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.2426 - mae: 0.3676 - mse: 0.2426 - val_loss: 0.2586 - val_mae: 0.3828 - val_mse: 0.2586\n",
      "Epoch 321/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 0.2444 - mae: 0.3636 - mse: 0.2444 - val_loss: 0.2608 - val_mae: 0.3849 - val_mse: 0.2608\n",
      "Epoch 322/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.2415 - mae: 0.3692 - mse: 0.2415 - val_loss: 0.2719 - val_mae: 0.3924 - val_mse: 0.2719\n",
      "Epoch 323/1000\n",
      "265/265 [==============================] - 0s 222us/sample - loss: 0.2444 - mae: 0.3694 - mse: 0.2444 - val_loss: 0.2585 - val_mae: 0.3823 - val_mse: 0.2585\n",
      "Epoch 324/1000\n",
      "265/265 [==============================] - 0s 258us/sample - loss: 0.2412 - mae: 0.3652 - mse: 0.2412 - val_loss: 0.2632 - val_mae: 0.3868 - val_mse: 0.2632\n",
      "Epoch 325/1000\n",
      "265/265 [==============================] - 0s 254us/sample - loss: 0.2409 - mae: 0.3677 - mse: 0.2409 - val_loss: 0.2601 - val_mae: 0.3841 - val_mse: 0.2601\n",
      "Epoch 326/1000\n",
      "265/265 [==============================] - 0s 247us/sample - loss: 0.2423 - mae: 0.3624 - mse: 0.2423 - val_loss: 0.2583 - val_mae: 0.3820 - val_mse: 0.2583\n",
      "Epoch 327/1000\n",
      "265/265 [==============================] - 0s 274us/sample - loss: 0.2440 - mae: 0.3684 - mse: 0.2440 - val_loss: 0.2632 - val_mae: 0.3862 - val_mse: 0.2632\n",
      "Epoch 328/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.2433 - mae: 0.3640 - mse: 0.2433 - val_loss: 0.2593 - val_mae: 0.3841 - val_mse: 0.2593\n",
      "Epoch 329/1000\n",
      "265/265 [==============================] - 0s 241us/sample - loss: 0.2398 - mae: 0.3633 - mse: 0.2398 - val_loss: 0.2679 - val_mae: 0.3914 - val_mse: 0.2679\n",
      "Epoch 330/1000\n",
      "265/265 [==============================] - 0s 288us/sample - loss: 0.2426 - mae: 0.3706 - mse: 0.2426 - val_loss: 0.2625 - val_mae: 0.3873 - val_mse: 0.2625\n",
      "Epoch 331/1000\n",
      "265/265 [==============================] - 0s 252us/sample - loss: 0.2395 - mae: 0.3645 - mse: 0.2395 - val_loss: 0.2628 - val_mae: 0.3876 - val_mse: 0.2628\n",
      "Epoch 332/1000\n",
      "265/265 [==============================] - 0s 258us/sample - loss: 0.2394 - mae: 0.3644 - mse: 0.2394 - val_loss: 0.2557 - val_mae: 0.3816 - val_mse: 0.2557\n",
      "Epoch 333/1000\n",
      "265/265 [==============================] - 0s 245us/sample - loss: 0.2394 - mae: 0.3613 - mse: 0.2394 - val_loss: 0.2580 - val_mae: 0.3822 - val_mse: 0.2580\n",
      "Epoch 334/1000\n",
      "265/265 [==============================] - 0s 248us/sample - loss: 0.2384 - mae: 0.3614 - mse: 0.2384 - val_loss: 0.2548 - val_mae: 0.3808 - val_mse: 0.2548\n",
      "Epoch 335/1000\n",
      "265/265 [==============================] - 0s 266us/sample - loss: 0.2410 - mae: 0.3657 - mse: 0.2410 - val_loss: 0.2568 - val_mae: 0.3826 - val_mse: 0.2568\n",
      "Epoch 336/1000\n",
      "265/265 [==============================] - 0s 312us/sample - loss: 0.2500 - mae: 0.3627 - mse: 0.2500 - val_loss: 0.2580 - val_mae: 0.3842 - val_mse: 0.2580\n",
      "Epoch 337/1000\n",
      "265/265 [==============================] - 0s 272us/sample - loss: 0.2483 - mae: 0.3805 - mse: 0.2483 - val_loss: 0.2610 - val_mae: 0.3867 - val_mse: 0.2610\n",
      "Epoch 338/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.2392 - mae: 0.3602 - mse: 0.2392 - val_loss: 0.2513 - val_mae: 0.3784 - val_mse: 0.2513\n",
      "Epoch 339/1000\n",
      "265/265 [==============================] - 0s 303us/sample - loss: 0.2392 - mae: 0.3627 - mse: 0.2392 - val_loss: 0.2630 - val_mae: 0.3853 - val_mse: 0.2630\n",
      "Epoch 340/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.2413 - mae: 0.3684 - mse: 0.2413 - val_loss: 0.2575 - val_mae: 0.3830 - val_mse: 0.2575\n",
      "Epoch 341/1000\n",
      "265/265 [==============================] - 0s 228us/sample - loss: 0.2391 - mae: 0.3661 - mse: 0.2391 - val_loss: 0.2652 - val_mae: 0.3893 - val_mse: 0.2652\n",
      "Epoch 342/1000\n",
      "265/265 [==============================] - 0s 262us/sample - loss: 0.2383 - mae: 0.3627 - mse: 0.2383 - val_loss: 0.2545 - val_mae: 0.3802 - val_mse: 0.2545\n",
      "Epoch 343/1000\n",
      "265/265 [==============================] - 0s 306us/sample - loss: 0.2378 - mae: 0.3638 - mse: 0.2378 - val_loss: 0.2589 - val_mae: 0.3825 - val_mse: 0.2589\n",
      "Epoch 344/1000\n",
      "265/265 [==============================] - 0s 269us/sample - loss: 0.2360 - mae: 0.3597 - mse: 0.2360 - val_loss: 0.2515 - val_mae: 0.3784 - val_mse: 0.2515\n",
      "Epoch 345/1000\n",
      "265/265 [==============================] - 0s 253us/sample - loss: 0.2379 - mae: 0.3621 - mse: 0.2379 - val_loss: 0.2650 - val_mae: 0.3893 - val_mse: 0.2650\n",
      "Epoch 346/1000\n",
      "265/265 [==============================] - 0s 260us/sample - loss: 0.2376 - mae: 0.3667 - mse: 0.2376 - val_loss: 0.2517 - val_mae: 0.3799 - val_mse: 0.2517\n",
      "Epoch 347/1000\n",
      "265/265 [==============================] - 0s 224us/sample - loss: 0.2402 - mae: 0.3597 - mse: 0.2402 - val_loss: 0.2572 - val_mae: 0.3825 - val_mse: 0.2572\n",
      "Epoch 348/1000\n",
      "265/265 [==============================] - 0s 249us/sample - loss: 0.2410 - mae: 0.3678 - mse: 0.2410 - val_loss: 0.2573 - val_mae: 0.3820 - val_mse: 0.2573\n",
      "Epoch 349/1000\n",
      "265/265 [==============================] - 0s 302us/sample - loss: 0.2371 - mae: 0.3611 - mse: 0.2371 - val_loss: 0.2543 - val_mae: 0.3793 - val_mse: 0.2543\n",
      "Epoch 350/1000\n",
      "265/265 [==============================] - 0s 321us/sample - loss: 0.2414 - mae: 0.3600 - mse: 0.2414 - val_loss: 0.2620 - val_mae: 0.3862 - val_mse: 0.2620\n",
      "Epoch 351/1000\n",
      "265/265 [==============================] - 0s 249us/sample - loss: 0.2376 - mae: 0.3663 - mse: 0.2376 - val_loss: 0.2520 - val_mae: 0.3789 - val_mse: 0.2520\n",
      "Epoch 352/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.2398 - mae: 0.3557 - mse: 0.2398 - val_loss: 0.2479 - val_mae: 0.3772 - val_mse: 0.2479\n",
      "Epoch 353/1000\n",
      "265/265 [==============================] - 0s 231us/sample - loss: 0.2362 - mae: 0.3576 - mse: 0.2362 - val_loss: 0.2615 - val_mae: 0.3867 - val_mse: 0.2615\n",
      "Epoch 354/1000\n",
      "265/265 [==============================] - 0s 222us/sample - loss: 0.2364 - mae: 0.3600 - mse: 0.2364 - val_loss: 0.2517 - val_mae: 0.3790 - val_mse: 0.2517\n",
      "Epoch 355/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.2356 - mae: 0.3578 - mse: 0.2356 - val_loss: 0.2577 - val_mae: 0.3826 - val_mse: 0.2577\n",
      "Epoch 356/1000\n",
      "265/265 [==============================] - 0s 253us/sample - loss: 0.2362 - mae: 0.3623 - mse: 0.2362 - val_loss: 0.2548 - val_mae: 0.3792 - val_mse: 0.2548\n",
      "Epoch 357/1000\n",
      "265/265 [==============================] - 0s 275us/sample - loss: 0.2371 - mae: 0.3636 - mse: 0.2371 - val_loss: 0.2508 - val_mae: 0.3763 - val_mse: 0.2508\n",
      "Epoch 358/1000\n",
      "265/265 [==============================] - 0s 254us/sample - loss: 0.2384 - mae: 0.3598 - mse: 0.2384 - val_loss: 0.2537 - val_mae: 0.3793 - val_mse: 0.2537\n",
      "Epoch 359/1000\n",
      "265/265 [==============================] - 0s 241us/sample - loss: 0.2358 - mae: 0.3616 - mse: 0.2358 - val_loss: 0.2521 - val_mae: 0.3787 - val_mse: 0.2521\n",
      "Epoch 360/1000\n",
      "265/265 [==============================] - 0s 247us/sample - loss: 0.2386 - mae: 0.3589 - mse: 0.2386 - val_loss: 0.2544 - val_mae: 0.3809 - val_mse: 0.2544\n",
      "Epoch 361/1000\n",
      "265/265 [==============================] - 0s 300us/sample - loss: 0.2405 - mae: 0.3717 - mse: 0.2405 - val_loss: 0.2649 - val_mae: 0.3882 - val_mse: 0.2649\n",
      "Epoch 362/1000\n",
      "265/265 [==============================] - 0s 233us/sample - loss: 0.2427 - mae: 0.3572 - mse: 0.2427 - val_loss: 0.2472 - val_mae: 0.3755 - val_mse: 0.2472\n",
      "Epoch 363/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.2328 - mae: 0.3575 - mse: 0.2328 - val_loss: 0.2695 - val_mae: 0.3890 - val_mse: 0.2695\n",
      "Epoch 364/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.2356 - mae: 0.3635 - mse: 0.2356 - val_loss: 0.2496 - val_mae: 0.3763 - val_mse: 0.2496\n",
      "Epoch 365/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 0.2427 - mae: 0.3615 - mse: 0.2427 - val_loss: 0.2662 - val_mae: 0.3892 - val_mse: 0.2662\n",
      "Epoch 366/1000\n",
      "265/265 [==============================] - 0s 236us/sample - loss: 0.2415 - mae: 0.3714 - mse: 0.2415 - val_loss: 0.2467 - val_mae: 0.3758 - val_mse: 0.2467\n",
      "Epoch 367/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.2389 - mae: 0.3576 - mse: 0.2389 - val_loss: 0.2600 - val_mae: 0.3826 - val_mse: 0.2600\n",
      "Epoch 368/1000\n",
      "265/265 [==============================] - 0s 199us/sample - loss: 0.2362 - mae: 0.3664 - mse: 0.2362 - val_loss: 0.2635 - val_mae: 0.3862 - val_mse: 0.2635\n",
      "Epoch 369/1000\n",
      "265/265 [==============================] - 0s 206us/sample - loss: 0.2355 - mae: 0.3566 - mse: 0.2355 - val_loss: 0.2508 - val_mae: 0.3773 - val_mse: 0.2508\n",
      "Epoch 370/1000\n",
      "265/265 [==============================] - 0s 219us/sample - loss: 0.2450 - mae: 0.3783 - mse: 0.2450 - val_loss: 0.2681 - val_mae: 0.3919 - val_mse: 0.2681\n",
      "Epoch 371/1000\n",
      "265/265 [==============================] - 0s 267us/sample - loss: 0.2413 - mae: 0.3589 - mse: 0.2413 - val_loss: 0.2551 - val_mae: 0.3857 - val_mse: 0.2551\n",
      "Epoch 372/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.2335 - mae: 0.3641 - mse: 0.2335 - val_loss: 0.2682 - val_mae: 0.3921 - val_mse: 0.2682\n",
      "Epoch 373/1000\n",
      "265/265 [==============================] - 0s 284us/sample - loss: 0.2356 - mae: 0.3633 - mse: 0.2356 - val_loss: 0.2502 - val_mae: 0.3752 - val_mse: 0.2502\n",
      "Epoch 374/1000\n",
      "265/265 [==============================] - 0s 252us/sample - loss: 0.2340 - mae: 0.3571 - mse: 0.2340 - val_loss: 0.2482 - val_mae: 0.3744 - val_mse: 0.2482\n",
      "Epoch 375/1000\n",
      "265/265 [==============================] - 0s 260us/sample - loss: 0.2329 - mae: 0.3578 - mse: 0.2329 - val_loss: 0.2535 - val_mae: 0.3790 - val_mse: 0.2535\n",
      "Epoch 376/1000\n",
      "265/265 [==============================] - 0s 227us/sample - loss: 0.2342 - mae: 0.3561 - mse: 0.2342 - val_loss: 0.2614 - val_mae: 0.3869 - val_mse: 0.2614\n",
      "Epoch 377/1000\n",
      "265/265 [==============================] - 0s 206us/sample - loss: 0.2485 - mae: 0.3882 - mse: 0.2485 - val_loss: 0.2534 - val_mae: 0.3820 - val_mse: 0.2534\n",
      "Epoch 378/1000\n",
      "265/265 [==============================] - 0s 262us/sample - loss: 0.2425 - mae: 0.3583 - mse: 0.2425 - val_loss: 0.2543 - val_mae: 0.3831 - val_mse: 0.2543\n",
      "Epoch 379/1000\n",
      "265/265 [==============================] - 0s 286us/sample - loss: 0.2334 - mae: 0.3601 - mse: 0.2334 - val_loss: 0.2612 - val_mae: 0.3874 - val_mse: 0.2612\n",
      "Epoch 380/1000\n",
      "265/265 [==============================] - 0s 258us/sample - loss: 0.2350 - mae: 0.3544 - mse: 0.2350 - val_loss: 0.2538 - val_mae: 0.3826 - val_mse: 0.2538\n",
      "Epoch 381/1000\n",
      "265/265 [==============================] - 0s 249us/sample - loss: 0.2313 - mae: 0.3565 - mse: 0.2313 - val_loss: 0.2616 - val_mae: 0.3886 - val_mse: 0.2616\n",
      "Epoch 382/1000\n",
      "265/265 [==============================] - 0s 224us/sample - loss: 0.2348 - mae: 0.3663 - mse: 0.2348 - val_loss: 0.2487 - val_mae: 0.3799 - val_mse: 0.2487\n",
      "Epoch 383/1000\n",
      "265/265 [==============================] - 0s 254us/sample - loss: 0.2367 - mae: 0.3528 - mse: 0.2367 - val_loss: 0.2542 - val_mae: 0.3814 - val_mse: 0.2542\n",
      "Epoch 384/1000\n",
      "265/265 [==============================] - 0s 238us/sample - loss: 0.2329 - mae: 0.3592 - mse: 0.2329 - val_loss: 0.2484 - val_mae: 0.3780 - val_mse: 0.2484\n",
      "Epoch 385/1000\n",
      "265/265 [==============================] - 0s 313us/sample - loss: 0.2320 - mae: 0.3551 - mse: 0.2320 - val_loss: 0.2526 - val_mae: 0.3802 - val_mse: 0.2526\n",
      "Epoch 386/1000\n",
      "265/265 [==============================] - 0s 240us/sample - loss: 0.2323 - mae: 0.3599 - mse: 0.2323 - val_loss: 0.2552 - val_mae: 0.3816 - val_mse: 0.2552\n",
      "Epoch 387/1000\n",
      "265/265 [==============================] - 0s 264us/sample - loss: 0.2391 - mae: 0.3564 - mse: 0.2391 - val_loss: 0.2589 - val_mae: 0.3853 - val_mse: 0.2589\n",
      "Epoch 388/1000\n",
      "265/265 [==============================] - 0s 257us/sample - loss: 0.2391 - mae: 0.3697 - mse: 0.2391 - val_loss: 0.2553 - val_mae: 0.3830 - val_mse: 0.2553\n",
      "Epoch 389/1000\n",
      "265/265 [==============================] - 0s 238us/sample - loss: 0.2326 - mae: 0.3629 - mse: 0.2326 - val_loss: 0.2476 - val_mae: 0.3751 - val_mse: 0.2476\n",
      "Epoch 390/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.2416 - mae: 0.3546 - mse: 0.2416 - val_loss: 0.2618 - val_mae: 0.3853 - val_mse: 0.2618\n",
      "Epoch 391/1000\n",
      "265/265 [==============================] - 0s 267us/sample - loss: 0.2448 - mae: 0.3822 - mse: 0.2448 - val_loss: 0.2426 - val_mae: 0.3717 - val_mse: 0.2426\n",
      "Epoch 392/1000\n",
      "265/265 [==============================] - 0s 241us/sample - loss: 0.2405 - mae: 0.3547 - mse: 0.2405 - val_loss: 0.2599 - val_mae: 0.3820 - val_mse: 0.2599\n",
      "Epoch 393/1000\n",
      "265/265 [==============================] - 0s 313us/sample - loss: 0.2337 - mae: 0.3614 - mse: 0.2337 - val_loss: 0.2477 - val_mae: 0.3748 - val_mse: 0.2477\n",
      "Epoch 394/1000\n",
      "265/265 [==============================] - 0s 243us/sample - loss: 0.2304 - mae: 0.3550 - mse: 0.2304 - val_loss: 0.2543 - val_mae: 0.3814 - val_mse: 0.2543\n",
      "Epoch 395/1000\n",
      "265/265 [==============================] - 0s 265us/sample - loss: 0.2332 - mae: 0.3561 - mse: 0.2332 - val_loss: 0.2571 - val_mae: 0.3810 - val_mse: 0.2571\n",
      "Epoch 396/1000\n",
      "265/265 [==============================] - 0s 244us/sample - loss: 0.2522 - mae: 0.3894 - mse: 0.2522 - val_loss: 0.2454 - val_mae: 0.3735 - val_mse: 0.2454\n",
      "Epoch 397/1000\n",
      "265/265 [==============================] - 0s 197us/sample - loss: 0.2513 - mae: 0.3662 - mse: 0.2513 - val_loss: 0.2609 - val_mae: 0.3852 - val_mse: 0.2609\n",
      "Epoch 398/1000\n",
      "265/265 [==============================] - 0s 250us/sample - loss: 0.2359 - mae: 0.3675 - mse: 0.2359 - val_loss: 0.2457 - val_mae: 0.3762 - val_mse: 0.2457\n",
      "Epoch 399/1000\n",
      "265/265 [==============================] - 0s 271us/sample - loss: 0.2416 - mae: 0.3585 - mse: 0.2416 - val_loss: 0.2665 - val_mae: 0.3876 - val_mse: 0.2665\n",
      "Epoch 400/1000\n",
      "265/265 [==============================] - 0s 264us/sample - loss: 0.2408 - mae: 0.3696 - mse: 0.2408 - val_loss: 0.2549 - val_mae: 0.3796 - val_mse: 0.2549\n",
      "Epoch 401/1000\n",
      "265/265 [==============================] - 0s 262us/sample - loss: 0.2353 - mae: 0.3653 - mse: 0.2353 - val_loss: 0.2509 - val_mae: 0.3763 - val_mse: 0.2509\n",
      "Epoch 402/1000\n",
      "265/265 [==============================] - 0s 241us/sample - loss: 0.2298 - mae: 0.3547 - mse: 0.2298 - val_loss: 0.2564 - val_mae: 0.3783 - val_mse: 0.2564\n",
      "Epoch 403/1000\n",
      "265/265 [==============================] - 0s 259us/sample - loss: 0.2299 - mae: 0.3584 - mse: 0.2299 - val_loss: 0.2482 - val_mae: 0.3746 - val_mse: 0.2482\n",
      "Epoch 404/1000\n",
      "265/265 [==============================] - 0s 268us/sample - loss: 0.2325 - mae: 0.3517 - mse: 0.2325 - val_loss: 0.2528 - val_mae: 0.3769 - val_mse: 0.2528\n",
      "Epoch 405/1000\n",
      "265/265 [==============================] - 0s 267us/sample - loss: 0.2306 - mae: 0.3603 - mse: 0.2306 - val_loss: 0.2472 - val_mae: 0.3765 - val_mse: 0.2472\n",
      "Epoch 406/1000\n",
      "265/265 [==============================] - 0s 218us/sample - loss: 0.2343 - mae: 0.3537 - mse: 0.2343 - val_loss: 0.2476 - val_mae: 0.3775 - val_mse: 0.2476\n",
      "Epoch 407/1000\n",
      "265/265 [==============================] - 0s 226us/sample - loss: 0.2299 - mae: 0.3557 - mse: 0.2299 - val_loss: 0.2494 - val_mae: 0.3794 - val_mse: 0.2494\n",
      "Epoch 408/1000\n",
      "265/265 [==============================] - 0s 298us/sample - loss: 0.2288 - mae: 0.3543 - mse: 0.2288 - val_loss: 0.2497 - val_mae: 0.3785 - val_mse: 0.2497\n",
      "Epoch 409/1000\n",
      "265/265 [==============================] - 0s 234us/sample - loss: 0.2281 - mae: 0.3562 - mse: 0.2281 - val_loss: 0.2492 - val_mae: 0.3781 - val_mse: 0.2492\n",
      "Epoch 410/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.2279 - mae: 0.3515 - mse: 0.2279 - val_loss: 0.2467 - val_mae: 0.3777 - val_mse: 0.2467\n",
      "Epoch 411/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.2307 - mae: 0.3585 - mse: 0.2307 - val_loss: 0.2475 - val_mae: 0.3775 - val_mse: 0.2475\n",
      "Epoch 412/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.2345 - mae: 0.3516 - mse: 0.2345 - val_loss: 0.2582 - val_mae: 0.3840 - val_mse: 0.2582\n",
      "Epoch 413/1000\n",
      "265/265 [==============================] - 0s 231us/sample - loss: 0.2313 - mae: 0.3590 - mse: 0.2313 - val_loss: 0.2550 - val_mae: 0.3809 - val_mse: 0.2550\n",
      "Epoch 414/1000\n",
      "265/265 [==============================] - 0s 241us/sample - loss: 0.2279 - mae: 0.3529 - mse: 0.2279 - val_loss: 0.2424 - val_mae: 0.3719 - val_mse: 0.2424\n",
      "Epoch 415/1000\n",
      "265/265 [==============================] - 0s 262us/sample - loss: 0.2277 - mae: 0.3492 - mse: 0.2277 - val_loss: 0.2495 - val_mae: 0.3778 - val_mse: 0.2495\n",
      "Epoch 416/1000\n",
      "265/265 [==============================] - 0s 237us/sample - loss: 0.2390 - mae: 0.3727 - mse: 0.2390 - val_loss: 0.2389 - val_mae: 0.3729 - val_mse: 0.2389\n",
      "Epoch 417/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.2446 - mae: 0.3556 - mse: 0.2446 - val_loss: 0.2595 - val_mae: 0.3846 - val_mse: 0.2595\n",
      "Epoch 418/1000\n",
      "265/265 [==============================] - 0s 241us/sample - loss: 0.2276 - mae: 0.3536 - mse: 0.2276 - val_loss: 0.2425 - val_mae: 0.3726 - val_mse: 0.2425\n",
      "Epoch 419/1000\n",
      "265/265 [==============================] - 0s 230us/sample - loss: 0.2304 - mae: 0.3595 - mse: 0.2304 - val_loss: 0.2464 - val_mae: 0.3756 - val_mse: 0.2464\n",
      "Epoch 420/1000\n",
      "265/265 [==============================] - 0s 240us/sample - loss: 0.2268 - mae: 0.3513 - mse: 0.2268 - val_loss: 0.2434 - val_mae: 0.3725 - val_mse: 0.2434\n",
      "Epoch 421/1000\n",
      "265/265 [==============================] - 0s 227us/sample - loss: 0.2292 - mae: 0.3521 - mse: 0.2292 - val_loss: 0.2583 - val_mae: 0.3805 - val_mse: 0.2583\n",
      "Epoch 422/1000\n",
      "265/265 [==============================] - 0s 221us/sample - loss: 0.2302 - mae: 0.3567 - mse: 0.2302 - val_loss: 0.2484 - val_mae: 0.3771 - val_mse: 0.2484\n",
      "Epoch 423/1000\n",
      "265/265 [==============================] - 0s 244us/sample - loss: 0.2270 - mae: 0.3512 - mse: 0.2270 - val_loss: 0.2474 - val_mae: 0.3766 - val_mse: 0.2474\n",
      "Epoch 424/1000\n",
      "265/265 [==============================] - 0s 260us/sample - loss: 0.2332 - mae: 0.3638 - mse: 0.2332 - val_loss: 0.2432 - val_mae: 0.3736 - val_mse: 0.2432\n",
      "Epoch 425/1000\n",
      "265/265 [==============================] - 0s 271us/sample - loss: 0.2341 - mae: 0.3570 - mse: 0.2341 - val_loss: 0.2538 - val_mae: 0.3804 - val_mse: 0.2538\n",
      "Epoch 426/1000\n",
      "265/265 [==============================] - 0s 333us/sample - loss: 0.2261 - mae: 0.3520 - mse: 0.2261 - val_loss: 0.2431 - val_mae: 0.3733 - val_mse: 0.2431\n",
      "Epoch 427/1000\n",
      "265/265 [==============================] - 0s 272us/sample - loss: 0.2261 - mae: 0.3558 - mse: 0.2261 - val_loss: 0.2541 - val_mae: 0.3798 - val_mse: 0.2541\n",
      "Epoch 428/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.2257 - mae: 0.3522 - mse: 0.2257 - val_loss: 0.2406 - val_mae: 0.3714 - val_mse: 0.2406\n",
      "Epoch 429/1000\n",
      "265/265 [==============================] - 0s 215us/sample - loss: 0.2296 - mae: 0.3571 - mse: 0.2296 - val_loss: 0.2461 - val_mae: 0.3750 - val_mse: 0.2461\n",
      "Epoch 430/1000\n",
      "265/265 [==============================] - 0s 221us/sample - loss: 0.2276 - mae: 0.3507 - mse: 0.2276 - val_loss: 0.2571 - val_mae: 0.3832 - val_mse: 0.2571\n",
      "Epoch 431/1000\n",
      "265/265 [==============================] - 0s 238us/sample - loss: 0.2303 - mae: 0.3608 - mse: 0.2303 - val_loss: 0.2439 - val_mae: 0.3720 - val_mse: 0.2439\n",
      "Epoch 432/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2252 - mae: 0.3523 - mse: 0.2252 - val_loss: 0.2394 - val_mae: 0.3697 - val_mse: 0.2394\n",
      "Epoch 433/1000\n",
      "265/265 [==============================] - 0s 237us/sample - loss: 0.2263 - mae: 0.3478 - mse: 0.2263 - val_loss: 0.2485 - val_mae: 0.3766 - val_mse: 0.2485\n",
      "Epoch 434/1000\n",
      "265/265 [==============================] - 0s 200us/sample - loss: 0.2255 - mae: 0.3548 - mse: 0.2255 - val_loss: 0.2491 - val_mae: 0.3782 - val_mse: 0.2491\n",
      "Epoch 435/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.2269 - mae: 0.3498 - mse: 0.2269 - val_loss: 0.2541 - val_mae: 0.3826 - val_mse: 0.2541\n",
      "Epoch 436/1000\n",
      "265/265 [==============================] - 0s 193us/sample - loss: 0.2282 - mae: 0.3622 - mse: 0.2282 - val_loss: 0.2455 - val_mae: 0.3727 - val_mse: 0.2455\n",
      "Epoch 437/1000\n",
      "265/265 [==============================] - 0s 225us/sample - loss: 0.2368 - mae: 0.3530 - mse: 0.2368 - val_loss: 0.2551 - val_mae: 0.3789 - val_mse: 0.2551\n",
      "Epoch 438/1000\n",
      "265/265 [==============================] - 0s 248us/sample - loss: 0.2398 - mae: 0.3731 - mse: 0.2398 - val_loss: 0.2389 - val_mae: 0.3715 - val_mse: 0.2389\n",
      "Epoch 439/1000\n",
      "265/265 [==============================] - 0s 239us/sample - loss: 0.2309 - mae: 0.3498 - mse: 0.2309 - val_loss: 0.2553 - val_mae: 0.3818 - val_mse: 0.2553\n",
      "Epoch 440/1000\n",
      "265/265 [==============================] - 0s 196us/sample - loss: 0.2295 - mae: 0.3622 - mse: 0.2295 - val_loss: 0.2408 - val_mae: 0.3731 - val_mse: 0.2408\n",
      "Epoch 441/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.2312 - mae: 0.3519 - mse: 0.2312 - val_loss: 0.2454 - val_mae: 0.3735 - val_mse: 0.2454\n",
      "Epoch 442/1000\n",
      "265/265 [==============================] - 0s 223us/sample - loss: 0.2234 - mae: 0.3486 - mse: 0.2234 - val_loss: 0.2427 - val_mae: 0.3743 - val_mse: 0.2427\n",
      "Epoch 443/1000\n",
      "265/265 [==============================] - 0s 196us/sample - loss: 0.2264 - mae: 0.3502 - mse: 0.2264 - val_loss: 0.2558 - val_mae: 0.3834 - val_mse: 0.2558\n",
      "Epoch 444/1000\n",
      "265/265 [==============================] - 0s 213us/sample - loss: 0.2295 - mae: 0.3576 - mse: 0.2295 - val_loss: 0.2392 - val_mae: 0.3712 - val_mse: 0.2392\n",
      "Epoch 445/1000\n",
      "265/265 [==============================] - 0s 197us/sample - loss: 0.2242 - mae: 0.3493 - mse: 0.2242 - val_loss: 0.2515 - val_mae: 0.3775 - val_mse: 0.2515\n",
      "Epoch 446/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2222 - mae: 0.3478 - mse: 0.2222 - val_loss: 0.2381 - val_mae: 0.3682 - val_mse: 0.2381\n",
      "Epoch 447/1000\n",
      "265/265 [==============================] - 0s 197us/sample - loss: 0.2251 - mae: 0.3490 - mse: 0.2251 - val_loss: 0.2622 - val_mae: 0.3854 - val_mse: 0.2622\n",
      "Epoch 448/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2251 - mae: 0.3562 - mse: 0.2251 - val_loss: 0.2434 - val_mae: 0.3746 - val_mse: 0.2434\n",
      "Epoch 449/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 0.2239 - mae: 0.3468 - mse: 0.2239 - val_loss: 0.2569 - val_mae: 0.3816 - val_mse: 0.2569\n",
      "Epoch 450/1000\n",
      "265/265 [==============================] - 0s 241us/sample - loss: 0.2332 - mae: 0.3642 - mse: 0.2332 - val_loss: 0.2354 - val_mae: 0.3656 - val_mse: 0.2354\n",
      "Epoch 451/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.2243 - mae: 0.3472 - mse: 0.2243 - val_loss: 0.2685 - val_mae: 0.3882 - val_mse: 0.2685\n",
      "Epoch 452/1000\n",
      "265/265 [==============================] - 0s 236us/sample - loss: 0.2320 - mae: 0.3625 - mse: 0.2320 - val_loss: 0.2371 - val_mae: 0.3711 - val_mse: 0.2371\n",
      "Epoch 453/1000\n",
      "265/265 [==============================] - 0s 193us/sample - loss: 0.2238 - mae: 0.3443 - mse: 0.2238 - val_loss: 0.2580 - val_mae: 0.3859 - val_mse: 0.2580\n",
      "Epoch 454/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2242 - mae: 0.3566 - mse: 0.2242 - val_loss: 0.2376 - val_mae: 0.3717 - val_mse: 0.2376\n",
      "Epoch 455/1000\n",
      "265/265 [==============================] - 0s 232us/sample - loss: 0.2305 - mae: 0.3500 - mse: 0.2305 - val_loss: 0.2666 - val_mae: 0.3947 - val_mse: 0.2666\n",
      "Epoch 456/1000\n",
      "265/265 [==============================] - 0s 192us/sample - loss: 0.2488 - mae: 0.3812 - mse: 0.2488 - val_loss: 0.2376 - val_mae: 0.3713 - val_mse: 0.2376\n",
      "Epoch 457/1000\n",
      "265/265 [==============================] - 0s 241us/sample - loss: 0.2249 - mae: 0.3453 - mse: 0.2249 - val_loss: 0.2742 - val_mae: 0.3957 - val_mse: 0.2742\n",
      "Epoch 458/1000\n",
      "265/265 [==============================] - 0s 213us/sample - loss: 0.2312 - mae: 0.3544 - mse: 0.2312 - val_loss: 0.2361 - val_mae: 0.3656 - val_mse: 0.2361\n",
      "Epoch 459/1000\n",
      "265/265 [==============================] - 0s 215us/sample - loss: 0.2230 - mae: 0.3501 - mse: 0.2230 - val_loss: 0.2362 - val_mae: 0.3652 - val_mse: 0.2362\n",
      "Epoch 460/1000\n",
      "265/265 [==============================] - 0s 193us/sample - loss: 0.2222 - mae: 0.3484 - mse: 0.2222 - val_loss: 0.2363 - val_mae: 0.3648 - val_mse: 0.2363\n",
      "Epoch 461/1000\n",
      "265/265 [==============================] - 0s 252us/sample - loss: 0.2229 - mae: 0.3486 - mse: 0.2229 - val_loss: 0.2380 - val_mae: 0.3693 - val_mse: 0.2380\n",
      "Epoch 462/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.2196 - mae: 0.3437 - mse: 0.2196 - val_loss: 0.2572 - val_mae: 0.3804 - val_mse: 0.2572\n",
      "Epoch 463/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.2245 - mae: 0.3558 - mse: 0.2245 - val_loss: 0.2351 - val_mae: 0.3652 - val_mse: 0.2351\n",
      "Epoch 464/1000\n",
      "265/265 [==============================] - 0s 213us/sample - loss: 0.2222 - mae: 0.3452 - mse: 0.2222 - val_loss: 0.2555 - val_mae: 0.3813 - val_mse: 0.2555\n",
      "Epoch 465/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.2229 - mae: 0.3516 - mse: 0.2229 - val_loss: 0.2347 - val_mae: 0.3658 - val_mse: 0.2347\n",
      "Epoch 466/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.2214 - mae: 0.3433 - mse: 0.2214 - val_loss: 0.2593 - val_mae: 0.3850 - val_mse: 0.2593\n",
      "Epoch 467/1000\n",
      "265/265 [==============================] - 0s 215us/sample - loss: 0.2216 - mae: 0.3536 - mse: 0.2216 - val_loss: 0.2329 - val_mae: 0.3652 - val_mse: 0.2329\n",
      "Epoch 468/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2249 - mae: 0.3411 - mse: 0.2249 - val_loss: 0.2729 - val_mae: 0.3955 - val_mse: 0.2729\n",
      "Epoch 469/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.2288 - mae: 0.3602 - mse: 0.2288 - val_loss: 0.2305 - val_mae: 0.3630 - val_mse: 0.2305\n",
      "Epoch 470/1000\n",
      "265/265 [==============================] - 0s 212us/sample - loss: 0.2243 - mae: 0.3420 - mse: 0.2243 - val_loss: 0.2513 - val_mae: 0.3780 - val_mse: 0.2513\n",
      "Epoch 471/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.2251 - mae: 0.3477 - mse: 0.2251 - val_loss: 0.2467 - val_mae: 0.3756 - val_mse: 0.2467\n",
      "Epoch 472/1000\n",
      "265/265 [==============================] - 0s 234us/sample - loss: 0.2188 - mae: 0.3476 - mse: 0.2188 - val_loss: 0.2340 - val_mae: 0.3660 - val_mse: 0.2340\n",
      "Epoch 473/1000\n",
      "265/265 [==============================] - 0s 232us/sample - loss: 0.2229 - mae: 0.3419 - mse: 0.2229 - val_loss: 0.2596 - val_mae: 0.3865 - val_mse: 0.2596\n",
      "Epoch 474/1000\n",
      "265/265 [==============================] - 0s 193us/sample - loss: 0.2211 - mae: 0.3451 - mse: 0.2211 - val_loss: 0.2316 - val_mae: 0.3662 - val_mse: 0.2316\n",
      "Epoch 475/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.2195 - mae: 0.3420 - mse: 0.2195 - val_loss: 0.2399 - val_mae: 0.3729 - val_mse: 0.2399\n",
      "Epoch 476/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2186 - mae: 0.3408 - mse: 0.2186 - val_loss: 0.2394 - val_mae: 0.3724 - val_mse: 0.2394\n",
      "Epoch 477/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2178 - mae: 0.3446 - mse: 0.2178 - val_loss: 0.2408 - val_mae: 0.3724 - val_mse: 0.2408\n",
      "Epoch 478/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.2208 - mae: 0.3492 - mse: 0.2208 - val_loss: 0.2311 - val_mae: 0.3637 - val_mse: 0.2311\n",
      "Epoch 479/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.2199 - mae: 0.3434 - mse: 0.2199 - val_loss: 0.2375 - val_mae: 0.3680 - val_mse: 0.2375\n",
      "Epoch 480/1000\n",
      "265/265 [==============================] - 0s 230us/sample - loss: 0.2186 - mae: 0.3434 - mse: 0.2186 - val_loss: 0.2414 - val_mae: 0.3735 - val_mse: 0.2414\n",
      "Epoch 481/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.2263 - mae: 0.3447 - mse: 0.2263 - val_loss: 0.2729 - val_mae: 0.3978 - val_mse: 0.2729\n",
      "Epoch 482/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.2254 - mae: 0.3600 - mse: 0.2254 - val_loss: 0.2302 - val_mae: 0.3635 - val_mse: 0.2302\n",
      "Epoch 483/1000\n",
      "265/265 [==============================] - 0s 251us/sample - loss: 0.2270 - mae: 0.3463 - mse: 0.2270 - val_loss: 0.2403 - val_mae: 0.3685 - val_mse: 0.2403\n",
      "Epoch 484/1000\n",
      "265/265 [==============================] - 0s 199us/sample - loss: 0.2236 - mae: 0.3458 - mse: 0.2236 - val_loss: 0.2568 - val_mae: 0.3833 - val_mse: 0.2568\n",
      "Epoch 485/1000\n",
      "265/265 [==============================] - 0s 218us/sample - loss: 0.2236 - mae: 0.3460 - mse: 0.2236 - val_loss: 0.2469 - val_mae: 0.3766 - val_mse: 0.2469\n",
      "Epoch 486/1000\n",
      "265/265 [==============================] - 0s 225us/sample - loss: 0.2237 - mae: 0.3580 - mse: 0.2237 - val_loss: 0.2300 - val_mae: 0.3634 - val_mse: 0.2300\n",
      "Epoch 487/1000\n",
      "265/265 [==============================] - 0s 206us/sample - loss: 0.2212 - mae: 0.3476 - mse: 0.2212 - val_loss: 0.2377 - val_mae: 0.3703 - val_mse: 0.2377\n",
      "Epoch 488/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.2217 - mae: 0.3389 - mse: 0.2217 - val_loss: 0.2520 - val_mae: 0.3796 - val_mse: 0.2520\n",
      "Epoch 489/1000\n",
      "265/265 [==============================] - 0s 231us/sample - loss: 0.2201 - mae: 0.3487 - mse: 0.2201 - val_loss: 0.2339 - val_mae: 0.3685 - val_mse: 0.2339\n",
      "Epoch 490/1000\n",
      "265/265 [==============================] - 0s 192us/sample - loss: 0.2171 - mae: 0.3391 - mse: 0.2171 - val_loss: 0.2684 - val_mae: 0.3947 - val_mse: 0.2684\n",
      "Epoch 491/1000\n",
      "265/265 [==============================] - 0s 232us/sample - loss: 0.2253 - mae: 0.3560 - mse: 0.2253 - val_loss: 0.2324 - val_mae: 0.3641 - val_mse: 0.2324\n",
      "Epoch 492/1000\n",
      "265/265 [==============================] - 0s 218us/sample - loss: 0.2180 - mae: 0.3467 - mse: 0.2180 - val_loss: 0.2418 - val_mae: 0.3680 - val_mse: 0.2418\n",
      "Epoch 493/1000\n",
      "265/265 [==============================] - 0s 218us/sample - loss: 0.2291 - mae: 0.3487 - mse: 0.2291 - val_loss: 0.2833 - val_mae: 0.4038 - val_mse: 0.2833\n",
      "Epoch 494/1000\n",
      "265/265 [==============================] - 0s 255us/sample - loss: 0.2214 - mae: 0.3530 - mse: 0.2214 - val_loss: 0.2317 - val_mae: 0.3654 - val_mse: 0.2317\n",
      "Epoch 495/1000\n",
      "265/265 [==============================] - 0s 228us/sample - loss: 0.2291 - mae: 0.3500 - mse: 0.2291 - val_loss: 0.2630 - val_mae: 0.3876 - val_mse: 0.2630\n",
      "Epoch 496/1000\n",
      "265/265 [==============================] - 0s 205us/sample - loss: 0.2170 - mae: 0.3419 - mse: 0.2170 - val_loss: 0.2358 - val_mae: 0.3689 - val_mse: 0.2358\n",
      "Epoch 497/1000\n",
      "265/265 [==============================] - 0s 248us/sample - loss: 0.2148 - mae: 0.3435 - mse: 0.2148 - val_loss: 0.2457 - val_mae: 0.3749 - val_mse: 0.2457\n",
      "Epoch 498/1000\n",
      "265/265 [==============================] - 0s 189us/sample - loss: 0.2155 - mae: 0.3441 - mse: 0.2155 - val_loss: 0.2319 - val_mae: 0.3642 - val_mse: 0.2319\n",
      "Epoch 499/1000\n",
      "265/265 [==============================] - 0s 235us/sample - loss: 0.2217 - mae: 0.3538 - mse: 0.2217 - val_loss: 0.2280 - val_mae: 0.3606 - val_mse: 0.2280\n",
      "Epoch 500/1000\n",
      "265/265 [==============================] - 0s 206us/sample - loss: 0.2225 - mae: 0.3464 - mse: 0.2225 - val_loss: 0.2531 - val_mae: 0.3799 - val_mse: 0.2531\n",
      "Epoch 501/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.2160 - mae: 0.3397 - mse: 0.2160 - val_loss: 0.2296 - val_mae: 0.3637 - val_mse: 0.2296\n",
      "Epoch 502/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.2172 - mae: 0.3424 - mse: 0.2172 - val_loss: 0.2348 - val_mae: 0.3677 - val_mse: 0.2348\n",
      "Epoch 503/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.2234 - mae: 0.3377 - mse: 0.2234 - val_loss: 0.2882 - val_mae: 0.4137 - val_mse: 0.2882\n",
      "Epoch 504/1000\n",
      "265/265 [==============================] - 0s 221us/sample - loss: 0.2244 - mae: 0.3615 - mse: 0.2244 - val_loss: 0.2277 - val_mae: 0.3624 - val_mse: 0.2277\n",
      "Epoch 505/1000\n",
      "265/265 [==============================] - 0s 233us/sample - loss: 0.2203 - mae: 0.3368 - mse: 0.2203 - val_loss: 0.2428 - val_mae: 0.3735 - val_mse: 0.2428\n",
      "Epoch 506/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.2120 - mae: 0.3404 - mse: 0.2120 - val_loss: 0.2262 - val_mae: 0.3615 - val_mse: 0.2262\n",
      "Epoch 507/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2207 - mae: 0.3390 - mse: 0.2207 - val_loss: 0.2466 - val_mae: 0.3794 - val_mse: 0.2466\n",
      "Epoch 508/1000\n",
      "265/265 [==============================] - 0s 193us/sample - loss: 0.2239 - mae: 0.3410 - mse: 0.2239 - val_loss: 0.2436 - val_mae: 0.3809 - val_mse: 0.2436\n",
      "Epoch 509/1000\n",
      "265/265 [==============================] - 0s 191us/sample - loss: 0.2183 - mae: 0.3459 - mse: 0.2183 - val_loss: 0.2252 - val_mae: 0.3620 - val_mse: 0.2252\n",
      "Epoch 510/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.2129 - mae: 0.3351 - mse: 0.2129 - val_loss: 0.2510 - val_mae: 0.3811 - val_mse: 0.2510\n",
      "Epoch 511/1000\n",
      "265/265 [==============================] - 0s 244us/sample - loss: 0.2150 - mae: 0.3451 - mse: 0.2150 - val_loss: 0.2285 - val_mae: 0.3628 - val_mse: 0.2285\n",
      "Epoch 512/1000\n",
      "265/265 [==============================] - 0s 205us/sample - loss: 0.2151 - mae: 0.3377 - mse: 0.2151 - val_loss: 0.2483 - val_mae: 0.3746 - val_mse: 0.2483\n",
      "Epoch 513/1000\n",
      "265/265 [==============================] - 0s 225us/sample - loss: 0.2172 - mae: 0.3500 - mse: 0.2172 - val_loss: 0.2291 - val_mae: 0.3643 - val_mse: 0.2291\n",
      "Epoch 514/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.2223 - mae: 0.3391 - mse: 0.2223 - val_loss: 0.2600 - val_mae: 0.3897 - val_mse: 0.2600\n",
      "Epoch 515/1000\n",
      "265/265 [==============================] - 0s 226us/sample - loss: 0.2160 - mae: 0.3390 - mse: 0.2160 - val_loss: 0.2281 - val_mae: 0.3650 - val_mse: 0.2281\n",
      "Epoch 516/1000\n",
      "265/265 [==============================] - 0s 227us/sample - loss: 0.2149 - mae: 0.3359 - mse: 0.2149 - val_loss: 0.2519 - val_mae: 0.3829 - val_mse: 0.2519\n",
      "Epoch 517/1000\n",
      "265/265 [==============================] - 0s 219us/sample - loss: 0.2228 - mae: 0.3416 - mse: 0.2228 - val_loss: 0.2470 - val_mae: 0.3773 - val_mse: 0.2470\n",
      "Epoch 518/1000\n",
      "265/265 [==============================] - 0s 226us/sample - loss: 0.2251 - mae: 0.3574 - mse: 0.2251 - val_loss: 0.2265 - val_mae: 0.3613 - val_mse: 0.2265\n",
      "Epoch 519/1000\n",
      "265/265 [==============================] - 0s 195us/sample - loss: 0.2105 - mae: 0.3351 - mse: 0.2105 - val_loss: 0.2740 - val_mae: 0.4006 - val_mse: 0.2740\n",
      "Epoch 520/1000\n",
      "265/265 [==============================] - 0s 230us/sample - loss: 0.2250 - mae: 0.3452 - mse: 0.2250 - val_loss: 0.2418 - val_mae: 0.3728 - val_mse: 0.2418\n",
      "Epoch 521/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2155 - mae: 0.3460 - mse: 0.2155 - val_loss: 0.2316 - val_mae: 0.3643 - val_mse: 0.2316\n",
      "Epoch 522/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.2105 - mae: 0.3409 - mse: 0.2105 - val_loss: 0.2523 - val_mae: 0.3805 - val_mse: 0.2523\n",
      "Epoch 523/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.2181 - mae: 0.3561 - mse: 0.2181 - val_loss: 0.2360 - val_mae: 0.3700 - val_mse: 0.2360\n",
      "Epoch 524/1000\n",
      "265/265 [==============================] - 0s 205us/sample - loss: 0.2224 - mae: 0.3453 - mse: 0.2224 - val_loss: 0.3031 - val_mae: 0.4273 - val_mse: 0.3031\n",
      "Epoch 525/1000\n",
      "265/265 [==============================] - 0s 194us/sample - loss: 0.2180 - mae: 0.3534 - mse: 0.2180 - val_loss: 0.2310 - val_mae: 0.3658 - val_mse: 0.2310\n",
      "Epoch 526/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.2326 - mae: 0.3556 - mse: 0.2326 - val_loss: 0.2453 - val_mae: 0.3776 - val_mse: 0.2453\n",
      "Epoch 527/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.2183 - mae: 0.3398 - mse: 0.2183 - val_loss: 0.2403 - val_mae: 0.3760 - val_mse: 0.2403\n",
      "Epoch 528/1000\n",
      "265/265 [==============================] - 0s 225us/sample - loss: 0.2148 - mae: 0.3380 - mse: 0.2148 - val_loss: 0.2495 - val_mae: 0.3804 - val_mse: 0.2495\n",
      "Epoch 529/1000\n",
      "265/265 [==============================] - 0s 239us/sample - loss: 0.2264 - mae: 0.3556 - mse: 0.2264 - val_loss: 0.2261 - val_mae: 0.3605 - val_mse: 0.2261\n",
      "Epoch 530/1000\n",
      "265/265 [==============================] - 0s 222us/sample - loss: 0.2148 - mae: 0.3414 - mse: 0.2148 - val_loss: 0.2413 - val_mae: 0.3709 - val_mse: 0.2413\n",
      "Epoch 531/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.2095 - mae: 0.3363 - mse: 0.2095 - val_loss: 0.2419 - val_mae: 0.3710 - val_mse: 0.2419\n",
      "Epoch 532/1000\n",
      "265/265 [==============================] - 0s 222us/sample - loss: 0.2096 - mae: 0.3392 - mse: 0.2096 - val_loss: 0.2284 - val_mae: 0.3623 - val_mse: 0.2284\n",
      "Epoch 533/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.2174 - mae: 0.3366 - mse: 0.2174 - val_loss: 0.2589 - val_mae: 0.3880 - val_mse: 0.2589\n",
      "Epoch 534/1000\n",
      "265/265 [==============================] - 0s 187us/sample - loss: 0.2095 - mae: 0.3422 - mse: 0.2095 - val_loss: 0.2215 - val_mae: 0.3580 - val_mse: 0.2215\n",
      "Epoch 535/1000\n",
      "265/265 [==============================] - 0s 218us/sample - loss: 0.2113 - mae: 0.3311 - mse: 0.2113 - val_loss: 0.2473 - val_mae: 0.3766 - val_mse: 0.2473\n",
      "Epoch 536/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.2090 - mae: 0.3415 - mse: 0.2090 - val_loss: 0.2239 - val_mae: 0.3600 - val_mse: 0.2239\n",
      "Epoch 537/1000\n",
      "265/265 [==============================] - 0s 224us/sample - loss: 0.2191 - mae: 0.3353 - mse: 0.2191 - val_loss: 0.3359 - val_mae: 0.4547 - val_mse: 0.3359\n",
      "Epoch 538/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.2502 - mae: 0.3829 - mse: 0.2502 - val_loss: 0.2234 - val_mae: 0.3594 - val_mse: 0.2234\n",
      "Epoch 539/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.2300 - mae: 0.3604 - mse: 0.2300 - val_loss: 0.2244 - val_mae: 0.3595 - val_mse: 0.2244\n",
      "Epoch 540/1000\n",
      "265/265 [==============================] - 0s 226us/sample - loss: 0.2286 - mae: 0.3488 - mse: 0.2286 - val_loss: 0.2716 - val_mae: 0.3987 - val_mse: 0.2716\n",
      "Epoch 541/1000\n",
      "265/265 [==============================] - 0s 249us/sample - loss: 0.2243 - mae: 0.3449 - mse: 0.2243 - val_loss: 0.2540 - val_mae: 0.3878 - val_mse: 0.2540\n",
      "Epoch 542/1000\n",
      "265/265 [==============================] - 0s 185us/sample - loss: 0.2235 - mae: 0.3537 - mse: 0.2235 - val_loss: 0.2257 - val_mae: 0.3633 - val_mse: 0.2257\n",
      "Epoch 543/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.2119 - mae: 0.3363 - mse: 0.2119 - val_loss: 0.2393 - val_mae: 0.3712 - val_mse: 0.2393\n",
      "Epoch 544/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.2133 - mae: 0.3375 - mse: 0.2133 - val_loss: 0.2345 - val_mae: 0.3663 - val_mse: 0.2345\n",
      "Epoch 545/1000\n",
      "265/265 [==============================] - 0s 218us/sample - loss: 0.2068 - mae: 0.3382 - mse: 0.2068 - val_loss: 0.2238 - val_mae: 0.3593 - val_mse: 0.2238\n",
      "Epoch 546/1000\n",
      "265/265 [==============================] - 0s 213us/sample - loss: 0.2188 - mae: 0.3331 - mse: 0.2188 - val_loss: 0.2748 - val_mae: 0.4022 - val_mse: 0.2748\n",
      "Epoch 547/1000\n",
      "265/265 [==============================] - 0s 221us/sample - loss: 0.2253 - mae: 0.3491 - mse: 0.2253 - val_loss: 0.2381 - val_mae: 0.3737 - val_mse: 0.2381\n",
      "Epoch 548/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2230 - mae: 0.3582 - mse: 0.2230 - val_loss: 0.2233 - val_mae: 0.3610 - val_mse: 0.2233\n",
      "Epoch 549/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2163 - mae: 0.3404 - mse: 0.2163 - val_loss: 0.2268 - val_mae: 0.3607 - val_mse: 0.2268\n",
      "Epoch 550/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 0.2102 - mae: 0.3293 - mse: 0.2102 - val_loss: 0.2685 - val_mae: 0.3923 - val_mse: 0.2685\n",
      "Epoch 551/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2267 - mae: 0.3527 - mse: 0.2267 - val_loss: 0.2345 - val_mae: 0.3669 - val_mse: 0.2345\n",
      "Epoch 552/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.2127 - mae: 0.3504 - mse: 0.2127 - val_loss: 0.2269 - val_mae: 0.3639 - val_mse: 0.2269\n",
      "Epoch 553/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.2231 - mae: 0.3391 - mse: 0.2231 - val_loss: 0.2446 - val_mae: 0.3757 - val_mse: 0.2446\n",
      "Epoch 554/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.2110 - mae: 0.3317 - mse: 0.2110 - val_loss: 0.2408 - val_mae: 0.3746 - val_mse: 0.2408\n",
      "Epoch 555/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 0.2045 - mae: 0.3384 - mse: 0.2045 - val_loss: 0.2193 - val_mae: 0.3575 - val_mse: 0.2193\n",
      "Epoch 556/1000\n",
      "265/265 [==============================] - 0s 196us/sample - loss: 0.2155 - mae: 0.3339 - mse: 0.2155 - val_loss: 0.2476 - val_mae: 0.3793 - val_mse: 0.2476\n",
      "Epoch 557/1000\n",
      "265/265 [==============================] - 0s 224us/sample - loss: 0.2160 - mae: 0.3345 - mse: 0.2160 - val_loss: 0.2464 - val_mae: 0.3787 - val_mse: 0.2464\n",
      "Epoch 558/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.2107 - mae: 0.3408 - mse: 0.2107 - val_loss: 0.2249 - val_mae: 0.3609 - val_mse: 0.2249\n",
      "Epoch 559/1000\n",
      "265/265 [==============================] - 0s 238us/sample - loss: 0.2092 - mae: 0.3425 - mse: 0.2092 - val_loss: 0.2242 - val_mae: 0.3587 - val_mse: 0.2242\n",
      "Epoch 560/1000\n",
      "265/265 [==============================] - 0s 192us/sample - loss: 0.2106 - mae: 0.3336 - mse: 0.2106 - val_loss: 0.2552 - val_mae: 0.3827 - val_mse: 0.2552\n",
      "Epoch 561/1000\n",
      "265/265 [==============================] - 0s 194us/sample - loss: 0.2066 - mae: 0.3405 - mse: 0.2066 - val_loss: 0.2257 - val_mae: 0.3629 - val_mse: 0.2257\n",
      "Epoch 562/1000\n",
      "265/265 [==============================] - 0s 235us/sample - loss: 0.2062 - mae: 0.3321 - mse: 0.2062 - val_loss: 0.2459 - val_mae: 0.3775 - val_mse: 0.2459\n",
      "Epoch 563/1000\n",
      "265/265 [==============================] - 0s 189us/sample - loss: 0.2106 - mae: 0.3348 - mse: 0.2106 - val_loss: 0.2239 - val_mae: 0.3606 - val_mse: 0.2239\n",
      "Epoch 564/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.2033 - mae: 0.3294 - mse: 0.2033 - val_loss: 0.2389 - val_mae: 0.3728 - val_mse: 0.2389\n",
      "Epoch 565/1000\n",
      "265/265 [==============================] - 0s 181us/sample - loss: 0.2107 - mae: 0.3294 - mse: 0.2107 - val_loss: 0.2503 - val_mae: 0.3809 - val_mse: 0.2503\n",
      "Epoch 566/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.2082 - mae: 0.3341 - mse: 0.2082 - val_loss: 0.2292 - val_mae: 0.3643 - val_mse: 0.2292\n",
      "Epoch 567/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.2072 - mae: 0.3327 - mse: 0.2072 - val_loss: 0.2253 - val_mae: 0.3588 - val_mse: 0.2253\n",
      "Epoch 568/1000\n",
      "265/265 [==============================] - 0s 222us/sample - loss: 0.2045 - mae: 0.3347 - mse: 0.2045 - val_loss: 0.2387 - val_mae: 0.3666 - val_mse: 0.2387\n",
      "Epoch 569/1000\n",
      "265/265 [==============================] - 0s 248us/sample - loss: 0.2082 - mae: 0.3358 - mse: 0.2082 - val_loss: 0.2503 - val_mae: 0.3761 - val_mse: 0.2503\n",
      "Epoch 570/1000\n",
      "265/265 [==============================] - 0s 259us/sample - loss: 0.2192 - mae: 0.3573 - mse: 0.2192 - val_loss: 0.2210 - val_mae: 0.3589 - val_mse: 0.2210\n",
      "Epoch 571/1000\n",
      "265/265 [==============================] - 0s 226us/sample - loss: 0.2008 - mae: 0.3273 - mse: 0.2008 - val_loss: 0.2687 - val_mae: 0.3956 - val_mse: 0.2687\n",
      "Epoch 572/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.2031 - mae: 0.3350 - mse: 0.2031 - val_loss: 0.2200 - val_mae: 0.3577 - val_mse: 0.2200\n",
      "Epoch 573/1000\n",
      "265/265 [==============================] - 0s 185us/sample - loss: 0.2121 - mae: 0.3403 - mse: 0.2121 - val_loss: 0.2413 - val_mae: 0.3699 - val_mse: 0.2413\n",
      "Epoch 574/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.2090 - mae: 0.3320 - mse: 0.2090 - val_loss: 0.2813 - val_mae: 0.4049 - val_mse: 0.2813\n",
      "Epoch 575/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.2504 - mae: 0.3792 - mse: 0.2504 - val_loss: 0.2273 - val_mae: 0.3643 - val_mse: 0.2273\n",
      "Epoch 576/1000\n",
      "265/265 [==============================] - 0s 194us/sample - loss: 0.2242 - mae: 0.3560 - mse: 0.2242 - val_loss: 0.2285 - val_mae: 0.3658 - val_mse: 0.2285\n",
      "Epoch 577/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.2249 - mae: 0.3572 - mse: 0.2249 - val_loss: 0.2342 - val_mae: 0.3681 - val_mse: 0.2342\n",
      "Epoch 578/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.2007 - mae: 0.3263 - mse: 0.2007 - val_loss: 0.2263 - val_mae: 0.3627 - val_mse: 0.2263\n",
      "Epoch 579/1000\n",
      "265/265 [==============================] - 0s 241us/sample - loss: 0.2017 - mae: 0.3291 - mse: 0.2017 - val_loss: 0.2230 - val_mae: 0.3607 - val_mse: 0.2230\n",
      "Epoch 580/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 0.2038 - mae: 0.3269 - mse: 0.2038 - val_loss: 0.2381 - val_mae: 0.3677 - val_mse: 0.2381\n",
      "Epoch 581/1000\n",
      "265/265 [==============================] - 0s 239us/sample - loss: 0.2033 - mae: 0.3348 - mse: 0.2033 - val_loss: 0.2225 - val_mae: 0.3606 - val_mse: 0.2225\n",
      "Epoch 582/1000\n",
      "265/265 [==============================] - 0s 195us/sample - loss: 0.2150 - mae: 0.3367 - mse: 0.2150 - val_loss: 0.2441 - val_mae: 0.3752 - val_mse: 0.2441\n",
      "Epoch 583/1000\n",
      "265/265 [==============================] - 0s 196us/sample - loss: 0.2026 - mae: 0.3253 - mse: 0.2026 - val_loss: 0.2323 - val_mae: 0.3672 - val_mse: 0.2323\n",
      "Epoch 584/1000\n",
      "265/265 [==============================] - 0s 213us/sample - loss: 0.2080 - mae: 0.3441 - mse: 0.2080 - val_loss: 0.2231 - val_mae: 0.3618 - val_mse: 0.2231\n",
      "Epoch 585/1000\n",
      "265/265 [==============================] - 0s 225us/sample - loss: 0.2125 - mae: 0.3427 - mse: 0.2125 - val_loss: 0.2249 - val_mae: 0.3614 - val_mse: 0.2249\n",
      "Epoch 586/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.2036 - mae: 0.3294 - mse: 0.2036 - val_loss: 0.2465 - val_mae: 0.3763 - val_mse: 0.2465\n",
      "Epoch 587/1000\n",
      "265/265 [==============================] - 0s 228us/sample - loss: 0.2038 - mae: 0.3271 - mse: 0.2038 - val_loss: 0.2384 - val_mae: 0.3678 - val_mse: 0.2384\n",
      "Epoch 588/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.2079 - mae: 0.3352 - mse: 0.2079 - val_loss: 0.2342 - val_mae: 0.3649 - val_mse: 0.2342\n",
      "Epoch 589/1000\n",
      "265/265 [==============================] - 0s 228us/sample - loss: 0.2091 - mae: 0.3421 - mse: 0.2091 - val_loss: 0.2226 - val_mae: 0.3618 - val_mse: 0.2226\n",
      "Epoch 590/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.2218 - mae: 0.3413 - mse: 0.2218 - val_loss: 0.2348 - val_mae: 0.3661 - val_mse: 0.2348\n",
      "Epoch 591/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.2032 - mae: 0.3256 - mse: 0.2032 - val_loss: 0.2281 - val_mae: 0.3645 - val_mse: 0.2281\n",
      "Epoch 592/1000\n",
      "265/265 [==============================] - 0s 218us/sample - loss: 0.2028 - mae: 0.3280 - mse: 0.2028 - val_loss: 0.2242 - val_mae: 0.3602 - val_mse: 0.2242\n",
      "Epoch 593/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2059 - mae: 0.3281 - mse: 0.2059 - val_loss: 0.2463 - val_mae: 0.3762 - val_mse: 0.2463\n",
      "Epoch 594/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.2102 - mae: 0.3348 - mse: 0.2102 - val_loss: 0.2551 - val_mae: 0.3834 - val_mse: 0.2551\n",
      "Epoch 595/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2004 - mae: 0.3324 - mse: 0.2004 - val_loss: 0.2204 - val_mae: 0.3591 - val_mse: 0.2204\n",
      "Epoch 596/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.2109 - mae: 0.3283 - mse: 0.2109 - val_loss: 0.2931 - val_mae: 0.4168 - val_mse: 0.2931\n",
      "Epoch 597/1000\n",
      "265/265 [==============================] - 0s 223us/sample - loss: 0.2121 - mae: 0.3439 - mse: 0.2121 - val_loss: 0.2208 - val_mae: 0.3555 - val_mse: 0.2208\n",
      "Epoch 598/1000\n",
      "265/265 [==============================] - 0s 212us/sample - loss: 0.2028 - mae: 0.3339 - mse: 0.2028 - val_loss: 0.2191 - val_mae: 0.3571 - val_mse: 0.2191\n",
      "Epoch 599/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.2094 - mae: 0.3366 - mse: 0.2094 - val_loss: 0.2362 - val_mae: 0.3679 - val_mse: 0.2362\n",
      "Epoch 600/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.2061 - mae: 0.3285 - mse: 0.2061 - val_loss: 0.2686 - val_mae: 0.3944 - val_mse: 0.2686\n",
      "Epoch 601/1000\n",
      "265/265 [==============================] - 0s 218us/sample - loss: 0.2037 - mae: 0.3419 - mse: 0.2037 - val_loss: 0.2235 - val_mae: 0.3625 - val_mse: 0.2235\n",
      "Epoch 602/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.2092 - mae: 0.3299 - mse: 0.2092 - val_loss: 0.2903 - val_mae: 0.4135 - val_mse: 0.2903\n",
      "Epoch 603/1000\n",
      "265/265 [==============================] - 0s 212us/sample - loss: 0.2100 - mae: 0.3303 - mse: 0.2100 - val_loss: 0.2288 - val_mae: 0.3625 - val_mse: 0.2288\n",
      "Epoch 604/1000\n",
      "265/265 [==============================] - 0s 222us/sample - loss: 0.2185 - mae: 0.3513 - mse: 0.2185 - val_loss: 0.2293 - val_mae: 0.3668 - val_mse: 0.2293\n",
      "Epoch 605/1000\n",
      "265/265 [==============================] - 0s 233us/sample - loss: 0.2336 - mae: 0.3679 - mse: 0.2336 - val_loss: 0.2227 - val_mae: 0.3614 - val_mse: 0.2227\n",
      "Epoch 606/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.2562 - mae: 0.3733 - mse: 0.2562 - val_loss: 0.3734 - val_mae: 0.4828 - val_mse: 0.3734\n",
      "Epoch 607/1000\n",
      "265/265 [==============================] - 0s 222us/sample - loss: 0.2300 - mae: 0.3511 - mse: 0.2300 - val_loss: 0.2175 - val_mae: 0.3541 - val_mse: 0.2175\n",
      "Epoch 608/1000\n",
      "265/265 [==============================] - 0s 215us/sample - loss: 0.2011 - mae: 0.3325 - mse: 0.2011 - val_loss: 0.2203 - val_mae: 0.3572 - val_mse: 0.2203\n",
      "Epoch 609/1000\n",
      "265/265 [==============================] - 0s 205us/sample - loss: 0.2129 - mae: 0.3366 - mse: 0.2129 - val_loss: 0.2326 - val_mae: 0.3643 - val_mse: 0.2326\n",
      "Epoch 610/1000\n",
      "265/265 [==============================] - 0s 251us/sample - loss: 0.2007 - mae: 0.3253 - mse: 0.2007 - val_loss: 0.2768 - val_mae: 0.3988 - val_mse: 0.2768\n",
      "Epoch 611/1000\n",
      "265/265 [==============================] - 0s 231us/sample - loss: 0.2311 - mae: 0.3654 - mse: 0.2311 - val_loss: 0.2406 - val_mae: 0.3787 - val_mse: 0.2406\n",
      "Epoch 612/1000\n",
      "265/265 [==============================] - 0s 235us/sample - loss: 0.2071 - mae: 0.3368 - mse: 0.2071 - val_loss: 0.2844 - val_mae: 0.4086 - val_mse: 0.2844\n",
      "Epoch 613/1000\n",
      "265/265 [==============================] - 0s 215us/sample - loss: 0.2032 - mae: 0.3395 - mse: 0.2032 - val_loss: 0.2195 - val_mae: 0.3588 - val_mse: 0.2195\n",
      "Epoch 614/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.2050 - mae: 0.3331 - mse: 0.2050 - val_loss: 0.2327 - val_mae: 0.3662 - val_mse: 0.2327\n",
      "Epoch 615/1000\n",
      "265/265 [==============================] - 0s 187us/sample - loss: 0.1979 - mae: 0.3252 - mse: 0.1979 - val_loss: 0.2503 - val_mae: 0.3772 - val_mse: 0.2503\n",
      "Epoch 616/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.2045 - mae: 0.3424 - mse: 0.2045 - val_loss: 0.2231 - val_mae: 0.3627 - val_mse: 0.2231\n",
      "Epoch 617/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2216 - mae: 0.3472 - mse: 0.2216 - val_loss: 0.3006 - val_mae: 0.4238 - val_mse: 0.3006\n",
      "Epoch 618/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.2255 - mae: 0.3484 - mse: 0.2255 - val_loss: 0.2451 - val_mae: 0.3745 - val_mse: 0.2451\n",
      "Epoch 619/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 0.2018 - mae: 0.3274 - mse: 0.2018 - val_loss: 0.2306 - val_mae: 0.3660 - val_mse: 0.2306\n",
      "Epoch 620/1000\n",
      "265/265 [==============================] - 0s 165us/sample - loss: 0.1970 - mae: 0.3289 - mse: 0.1970 - val_loss: 0.2141 - val_mae: 0.3532 - val_mse: 0.2141\n",
      "Epoch 621/1000\n",
      "265/265 [==============================] - 0s 187us/sample - loss: 0.2028 - mae: 0.3259 - mse: 0.2028 - val_loss: 0.2615 - val_mae: 0.3887 - val_mse: 0.2615\n",
      "Epoch 622/1000\n",
      "265/265 [==============================] - 0s 246us/sample - loss: 0.2042 - mae: 0.3354 - mse: 0.2042 - val_loss: 0.2189 - val_mae: 0.3566 - val_mse: 0.2189\n",
      "Epoch 623/1000\n",
      "265/265 [==============================] - 0s 213us/sample - loss: 0.1977 - mae: 0.3253 - mse: 0.1977 - val_loss: 0.2276 - val_mae: 0.3581 - val_mse: 0.2276\n",
      "Epoch 624/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2116 - mae: 0.3365 - mse: 0.2116 - val_loss: 0.2776 - val_mae: 0.4007 - val_mse: 0.2776\n",
      "Epoch 625/1000\n",
      "265/265 [==============================] - 0s 194us/sample - loss: 0.2045 - mae: 0.3272 - mse: 0.2045 - val_loss: 0.2210 - val_mae: 0.3596 - val_mse: 0.2210\n",
      "Epoch 626/1000\n",
      "265/265 [==============================] - 0s 193us/sample - loss: 0.1954 - mae: 0.3228 - mse: 0.1954 - val_loss: 0.2301 - val_mae: 0.3647 - val_mse: 0.2301\n",
      "Epoch 627/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.2001 - mae: 0.3215 - mse: 0.2001 - val_loss: 0.2603 - val_mae: 0.3871 - val_mse: 0.2603\n",
      "Epoch 628/1000\n",
      "265/265 [==============================] - 0s 223us/sample - loss: 0.2072 - mae: 0.3398 - mse: 0.2072 - val_loss: 0.2146 - val_mae: 0.3528 - val_mse: 0.2146\n",
      "Epoch 629/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.1989 - mae: 0.3266 - mse: 0.1989 - val_loss: 0.2196 - val_mae: 0.3568 - val_mse: 0.2196\n",
      "Epoch 630/1000\n",
      "265/265 [==============================] - 0s 263us/sample - loss: 0.2033 - mae: 0.3290 - mse: 0.2033 - val_loss: 0.2606 - val_mae: 0.3851 - val_mse: 0.2606\n",
      "Epoch 631/1000\n",
      "265/265 [==============================] - 0s 197us/sample - loss: 0.1969 - mae: 0.3193 - mse: 0.1969 - val_loss: 0.2181 - val_mae: 0.3540 - val_mse: 0.2181\n",
      "Epoch 632/1000\n",
      "265/265 [==============================] - 0s 199us/sample - loss: 0.1973 - mae: 0.3275 - mse: 0.1973 - val_loss: 0.2282 - val_mae: 0.3610 - val_mse: 0.2282\n",
      "Epoch 633/1000\n",
      "265/265 [==============================] - 0s 215us/sample - loss: 0.1964 - mae: 0.3262 - mse: 0.1964 - val_loss: 0.2222 - val_mae: 0.3592 - val_mse: 0.2222\n",
      "Epoch 634/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.2112 - mae: 0.3323 - mse: 0.2112 - val_loss: 0.3035 - val_mae: 0.4237 - val_mse: 0.3035\n",
      "Epoch 635/1000\n",
      "265/265 [==============================] - 0s 213us/sample - loss: 0.2268 - mae: 0.3555 - mse: 0.2268 - val_loss: 0.2641 - val_mae: 0.3854 - val_mse: 0.2641\n",
      "Epoch 636/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.2255 - mae: 0.3680 - mse: 0.2255 - val_loss: 0.2227 - val_mae: 0.3623 - val_mse: 0.2227\n",
      "Epoch 637/1000\n",
      "265/265 [==============================] - 0s 225us/sample - loss: 0.2152 - mae: 0.3419 - mse: 0.2152 - val_loss: 0.2192 - val_mae: 0.3575 - val_mse: 0.2192\n",
      "Epoch 638/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.2207 - mae: 0.3447 - mse: 0.2207 - val_loss: 0.2887 - val_mae: 0.4126 - val_mse: 0.2887\n",
      "Epoch 639/1000\n",
      "265/265 [==============================] - 0s 200us/sample - loss: 0.1987 - mae: 0.3285 - mse: 0.1987 - val_loss: 0.2155 - val_mae: 0.3562 - val_mse: 0.2155\n",
      "Epoch 640/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.2082 - mae: 0.3357 - mse: 0.2082 - val_loss: 0.2185 - val_mae: 0.3566 - val_mse: 0.2185\n",
      "Epoch 641/1000\n",
      "265/265 [==============================] - 0s 243us/sample - loss: 0.1997 - mae: 0.3267 - mse: 0.1997 - val_loss: 0.2197 - val_mae: 0.3555 - val_mse: 0.2197\n",
      "Epoch 642/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.1961 - mae: 0.3204 - mse: 0.1961 - val_loss: 0.2332 - val_mae: 0.3649 - val_mse: 0.2332\n",
      "Epoch 643/1000\n",
      "265/265 [==============================] - 0s 244us/sample - loss: 0.1968 - mae: 0.3223 - mse: 0.1968 - val_loss: 0.2182 - val_mae: 0.3564 - val_mse: 0.2182\n",
      "Epoch 644/1000\n",
      "265/265 [==============================] - 0s 200us/sample - loss: 0.1940 - mae: 0.3213 - mse: 0.1940 - val_loss: 0.2218 - val_mae: 0.3551 - val_mse: 0.2218\n",
      "Epoch 645/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.1955 - mae: 0.3254 - mse: 0.1955 - val_loss: 0.2139 - val_mae: 0.3527 - val_mse: 0.2139\n",
      "Epoch 646/1000\n",
      "265/265 [==============================] - 0s 232us/sample - loss: 0.1939 - mae: 0.3153 - mse: 0.1939 - val_loss: 0.2882 - val_mae: 0.4125 - val_mse: 0.2882\n",
      "Epoch 647/1000\n",
      "265/265 [==============================] - 0s 222us/sample - loss: 0.2135 - mae: 0.3608 - mse: 0.2135 - val_loss: 0.2171 - val_mae: 0.3563 - val_mse: 0.2171\n",
      "Epoch 648/1000\n",
      "265/265 [==============================] - 0s 187us/sample - loss: 0.1959 - mae: 0.3281 - mse: 0.1959 - val_loss: 0.2212 - val_mae: 0.3535 - val_mse: 0.2212\n",
      "Epoch 649/1000\n",
      "265/265 [==============================] - 0s 239us/sample - loss: 0.1966 - mae: 0.3197 - mse: 0.1966 - val_loss: 0.2171 - val_mae: 0.3535 - val_mse: 0.2171\n",
      "Epoch 650/1000\n",
      "265/265 [==============================] - 0s 252us/sample - loss: 0.2168 - mae: 0.3374 - mse: 0.2168 - val_loss: 0.2220 - val_mae: 0.3570 - val_mse: 0.2220\n",
      "Epoch 651/1000\n",
      "265/265 [==============================] - 0s 286us/sample - loss: 0.1949 - mae: 0.3133 - mse: 0.1949 - val_loss: 0.2445 - val_mae: 0.3746 - val_mse: 0.2445\n",
      "Epoch 652/1000\n",
      "265/265 [==============================] - 0s 226us/sample - loss: 0.2071 - mae: 0.3444 - mse: 0.2071 - val_loss: 0.2170 - val_mae: 0.3532 - val_mse: 0.2170\n",
      "Epoch 653/1000\n",
      "265/265 [==============================] - 0s 269us/sample - loss: 0.1922 - mae: 0.3268 - mse: 0.1922 - val_loss: 0.2201 - val_mae: 0.3560 - val_mse: 0.2201\n",
      "Epoch 654/1000\n",
      "265/265 [==============================] - 0s 243us/sample - loss: 0.2047 - mae: 0.3313 - mse: 0.2047 - val_loss: 0.2365 - val_mae: 0.3652 - val_mse: 0.2365\n",
      "Epoch 655/1000\n",
      "265/265 [==============================] - 0s 222us/sample - loss: 0.1958 - mae: 0.3214 - mse: 0.1958 - val_loss: 0.2314 - val_mae: 0.3614 - val_mse: 0.2314\n",
      "Epoch 656/1000\n",
      "265/265 [==============================] - 0s 246us/sample - loss: 0.2002 - mae: 0.3245 - mse: 0.2002 - val_loss: 0.2506 - val_mae: 0.3794 - val_mse: 0.2506\n",
      "Epoch 657/1000\n",
      "265/265 [==============================] - 0s 218us/sample - loss: 0.1992 - mae: 0.3372 - mse: 0.1992 - val_loss: 0.2124 - val_mae: 0.3531 - val_mse: 0.2124\n",
      "Epoch 658/1000\n",
      "265/265 [==============================] - 0s 258us/sample - loss: 0.2206 - mae: 0.3545 - mse: 0.2206 - val_loss: 0.2480 - val_mae: 0.3827 - val_mse: 0.2480\n",
      "Epoch 659/1000\n",
      "265/265 [==============================] - 0s 238us/sample - loss: 0.2145 - mae: 0.3407 - mse: 0.2145 - val_loss: 0.4434 - val_mae: 0.5384 - val_mse: 0.4434\n",
      "Epoch 660/1000\n",
      "265/265 [==============================] - 0s 274us/sample - loss: 0.2879 - mae: 0.4091 - mse: 0.2879 - val_loss: 0.2351 - val_mae: 0.3646 - val_mse: 0.2351\n",
      "Epoch 661/1000\n",
      "265/265 [==============================] - 0s 227us/sample - loss: 0.2149 - mae: 0.3593 - mse: 0.2149 - val_loss: 0.2129 - val_mae: 0.3538 - val_mse: 0.2129\n",
      "Epoch 662/1000\n",
      "265/265 [==============================] - 0s 259us/sample - loss: 0.1897 - mae: 0.3148 - mse: 0.1897 - val_loss: 0.2547 - val_mae: 0.3807 - val_mse: 0.2547\n",
      "Epoch 663/1000\n",
      "265/265 [==============================] - 0s 221us/sample - loss: 0.2066 - mae: 0.3275 - mse: 0.2066 - val_loss: 0.2682 - val_mae: 0.3951 - val_mse: 0.2682\n",
      "Epoch 664/1000\n",
      "265/265 [==============================] - 0s 252us/sample - loss: 0.2141 - mae: 0.3420 - mse: 0.2141 - val_loss: 0.2356 - val_mae: 0.3661 - val_mse: 0.2356\n",
      "Epoch 665/1000\n",
      "265/265 [==============================] - 0s 244us/sample - loss: 0.2102 - mae: 0.3412 - mse: 0.2102 - val_loss: 0.2204 - val_mae: 0.3599 - val_mse: 0.2204\n",
      "Epoch 666/1000\n",
      "265/265 [==============================] - 0s 248us/sample - loss: 0.1873 - mae: 0.3130 - mse: 0.1873 - val_loss: 0.3046 - val_mae: 0.4229 - val_mse: 0.3046\n",
      "Epoch 667/1000\n",
      "265/265 [==============================] - 0s 234us/sample - loss: 0.2109 - mae: 0.3596 - mse: 0.2109 - val_loss: 0.2173 - val_mae: 0.3581 - val_mse: 0.2173\n",
      "Epoch 668/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2121 - mae: 0.3402 - mse: 0.2121 - val_loss: 0.2147 - val_mae: 0.3493 - val_mse: 0.2147\n",
      "Epoch 669/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.1904 - mae: 0.3213 - mse: 0.1904 - val_loss: 0.2171 - val_mae: 0.3539 - val_mse: 0.2171\n",
      "Epoch 670/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.1932 - mae: 0.3154 - mse: 0.1932 - val_loss: 0.2727 - val_mae: 0.4000 - val_mse: 0.2727\n",
      "Epoch 671/1000\n",
      "265/265 [==============================] - 0s 240us/sample - loss: 0.2102 - mae: 0.3434 - mse: 0.2102 - val_loss: 0.2162 - val_mae: 0.3508 - val_mse: 0.2162\n",
      "Epoch 672/1000\n",
      "265/265 [==============================] - 0s 281us/sample - loss: 0.2026 - mae: 0.3352 - mse: 0.2026 - val_loss: 0.2136 - val_mae: 0.3516 - val_mse: 0.2136\n",
      "Epoch 673/1000\n",
      "265/265 [==============================] - 0s 212us/sample - loss: 0.2028 - mae: 0.3316 - mse: 0.2028 - val_loss: 0.2123 - val_mae: 0.3514 - val_mse: 0.2123\n",
      "Epoch 674/1000\n",
      "265/265 [==============================] - 0s 248us/sample - loss: 0.1979 - mae: 0.3331 - mse: 0.1979 - val_loss: 0.2132 - val_mae: 0.3535 - val_mse: 0.2132\n",
      "Epoch 675/1000\n",
      "265/265 [==============================] - 0s 225us/sample - loss: 0.1997 - mae: 0.3218 - mse: 0.1997 - val_loss: 0.2855 - val_mae: 0.4083 - val_mse: 0.2855\n",
      "Epoch 676/1000\n",
      "265/265 [==============================] - 0s 244us/sample - loss: 0.2098 - mae: 0.3413 - mse: 0.2098 - val_loss: 0.2305 - val_mae: 0.3633 - val_mse: 0.2305\n",
      "Epoch 677/1000\n",
      "265/265 [==============================] - 0s 240us/sample - loss: 0.1958 - mae: 0.3372 - mse: 0.1958 - val_loss: 0.2188 - val_mae: 0.3592 - val_mse: 0.2188\n",
      "Epoch 678/1000\n",
      "265/265 [==============================] - 0s 253us/sample - loss: 0.2094 - mae: 0.3398 - mse: 0.2094 - val_loss: 0.2786 - val_mae: 0.4017 - val_mse: 0.2786\n",
      "Epoch 679/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.1993 - mae: 0.3265 - mse: 0.1993 - val_loss: 0.2205 - val_mae: 0.3536 - val_mse: 0.2205\n",
      "Epoch 680/1000\n",
      "265/265 [==============================] - 0s 271us/sample - loss: 0.2032 - mae: 0.3472 - mse: 0.2032 - val_loss: 0.2504 - val_mae: 0.3847 - val_mse: 0.2504\n",
      "Epoch 681/1000\n",
      "265/265 [==============================] - 0s 242us/sample - loss: 0.2400 - mae: 0.3534 - mse: 0.2400 - val_loss: 0.2347 - val_mae: 0.3670 - val_mse: 0.2347\n",
      "Epoch 682/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.1960 - mae: 0.3122 - mse: 0.1960 - val_loss: 0.2527 - val_mae: 0.3781 - val_mse: 0.2527\n",
      "Epoch 683/1000\n",
      "265/265 [==============================] - 0s 269us/sample - loss: 0.1942 - mae: 0.3212 - mse: 0.1942 - val_loss: 0.2106 - val_mae: 0.3483 - val_mse: 0.2106\n",
      "Epoch 684/1000\n",
      "265/265 [==============================] - 0s 223us/sample - loss: 0.1919 - mae: 0.3230 - mse: 0.1919 - val_loss: 0.2131 - val_mae: 0.3492 - val_mse: 0.2131\n",
      "Epoch 685/1000\n",
      "265/265 [==============================] - 0s 238us/sample - loss: 0.1860 - mae: 0.3089 - mse: 0.1860 - val_loss: 0.2796 - val_mae: 0.4015 - val_mse: 0.2796\n",
      "Epoch 686/1000\n",
      "265/265 [==============================] - 0s 268us/sample - loss: 0.2199 - mae: 0.3519 - mse: 0.2199 - val_loss: 0.2181 - val_mae: 0.3546 - val_mse: 0.2181\n",
      "Epoch 687/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.2013 - mae: 0.3365 - mse: 0.2013 - val_loss: 0.2459 - val_mae: 0.3868 - val_mse: 0.2459\n",
      "Epoch 688/1000\n",
      "265/265 [==============================] - 0s 267us/sample - loss: 0.2420 - mae: 0.3691 - mse: 0.2420 - val_loss: 0.3226 - val_mae: 0.4374 - val_mse: 0.3226\n",
      "Epoch 689/1000\n",
      "265/265 [==============================] - 0s 228us/sample - loss: 0.2286 - mae: 0.3603 - mse: 0.2286 - val_loss: 0.2160 - val_mae: 0.3558 - val_mse: 0.2160\n",
      "Epoch 690/1000\n",
      "265/265 [==============================] - 0s 244us/sample - loss: 0.2165 - mae: 0.3457 - mse: 0.2165 - val_loss: 0.2132 - val_mae: 0.3565 - val_mse: 0.2132\n",
      "Epoch 691/1000\n",
      "265/265 [==============================] - 0s 252us/sample - loss: 0.2146 - mae: 0.3509 - mse: 0.2146 - val_loss: 0.2285 - val_mae: 0.3693 - val_mse: 0.2285\n",
      "Epoch 692/1000\n",
      "265/265 [==============================] - 0s 238us/sample - loss: 0.2173 - mae: 0.3484 - mse: 0.2173 - val_loss: 0.3337 - val_mae: 0.4434 - val_mse: 0.3337\n",
      "Epoch 693/1000\n",
      "265/265 [==============================] - 0s 197us/sample - loss: 0.2031 - mae: 0.3389 - mse: 0.2031 - val_loss: 0.2145 - val_mae: 0.3511 - val_mse: 0.2145\n",
      "Epoch 694/1000\n",
      "265/265 [==============================] - 0s 218us/sample - loss: 0.2202 - mae: 0.3576 - mse: 0.2202 - val_loss: 0.2279 - val_mae: 0.3696 - val_mse: 0.2279\n",
      "Epoch 695/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.1903 - mae: 0.3170 - mse: 0.1903 - val_loss: 0.3086 - val_mae: 0.4248 - val_mse: 0.3086\n",
      "Epoch 696/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.2087 - mae: 0.3359 - mse: 0.2087 - val_loss: 0.2395 - val_mae: 0.3675 - val_mse: 0.2395\n",
      "Epoch 697/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.1896 - mae: 0.3243 - mse: 0.1896 - val_loss: 0.2136 - val_mae: 0.3556 - val_mse: 0.2136\n",
      "Epoch 698/1000\n",
      "265/265 [==============================] - 0s 191us/sample - loss: 0.1930 - mae: 0.3175 - mse: 0.1930 - val_loss: 0.2393 - val_mae: 0.3691 - val_mse: 0.2393\n",
      "Epoch 699/1000\n",
      "265/265 [==============================] - 0s 182us/sample - loss: 0.1880 - mae: 0.3165 - mse: 0.1880 - val_loss: 0.2181 - val_mae: 0.3551 - val_mse: 0.2181\n",
      "Epoch 700/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.1857 - mae: 0.3159 - mse: 0.1857 - val_loss: 0.2179 - val_mae: 0.3528 - val_mse: 0.2179\n",
      "Epoch 701/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.1867 - mae: 0.3127 - mse: 0.1867 - val_loss: 0.2093 - val_mae: 0.3498 - val_mse: 0.2093\n",
      "Epoch 702/1000\n",
      "265/265 [==============================] - 0s 188us/sample - loss: 0.2075 - mae: 0.3351 - mse: 0.2075 - val_loss: 0.2077 - val_mae: 0.3478 - val_mse: 0.2077\n",
      "Epoch 703/1000\n",
      "265/265 [==============================] - 0s 215us/sample - loss: 0.2067 - mae: 0.3293 - mse: 0.2067 - val_loss: 0.2673 - val_mae: 0.3902 - val_mse: 0.2673\n",
      "Epoch 704/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.1931 - mae: 0.3138 - mse: 0.1931 - val_loss: 0.2273 - val_mae: 0.3619 - val_mse: 0.2273\n",
      "Epoch 705/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2105 - mae: 0.3492 - mse: 0.2105 - val_loss: 0.2153 - val_mae: 0.3570 - val_mse: 0.2153\n",
      "Epoch 706/1000\n",
      "265/265 [==============================] - 0s 161us/sample - loss: 0.2016 - mae: 0.3322 - mse: 0.2016 - val_loss: 0.2095 - val_mae: 0.3504 - val_mse: 0.2095\n",
      "Epoch 707/1000\n",
      "265/265 [==============================] - 0s 269us/sample - loss: 0.1967 - mae: 0.3233 - mse: 0.1967 - val_loss: 0.2076 - val_mae: 0.3488 - val_mse: 0.2076\n",
      "Epoch 708/1000\n",
      "265/265 [==============================] - 0s 222us/sample - loss: 0.2145 - mae: 0.3386 - mse: 0.2145 - val_loss: 0.2578 - val_mae: 0.3802 - val_mse: 0.2578\n",
      "Epoch 709/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.2027 - mae: 0.3213 - mse: 0.2027 - val_loss: 0.2802 - val_mae: 0.4037 - val_mse: 0.2802\n",
      "Epoch 710/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.2002 - mae: 0.3323 - mse: 0.2002 - val_loss: 0.2243 - val_mae: 0.3642 - val_mse: 0.2243\n",
      "Epoch 711/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.1978 - mae: 0.3290 - mse: 0.1978 - val_loss: 0.2290 - val_mae: 0.3586 - val_mse: 0.2290\n",
      "Epoch 712/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.1906 - mae: 0.3120 - mse: 0.1906 - val_loss: 0.2342 - val_mae: 0.3618 - val_mse: 0.2342\n",
      "Epoch 713/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.1891 - mae: 0.3238 - mse: 0.1891 - val_loss: 0.2242 - val_mae: 0.3608 - val_mse: 0.2242\n",
      "Epoch 714/1000\n",
      "265/265 [==============================] - 0s 175us/sample - loss: 0.1858 - mae: 0.3119 - mse: 0.1858 - val_loss: 0.2361 - val_mae: 0.3669 - val_mse: 0.2361\n",
      "Epoch 715/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.1967 - mae: 0.3250 - mse: 0.1967 - val_loss: 0.2156 - val_mae: 0.3519 - val_mse: 0.2156\n",
      "Epoch 716/1000\n",
      "265/265 [==============================] - 0s 230us/sample - loss: 0.1871 - mae: 0.3200 - mse: 0.1871 - val_loss: 0.2076 - val_mae: 0.3495 - val_mse: 0.2076\n",
      "Epoch 717/1000\n",
      "265/265 [==============================] - 0s 233us/sample - loss: 0.1937 - mae: 0.3133 - mse: 0.1937 - val_loss: 0.2130 - val_mae: 0.3488 - val_mse: 0.2130\n",
      "Epoch 718/1000\n",
      "265/265 [==============================] - 0s 257us/sample - loss: 0.1866 - mae: 0.3078 - mse: 0.1866 - val_loss: 0.2418 - val_mae: 0.3720 - val_mse: 0.2418\n",
      "Epoch 719/1000\n",
      "265/265 [==============================] - 0s 223us/sample - loss: 0.1887 - mae: 0.3179 - mse: 0.1887 - val_loss: 0.2216 - val_mae: 0.3553 - val_mse: 0.2216\n",
      "Epoch 720/1000\n",
      "265/265 [==============================] - 0s 212us/sample - loss: 0.1850 - mae: 0.3141 - mse: 0.1850 - val_loss: 0.2109 - val_mae: 0.3504 - val_mse: 0.2109\n",
      "Epoch 721/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.1880 - mae: 0.3171 - mse: 0.1880 - val_loss: 0.2111 - val_mae: 0.3497 - val_mse: 0.2111\n",
      "Epoch 722/1000\n",
      "265/265 [==============================] - 0s 235us/sample - loss: 0.1897 - mae: 0.3183 - mse: 0.1897 - val_loss: 0.2150 - val_mae: 0.3481 - val_mse: 0.2150\n",
      "Epoch 723/1000\n",
      "265/265 [==============================] - 0s 234us/sample - loss: 0.1874 - mae: 0.3111 - mse: 0.1874 - val_loss: 0.2820 - val_mae: 0.4038 - val_mse: 0.2820\n",
      "Epoch 724/1000\n",
      "265/265 [==============================] - 0s 213us/sample - loss: 0.2139 - mae: 0.3439 - mse: 0.2139 - val_loss: 0.2315 - val_mae: 0.3616 - val_mse: 0.2315\n",
      "Epoch 725/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.1867 - mae: 0.3198 - mse: 0.1867 - val_loss: 0.2130 - val_mae: 0.3513 - val_mse: 0.2130\n",
      "Epoch 726/1000\n",
      "265/265 [==============================] - 0s 239us/sample - loss: 0.1857 - mae: 0.3132 - mse: 0.1857 - val_loss: 0.2393 - val_mae: 0.3685 - val_mse: 0.2393\n",
      "Epoch 727/1000\n",
      "265/265 [==============================] - 0s 196us/sample - loss: 0.1872 - mae: 0.3142 - mse: 0.1872 - val_loss: 0.2152 - val_mae: 0.3500 - val_mse: 0.2152\n",
      "Epoch 728/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.1852 - mae: 0.3153 - mse: 0.1852 - val_loss: 0.2082 - val_mae: 0.3478 - val_mse: 0.2082\n",
      "Epoch 729/1000\n",
      "265/265 [==============================] - 0s 231us/sample - loss: 0.1861 - mae: 0.3101 - mse: 0.1861 - val_loss: 0.2401 - val_mae: 0.3674 - val_mse: 0.2401\n",
      "Epoch 730/1000\n",
      "265/265 [==============================] - 0s 226us/sample - loss: 0.1931 - mae: 0.3225 - mse: 0.1931 - val_loss: 0.2301 - val_mae: 0.3586 - val_mse: 0.2301\n",
      "Epoch 731/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.1969 - mae: 0.3257 - mse: 0.1969 - val_loss: 0.2138 - val_mae: 0.3491 - val_mse: 0.2138\n",
      "Epoch 732/1000\n",
      "265/265 [==============================] - 0s 212us/sample - loss: 0.1829 - mae: 0.3120 - mse: 0.1829 - val_loss: 0.2145 - val_mae: 0.3506 - val_mse: 0.2145\n",
      "Epoch 733/1000\n",
      "265/265 [==============================] - 0s 199us/sample - loss: 0.1999 - mae: 0.3267 - mse: 0.1999 - val_loss: 0.2092 - val_mae: 0.3493 - val_mse: 0.2092\n",
      "Epoch 734/1000\n",
      "265/265 [==============================] - 0s 262us/sample - loss: 0.1981 - mae: 0.3138 - mse: 0.1981 - val_loss: 0.2438 - val_mae: 0.3745 - val_mse: 0.2438\n",
      "Epoch 735/1000\n",
      "265/265 [==============================] - 0s 183us/sample - loss: 0.1886 - mae: 0.3182 - mse: 0.1886 - val_loss: 0.2001 - val_mae: 0.3428 - val_mse: 0.2001\n",
      "Epoch 736/1000\n",
      "265/265 [==============================] - 0s 197us/sample - loss: 0.1880 - mae: 0.3183 - mse: 0.1880 - val_loss: 0.2024 - val_mae: 0.3432 - val_mse: 0.2024\n",
      "Epoch 737/1000\n",
      "265/265 [==============================] - 0s 191us/sample - loss: 0.1879 - mae: 0.3103 - mse: 0.1879 - val_loss: 0.2554 - val_mae: 0.3827 - val_mse: 0.2554\n",
      "Epoch 738/1000\n",
      "265/265 [==============================] - 0s 191us/sample - loss: 0.1901 - mae: 0.3123 - mse: 0.1901 - val_loss: 0.2336 - val_mae: 0.3635 - val_mse: 0.2336\n",
      "Epoch 739/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.1904 - mae: 0.3155 - mse: 0.1904 - val_loss: 0.2103 - val_mae: 0.3488 - val_mse: 0.2103\n",
      "Epoch 740/1000\n",
      "265/265 [==============================] - 0s 195us/sample - loss: 0.1855 - mae: 0.3159 - mse: 0.1855 - val_loss: 0.2106 - val_mae: 0.3509 - val_mse: 0.2106\n",
      "Epoch 741/1000\n",
      "265/265 [==============================] - 0s 212us/sample - loss: 0.1848 - mae: 0.3097 - mse: 0.1848 - val_loss: 0.2831 - val_mae: 0.4029 - val_mse: 0.2831\n",
      "Epoch 742/1000\n",
      "265/265 [==============================] - 0s 212us/sample - loss: 0.2021 - mae: 0.3446 - mse: 0.2021 - val_loss: 0.2128 - val_mae: 0.3553 - val_mse: 0.2128\n",
      "Epoch 743/1000\n",
      "265/265 [==============================] - 0s 199us/sample - loss: 0.1988 - mae: 0.3326 - mse: 0.1988 - val_loss: 0.2085 - val_mae: 0.3461 - val_mse: 0.2085\n",
      "Epoch 744/1000\n",
      "265/265 [==============================] - 0s 212us/sample - loss: 0.1808 - mae: 0.3067 - mse: 0.1808 - val_loss: 0.2162 - val_mae: 0.3473 - val_mse: 0.2162\n",
      "Epoch 745/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.1841 - mae: 0.3041 - mse: 0.1841 - val_loss: 0.2643 - val_mae: 0.3899 - val_mse: 0.2643\n",
      "Epoch 746/1000\n",
      "265/265 [==============================] - 0s 224us/sample - loss: 0.1986 - mae: 0.3297 - mse: 0.1986 - val_loss: 0.2162 - val_mae: 0.3527 - val_mse: 0.2162\n",
      "Epoch 747/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.1978 - mae: 0.3310 - mse: 0.1978 - val_loss: 0.2133 - val_mae: 0.3470 - val_mse: 0.2133\n",
      "Epoch 748/1000\n",
      "265/265 [==============================] - 0s 195us/sample - loss: 0.1941 - mae: 0.3263 - mse: 0.1941 - val_loss: 0.2070 - val_mae: 0.3510 - val_mse: 0.2070\n",
      "Epoch 749/1000\n",
      "265/265 [==============================] - 0s 234us/sample - loss: 0.1844 - mae: 0.3162 - mse: 0.1844 - val_loss: 0.2479 - val_mae: 0.3770 - val_mse: 0.2479\n",
      "Epoch 750/1000\n",
      "265/265 [==============================] - 0s 213us/sample - loss: 0.1859 - mae: 0.3162 - mse: 0.1859 - val_loss: 0.2505 - val_mae: 0.3735 - val_mse: 0.2505\n",
      "Epoch 751/1000\n",
      "265/265 [==============================] - 0s 235us/sample - loss: 0.1940 - mae: 0.3265 - mse: 0.1940 - val_loss: 0.2115 - val_mae: 0.3507 - val_mse: 0.2115\n",
      "Epoch 752/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 0.1884 - mae: 0.3206 - mse: 0.1884 - val_loss: 0.2077 - val_mae: 0.3474 - val_mse: 0.2077\n",
      "Epoch 753/1000\n",
      "265/265 [==============================] - 0s 189us/sample - loss: 0.1912 - mae: 0.3124 - mse: 0.1912 - val_loss: 0.2462 - val_mae: 0.3726 - val_mse: 0.2462\n",
      "Epoch 754/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.1962 - mae: 0.3120 - mse: 0.1962 - val_loss: 0.2726 - val_mae: 0.3987 - val_mse: 0.2726\n",
      "Epoch 755/1000\n",
      "265/265 [==============================] - 0s 196us/sample - loss: 0.1966 - mae: 0.3259 - mse: 0.1966 - val_loss: 0.2032 - val_mae: 0.3435 - val_mse: 0.2032\n",
      "Epoch 756/1000\n",
      "265/265 [==============================] - 0s 196us/sample - loss: 0.1838 - mae: 0.3092 - mse: 0.1838 - val_loss: 0.2320 - val_mae: 0.3611 - val_mse: 0.2320\n",
      "Epoch 757/1000\n",
      "265/265 [==============================] - 0s 230us/sample - loss: 0.1862 - mae: 0.3057 - mse: 0.1862 - val_loss: 0.2425 - val_mae: 0.3665 - val_mse: 0.2425\n",
      "Epoch 758/1000\n",
      "265/265 [==============================] - 0s 238us/sample - loss: 0.1878 - mae: 0.3181 - mse: 0.1878 - val_loss: 0.2144 - val_mae: 0.3482 - val_mse: 0.2144\n",
      "Epoch 759/1000\n",
      "265/265 [==============================] - 0s 193us/sample - loss: 0.1978 - mae: 0.3353 - mse: 0.1978 - val_loss: 0.2068 - val_mae: 0.3494 - val_mse: 0.2068\n",
      "Epoch 760/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.1850 - mae: 0.3183 - mse: 0.1850 - val_loss: 0.2029 - val_mae: 0.3445 - val_mse: 0.2029\n",
      "Epoch 761/1000\n",
      "265/265 [==============================] - 0s 235us/sample - loss: 0.1852 - mae: 0.3097 - mse: 0.1852 - val_loss: 0.2061 - val_mae: 0.3451 - val_mse: 0.2061\n",
      "Epoch 762/1000\n",
      "265/265 [==============================] - 0s 246us/sample - loss: 0.1892 - mae: 0.3083 - mse: 0.1892 - val_loss: 0.2167 - val_mae: 0.3522 - val_mse: 0.2167\n",
      "Epoch 763/1000\n",
      "265/265 [==============================] - 0s 244us/sample - loss: 0.1807 - mae: 0.3060 - mse: 0.1807 - val_loss: 0.2434 - val_mae: 0.3730 - val_mse: 0.2434\n",
      "Epoch 764/1000\n",
      "265/265 [==============================] - 0s 244us/sample - loss: 0.2018 - mae: 0.3353 - mse: 0.2018 - val_loss: 0.2081 - val_mae: 0.3494 - val_mse: 0.2081\n",
      "Epoch 765/1000\n",
      "265/265 [==============================] - 0s 225us/sample - loss: 0.1912 - mae: 0.3283 - mse: 0.1912 - val_loss: 0.2085 - val_mae: 0.3508 - val_mse: 0.2085\n",
      "Epoch 766/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.1941 - mae: 0.3293 - mse: 0.1941 - val_loss: 0.2113 - val_mae: 0.3467 - val_mse: 0.2113\n",
      "Epoch 767/1000\n",
      "265/265 [==============================] - 0s 228us/sample - loss: 0.1817 - mae: 0.3061 - mse: 0.1817 - val_loss: 0.2130 - val_mae: 0.3486 - val_mse: 0.2130\n",
      "Epoch 768/1000\n",
      "265/265 [==============================] - 0s 271us/sample - loss: 0.1839 - mae: 0.3094 - mse: 0.1839 - val_loss: 0.2184 - val_mae: 0.3512 - val_mse: 0.2184\n",
      "Epoch 769/1000\n",
      "265/265 [==============================] - 0s 231us/sample - loss: 0.1794 - mae: 0.3041 - mse: 0.1794 - val_loss: 0.2883 - val_mae: 0.4075 - val_mse: 0.2883\n",
      "Epoch 770/1000\n",
      "265/265 [==============================] - 0s 274us/sample - loss: 0.2037 - mae: 0.3390 - mse: 0.2037 - val_loss: 0.2169 - val_mae: 0.3493 - val_mse: 0.2169\n",
      "Epoch 771/1000\n",
      "265/265 [==============================] - 0s 249us/sample - loss: 0.1928 - mae: 0.3271 - mse: 0.1928 - val_loss: 0.2035 - val_mae: 0.3442 - val_mse: 0.2035\n",
      "Epoch 772/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.1939 - mae: 0.3342 - mse: 0.1939 - val_loss: 0.2123 - val_mae: 0.3559 - val_mse: 0.2123\n",
      "Epoch 773/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.2105 - mae: 0.3387 - mse: 0.2105 - val_loss: 0.2092 - val_mae: 0.3527 - val_mse: 0.2092\n",
      "Epoch 774/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.1910 - mae: 0.3115 - mse: 0.1910 - val_loss: 0.2404 - val_mae: 0.3679 - val_mse: 0.2404\n",
      "Epoch 775/1000\n",
      "265/265 [==============================] - 0s 245us/sample - loss: 0.1857 - mae: 0.3058 - mse: 0.1857 - val_loss: 0.2271 - val_mae: 0.3555 - val_mse: 0.2271\n",
      "Epoch 776/1000\n",
      "265/265 [==============================] - 0s 233us/sample - loss: 0.1792 - mae: 0.3048 - mse: 0.1792 - val_loss: 0.2286 - val_mae: 0.3602 - val_mse: 0.2286\n",
      "Epoch 777/1000\n",
      "265/265 [==============================] - 0s 228us/sample - loss: 0.1804 - mae: 0.3069 - mse: 0.1804 - val_loss: 0.2242 - val_mae: 0.3595 - val_mse: 0.2242\n",
      "Epoch 778/1000\n",
      "265/265 [==============================] - 0s 266us/sample - loss: 0.1836 - mae: 0.3153 - mse: 0.1836 - val_loss: 0.2099 - val_mae: 0.3454 - val_mse: 0.2099\n",
      "Epoch 779/1000\n",
      "265/265 [==============================] - 0s 227us/sample - loss: 0.1891 - mae: 0.3224 - mse: 0.1891 - val_loss: 0.2146 - val_mae: 0.3473 - val_mse: 0.2146\n",
      "Epoch 780/1000\n",
      "265/265 [==============================] - 0s 182us/sample - loss: 0.1775 - mae: 0.3076 - mse: 0.1775 - val_loss: 0.2017 - val_mae: 0.3448 - val_mse: 0.2017\n",
      "Epoch 781/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.1900 - mae: 0.3180 - mse: 0.1900 - val_loss: 0.2021 - val_mae: 0.3456 - val_mse: 0.2021\n",
      "Epoch 782/1000\n",
      "265/265 [==============================] - 0s 172us/sample - loss: 0.1890 - mae: 0.3224 - mse: 0.1890 - val_loss: 0.2018 - val_mae: 0.3431 - val_mse: 0.2018\n",
      "Epoch 783/1000\n",
      "265/265 [==============================] - 0s 180us/sample - loss: 0.2030 - mae: 0.3328 - mse: 0.2030 - val_loss: 0.2205 - val_mae: 0.3542 - val_mse: 0.2205\n",
      "Epoch 784/1000\n",
      "265/265 [==============================] - 0s 215us/sample - loss: 0.1810 - mae: 0.3047 - mse: 0.1810 - val_loss: 0.2658 - val_mae: 0.3839 - val_mse: 0.2658\n",
      "Epoch 785/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.1846 - mae: 0.3131 - mse: 0.1846 - val_loss: 0.2113 - val_mae: 0.3446 - val_mse: 0.2113\n",
      "Epoch 786/1000\n",
      "265/265 [==============================] - 0s 242us/sample - loss: 0.1774 - mae: 0.3009 - mse: 0.1774 - val_loss: 0.2807 - val_mae: 0.3993 - val_mse: 0.2807\n",
      "Epoch 787/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.1937 - mae: 0.3247 - mse: 0.1937 - val_loss: 0.2128 - val_mae: 0.3487 - val_mse: 0.2128\n",
      "Epoch 788/1000\n",
      "265/265 [==============================] - 0s 225us/sample - loss: 0.1920 - mae: 0.3237 - mse: 0.1920 - val_loss: 0.2010 - val_mae: 0.3439 - val_mse: 0.2010\n",
      "Epoch 789/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.1817 - mae: 0.3110 - mse: 0.1817 - val_loss: 0.2001 - val_mae: 0.3425 - val_mse: 0.2001\n",
      "Epoch 790/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.1785 - mae: 0.3080 - mse: 0.1785 - val_loss: 0.2953 - val_mae: 0.4177 - val_mse: 0.2953\n",
      "Epoch 791/1000\n",
      "265/265 [==============================] - 0s 206us/sample - loss: 0.1816 - mae: 0.3137 - mse: 0.1816 - val_loss: 0.2027 - val_mae: 0.3435 - val_mse: 0.2027\n",
      "Epoch 792/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.1915 - mae: 0.3212 - mse: 0.1915 - val_loss: 0.2047 - val_mae: 0.3516 - val_mse: 0.2047\n",
      "Epoch 793/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.1828 - mae: 0.3150 - mse: 0.1828 - val_loss: 0.2170 - val_mae: 0.3468 - val_mse: 0.2170\n",
      "Epoch 794/1000\n",
      "265/265 [==============================] - 0s 221us/sample - loss: 0.1839 - mae: 0.3136 - mse: 0.1839 - val_loss: 0.2093 - val_mae: 0.3453 - val_mse: 0.2093\n",
      "Epoch 795/1000\n",
      "265/265 [==============================] - 0s 199us/sample - loss: 0.1760 - mae: 0.3042 - mse: 0.1760 - val_loss: 0.2150 - val_mae: 0.3485 - val_mse: 0.2150\n",
      "Epoch 796/1000\n",
      "265/265 [==============================] - 0s 173us/sample - loss: 0.1765 - mae: 0.3005 - mse: 0.1765 - val_loss: 0.2271 - val_mae: 0.3553 - val_mse: 0.2271\n",
      "Epoch 797/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.1792 - mae: 0.3094 - mse: 0.1792 - val_loss: 0.2024 - val_mae: 0.3450 - val_mse: 0.2024\n",
      "Epoch 798/1000\n",
      "265/265 [==============================] - 0s 188us/sample - loss: 0.1954 - mae: 0.3231 - mse: 0.1954 - val_loss: 0.2480 - val_mae: 0.3753 - val_mse: 0.2480\n",
      "Epoch 799/1000\n",
      "265/265 [==============================] - 0s 242us/sample - loss: 0.1873 - mae: 0.3107 - mse: 0.1873 - val_loss: 0.3002 - val_mae: 0.4184 - val_mse: 0.3002\n",
      "Epoch 800/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.1969 - mae: 0.3333 - mse: 0.1969 - val_loss: 0.1997 - val_mae: 0.3378 - val_mse: 0.1997\n",
      "Epoch 801/1000\n",
      "265/265 [==============================] - 0s 218us/sample - loss: 0.1783 - mae: 0.3038 - mse: 0.1783 - val_loss: 0.2193 - val_mae: 0.3503 - val_mse: 0.2193\n",
      "Epoch 802/1000\n",
      "265/265 [==============================] - 0s 195us/sample - loss: 0.1763 - mae: 0.2990 - mse: 0.1763 - val_loss: 0.2211 - val_mae: 0.3544 - val_mse: 0.2211\n",
      "Epoch 803/1000\n",
      "265/265 [==============================] - 0s 221us/sample - loss: 0.1744 - mae: 0.3028 - mse: 0.1744 - val_loss: 0.2061 - val_mae: 0.3451 - val_mse: 0.2061\n",
      "Epoch 804/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.1762 - mae: 0.3077 - mse: 0.1762 - val_loss: 0.2055 - val_mae: 0.3466 - val_mse: 0.2055\n",
      "Epoch 805/1000\n",
      "265/265 [==============================] - 0s 192us/sample - loss: 0.1767 - mae: 0.3075 - mse: 0.1767 - val_loss: 0.2011 - val_mae: 0.3441 - val_mse: 0.2011\n",
      "Epoch 806/1000\n",
      "265/265 [==============================] - 0s 186us/sample - loss: 0.1726 - mae: 0.2967 - mse: 0.1726 - val_loss: 0.2958 - val_mae: 0.4169 - val_mse: 0.2958\n",
      "Epoch 807/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 0.1986 - mae: 0.3313 - mse: 0.1986 - val_loss: 0.2054 - val_mae: 0.3491 - val_mse: 0.2054\n",
      "Epoch 808/1000\n",
      "265/265 [==============================] - 0s 212us/sample - loss: 0.2119 - mae: 0.3523 - mse: 0.2119 - val_loss: 0.2421 - val_mae: 0.3843 - val_mse: 0.2421\n",
      "Epoch 809/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.2190 - mae: 0.3352 - mse: 0.2190 - val_loss: 0.2016 - val_mae: 0.3435 - val_mse: 0.2016\n",
      "Epoch 810/1000\n",
      "265/265 [==============================] - 0s 251us/sample - loss: 0.1837 - mae: 0.3084 - mse: 0.1837 - val_loss: 0.2905 - val_mae: 0.4090 - val_mse: 0.2905\n",
      "Epoch 811/1000\n",
      "265/265 [==============================] - 0s 236us/sample - loss: 0.1864 - mae: 0.3103 - mse: 0.1864 - val_loss: 0.2175 - val_mae: 0.3498 - val_mse: 0.2175\n",
      "Epoch 812/1000\n",
      "265/265 [==============================] - 0s 255us/sample - loss: 0.1923 - mae: 0.3198 - mse: 0.1923 - val_loss: 0.2024 - val_mae: 0.3405 - val_mse: 0.2024\n",
      "Epoch 813/1000\n",
      "265/265 [==============================] - 0s 262us/sample - loss: 0.1802 - mae: 0.3135 - mse: 0.1802 - val_loss: 0.2082 - val_mae: 0.3524 - val_mse: 0.2082\n",
      "Epoch 814/1000\n",
      "265/265 [==============================] - 0s 183us/sample - loss: 0.1962 - mae: 0.3208 - mse: 0.1962 - val_loss: 0.2062 - val_mae: 0.3436 - val_mse: 0.2062\n",
      "Epoch 815/1000\n",
      "265/265 [==============================] - 0s 237us/sample - loss: 0.1904 - mae: 0.3082 - mse: 0.1904 - val_loss: 0.2347 - val_mae: 0.3635 - val_mse: 0.2347\n",
      "Epoch 816/1000\n",
      "265/265 [==============================] - 0s 178us/sample - loss: 0.1780 - mae: 0.3031 - mse: 0.1780 - val_loss: 0.2139 - val_mae: 0.3481 - val_mse: 0.2139\n",
      "Epoch 817/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 0.1754 - mae: 0.3024 - mse: 0.1754 - val_loss: 0.2541 - val_mae: 0.3759 - val_mse: 0.2541\n",
      "Epoch 818/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.1902 - mae: 0.3237 - mse: 0.1902 - val_loss: 0.2002 - val_mae: 0.3398 - val_mse: 0.2002\n",
      "Epoch 819/1000\n",
      "265/265 [==============================] - 0s 190us/sample - loss: 0.1740 - mae: 0.3007 - mse: 0.1740 - val_loss: 0.2069 - val_mae: 0.3439 - val_mse: 0.2069\n",
      "Epoch 820/1000\n",
      "265/265 [==============================] - 0s 196us/sample - loss: 0.1730 - mae: 0.3021 - mse: 0.1730 - val_loss: 0.1985 - val_mae: 0.3400 - val_mse: 0.1985\n",
      "Epoch 821/1000\n",
      "265/265 [==============================] - 0s 186us/sample - loss: 0.1796 - mae: 0.3068 - mse: 0.1796 - val_loss: 0.2079 - val_mae: 0.3435 - val_mse: 0.2079\n",
      "Epoch 822/1000\n",
      "265/265 [==============================] - 0s 195us/sample - loss: 0.1735 - mae: 0.3004 - mse: 0.1735 - val_loss: 0.1993 - val_mae: 0.3400 - val_mse: 0.1993\n",
      "Epoch 823/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.1815 - mae: 0.3110 - mse: 0.1815 - val_loss: 0.2013 - val_mae: 0.3457 - val_mse: 0.2013\n",
      "Epoch 824/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.1980 - mae: 0.3196 - mse: 0.1980 - val_loss: 0.2330 - val_mae: 0.3652 - val_mse: 0.2330\n",
      "Epoch 825/1000\n",
      "265/265 [==============================] - 0s 196us/sample - loss: 0.1770 - mae: 0.3043 - mse: 0.1770 - val_loss: 0.2030 - val_mae: 0.3401 - val_mse: 0.2030\n",
      "Epoch 826/1000\n",
      "265/265 [==============================] - 0s 223us/sample - loss: 0.1712 - mae: 0.2952 - mse: 0.1712 - val_loss: 0.2233 - val_mae: 0.3539 - val_mse: 0.2233\n",
      "Epoch 827/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.1737 - mae: 0.3057 - mse: 0.1737 - val_loss: 0.1970 - val_mae: 0.3417 - val_mse: 0.1970\n",
      "Epoch 828/1000\n",
      "265/265 [==============================] - 0s 224us/sample - loss: 0.1811 - mae: 0.3058 - mse: 0.1811 - val_loss: 0.2622 - val_mae: 0.3862 - val_mse: 0.2622\n",
      "Epoch 829/1000\n",
      "265/265 [==============================] - 0s 234us/sample - loss: 0.1845 - mae: 0.3101 - mse: 0.1845 - val_loss: 0.2595 - val_mae: 0.3821 - val_mse: 0.2595\n",
      "Epoch 830/1000\n",
      "265/265 [==============================] - 0s 268us/sample - loss: 0.1835 - mae: 0.3215 - mse: 0.1835 - val_loss: 0.1973 - val_mae: 0.3384 - val_mse: 0.1973\n",
      "Epoch 831/1000\n",
      "265/265 [==============================] - 0s 245us/sample - loss: 0.1897 - mae: 0.3301 - mse: 0.1897 - val_loss: 0.1982 - val_mae: 0.3427 - val_mse: 0.1982\n",
      "Epoch 832/1000\n",
      "265/265 [==============================] - 0s 252us/sample - loss: 0.1895 - mae: 0.3344 - mse: 0.1895 - val_loss: 0.2024 - val_mae: 0.3473 - val_mse: 0.2024\n",
      "Epoch 833/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.2080 - mae: 0.3358 - mse: 0.2080 - val_loss: 0.2030 - val_mae: 0.3397 - val_mse: 0.2030\n",
      "Epoch 834/1000\n",
      "265/265 [==============================] - 0s 261us/sample - loss: 0.1893 - mae: 0.3031 - mse: 0.1893 - val_loss: 0.2467 - val_mae: 0.3741 - val_mse: 0.2467\n",
      "Epoch 835/1000\n",
      "265/265 [==============================] - 0s 246us/sample - loss: 0.1855 - mae: 0.3036 - mse: 0.1855 - val_loss: 0.2817 - val_mae: 0.4025 - val_mse: 0.2817\n",
      "Epoch 836/1000\n",
      "265/265 [==============================] - 0s 229us/sample - loss: 0.1841 - mae: 0.3213 - mse: 0.1841 - val_loss: 0.2007 - val_mae: 0.3381 - val_mse: 0.2007\n",
      "Epoch 837/1000\n",
      "265/265 [==============================] - 0s 278us/sample - loss: 0.1939 - mae: 0.3258 - mse: 0.1939 - val_loss: 0.1965 - val_mae: 0.3420 - val_mse: 0.1965\n",
      "Epoch 838/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.1836 - mae: 0.3171 - mse: 0.1836 - val_loss: 0.1993 - val_mae: 0.3433 - val_mse: 0.1993\n",
      "Epoch 839/1000\n",
      "265/265 [==============================] - 0s 254us/sample - loss: 0.1774 - mae: 0.3110 - mse: 0.1774 - val_loss: 0.2056 - val_mae: 0.3449 - val_mse: 0.2056\n",
      "Epoch 840/1000\n",
      "265/265 [==============================] - 0s 199us/sample - loss: 0.1725 - mae: 0.2973 - mse: 0.1725 - val_loss: 0.2627 - val_mae: 0.3845 - val_mse: 0.2627\n",
      "Epoch 841/1000\n",
      "265/265 [==============================] - 0s 206us/sample - loss: 0.1850 - mae: 0.3122 - mse: 0.1850 - val_loss: 0.2459 - val_mae: 0.3677 - val_mse: 0.2459\n",
      "Epoch 842/1000\n",
      "265/265 [==============================] - 0s 244us/sample - loss: 0.1821 - mae: 0.3082 - mse: 0.1821 - val_loss: 0.2467 - val_mae: 0.3685 - val_mse: 0.2467\n",
      "Epoch 843/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.1968 - mae: 0.3188 - mse: 0.1968 - val_loss: 0.2010 - val_mae: 0.3381 - val_mse: 0.2010\n",
      "Epoch 844/1000\n",
      "265/265 [==============================] - 0s 224us/sample - loss: 0.2088 - mae: 0.3437 - mse: 0.2088 - val_loss: 0.2094 - val_mae: 0.3522 - val_mse: 0.2094\n",
      "Epoch 845/1000\n",
      "265/265 [==============================] - 0s 247us/sample - loss: 0.1874 - mae: 0.3211 - mse: 0.1874 - val_loss: 0.1963 - val_mae: 0.3397 - val_mse: 0.1963\n",
      "Epoch 846/1000\n",
      "265/265 [==============================] - 0s 236us/sample - loss: 0.1703 - mae: 0.2906 - mse: 0.1703 - val_loss: 0.2847 - val_mae: 0.4013 - val_mse: 0.2847\n",
      "Epoch 847/1000\n",
      "265/265 [==============================] - 0s 279us/sample - loss: 0.1892 - mae: 0.3209 - mse: 0.1892 - val_loss: 0.2062 - val_mae: 0.3373 - val_mse: 0.2062\n",
      "Epoch 848/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.1894 - mae: 0.3240 - mse: 0.1894 - val_loss: 0.1956 - val_mae: 0.3327 - val_mse: 0.1956\n",
      "Epoch 849/1000\n",
      "265/265 [==============================] - 0s 251us/sample - loss: 0.1738 - mae: 0.3067 - mse: 0.1738 - val_loss: 0.2017 - val_mae: 0.3392 - val_mse: 0.2017\n",
      "Epoch 850/1000\n",
      "265/265 [==============================] - 0s 257us/sample - loss: 0.1763 - mae: 0.3084 - mse: 0.1763 - val_loss: 0.2019 - val_mae: 0.3468 - val_mse: 0.2019\n",
      "Epoch 851/1000\n",
      "265/265 [==============================] - 0s 235us/sample - loss: 0.1773 - mae: 0.3012 - mse: 0.1773 - val_loss: 0.2437 - val_mae: 0.3672 - val_mse: 0.2437\n",
      "Epoch 852/1000\n",
      "265/265 [==============================] - 0s 239us/sample - loss: 0.1826 - mae: 0.3022 - mse: 0.1826 - val_loss: 0.2509 - val_mae: 0.3720 - val_mse: 0.2509\n",
      "Epoch 853/1000\n",
      "265/265 [==============================] - 0s 235us/sample - loss: 0.1802 - mae: 0.3114 - mse: 0.1802 - val_loss: 0.2164 - val_mae: 0.3501 - val_mse: 0.2164\n",
      "Epoch 854/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.1781 - mae: 0.3086 - mse: 0.1781 - val_loss: 0.2175 - val_mae: 0.3492 - val_mse: 0.2175\n",
      "Epoch 855/1000\n",
      "265/265 [==============================] - 0s 196us/sample - loss: 0.1698 - mae: 0.3018 - mse: 0.1698 - val_loss: 0.1966 - val_mae: 0.3387 - val_mse: 0.1966\n",
      "Epoch 856/1000\n",
      "265/265 [==============================] - 0s 223us/sample - loss: 0.1726 - mae: 0.2973 - mse: 0.1726 - val_loss: 0.2682 - val_mae: 0.3863 - val_mse: 0.2682\n",
      "Epoch 857/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.2156 - mae: 0.3551 - mse: 0.2156 - val_loss: 0.2031 - val_mae: 0.3446 - val_mse: 0.2031\n",
      "Epoch 858/1000\n",
      "265/265 [==============================] - 0s 223us/sample - loss: 0.1916 - mae: 0.3310 - mse: 0.1916 - val_loss: 0.2150 - val_mae: 0.3600 - val_mse: 0.2150\n",
      "Epoch 859/1000\n",
      "265/265 [==============================] - 0s 200us/sample - loss: 0.1940 - mae: 0.3229 - mse: 0.1940 - val_loss: 0.1964 - val_mae: 0.3409 - val_mse: 0.1964\n",
      "Epoch 860/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.1763 - mae: 0.2977 - mse: 0.1763 - val_loss: 0.2429 - val_mae: 0.3691 - val_mse: 0.2429\n",
      "Epoch 861/1000\n",
      "265/265 [==============================] - 0s 232us/sample - loss: 0.1719 - mae: 0.2949 - mse: 0.1719 - val_loss: 0.2206 - val_mae: 0.3541 - val_mse: 0.2206\n",
      "Epoch 862/1000\n",
      "265/265 [==============================] - 0s 230us/sample - loss: 0.1822 - mae: 0.3199 - mse: 0.1822 - val_loss: 0.2049 - val_mae: 0.3440 - val_mse: 0.2049\n",
      "Epoch 863/1000\n",
      "265/265 [==============================] - 0s 227us/sample - loss: 0.1766 - mae: 0.3073 - mse: 0.1766 - val_loss: 0.1973 - val_mae: 0.3429 - val_mse: 0.1973\n",
      "Epoch 864/1000\n",
      "265/265 [==============================] - 0s 234us/sample - loss: 0.1714 - mae: 0.3046 - mse: 0.1714 - val_loss: 0.1989 - val_mae: 0.3374 - val_mse: 0.1989\n",
      "Epoch 865/1000\n",
      "265/265 [==============================] - 0s 221us/sample - loss: 0.1780 - mae: 0.3064 - mse: 0.1780 - val_loss: 0.2019 - val_mae: 0.3395 - val_mse: 0.2019\n",
      "Epoch 866/1000\n",
      "265/265 [==============================] - 0s 219us/sample - loss: 0.1745 - mae: 0.2983 - mse: 0.1745 - val_loss: 0.2244 - val_mae: 0.3538 - val_mse: 0.2244\n",
      "Epoch 867/1000\n",
      "265/265 [==============================] - 0s 253us/sample - loss: 0.1824 - mae: 0.3117 - mse: 0.1824 - val_loss: 0.2578 - val_mae: 0.3801 - val_mse: 0.2578\n",
      "Epoch 868/1000\n",
      "265/265 [==============================] - 0s 239us/sample - loss: 0.2008 - mae: 0.3273 - mse: 0.2008 - val_loss: 0.3430 - val_mae: 0.4575 - val_mse: 0.3430\n",
      "Epoch 869/1000\n",
      "265/265 [==============================] - 0s 170us/sample - loss: 0.2186 - mae: 0.3535 - mse: 0.2186 - val_loss: 0.2061 - val_mae: 0.3377 - val_mse: 0.2061\n",
      "Epoch 870/1000\n",
      "265/265 [==============================] - 0s 234us/sample - loss: 0.1719 - mae: 0.3052 - mse: 0.1719 - val_loss: 0.2011 - val_mae: 0.3392 - val_mse: 0.2011\n",
      "Epoch 871/1000\n",
      "265/265 [==============================] - 0s 256us/sample - loss: 0.1674 - mae: 0.2938 - mse: 0.1674 - val_loss: 0.2129 - val_mae: 0.3449 - val_mse: 0.2129\n",
      "Epoch 872/1000\n",
      "265/265 [==============================] - 0s 257us/sample - loss: 0.1687 - mae: 0.2985 - mse: 0.1687 - val_loss: 0.2089 - val_mae: 0.3404 - val_mse: 0.2089\n",
      "Epoch 873/1000\n",
      "265/265 [==============================] - 0s 239us/sample - loss: 0.1693 - mae: 0.2942 - mse: 0.1693 - val_loss: 0.2094 - val_mae: 0.3446 - val_mse: 0.2094\n",
      "Epoch 874/1000\n",
      "265/265 [==============================] - 0s 253us/sample - loss: 0.1693 - mae: 0.2940 - mse: 0.1693 - val_loss: 0.2119 - val_mae: 0.3444 - val_mse: 0.2119\n",
      "Epoch 875/1000\n",
      "265/265 [==============================] - 0s 258us/sample - loss: 0.1714 - mae: 0.2992 - mse: 0.1714 - val_loss: 0.1968 - val_mae: 0.3344 - val_mse: 0.1968\n",
      "Epoch 876/1000\n",
      "265/265 [==============================] - 0s 233us/sample - loss: 0.1775 - mae: 0.3086 - mse: 0.1775 - val_loss: 0.1945 - val_mae: 0.3360 - val_mse: 0.1945\n",
      "Epoch 877/1000\n",
      "265/265 [==============================] - 0s 203us/sample - loss: 0.1843 - mae: 0.3147 - mse: 0.1843 - val_loss: 0.2160 - val_mae: 0.3481 - val_mse: 0.2160\n",
      "Epoch 878/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.1711 - mae: 0.2946 - mse: 0.1711 - val_loss: 0.2636 - val_mae: 0.3843 - val_mse: 0.2636\n",
      "Epoch 879/1000\n",
      "265/265 [==============================] - 0s 224us/sample - loss: 0.1842 - mae: 0.3118 - mse: 0.1842 - val_loss: 0.2364 - val_mae: 0.3643 - val_mse: 0.2364\n",
      "Epoch 880/1000\n",
      "265/265 [==============================] - 0s 200us/sample - loss: 0.1719 - mae: 0.3022 - mse: 0.1719 - val_loss: 0.2001 - val_mae: 0.3378 - val_mse: 0.2001\n",
      "Epoch 881/1000\n",
      "265/265 [==============================] - 0s 195us/sample - loss: 0.1696 - mae: 0.2993 - mse: 0.1696 - val_loss: 0.1987 - val_mae: 0.3390 - val_mse: 0.1987\n",
      "Epoch 882/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.1737 - mae: 0.2987 - mse: 0.1737 - val_loss: 0.2307 - val_mae: 0.3586 - val_mse: 0.2307\n",
      "Epoch 883/1000\n",
      "265/265 [==============================] - 0s 199us/sample - loss: 0.1657 - mae: 0.2962 - mse: 0.1657 - val_loss: 0.1997 - val_mae: 0.3353 - val_mse: 0.1997\n",
      "Epoch 884/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.1655 - mae: 0.2887 - mse: 0.1655 - val_loss: 0.2620 - val_mae: 0.3858 - val_mse: 0.2620\n",
      "Epoch 885/1000\n",
      "265/265 [==============================] - 0s 186us/sample - loss: 0.1824 - mae: 0.3253 - mse: 0.1824 - val_loss: 0.1951 - val_mae: 0.3337 - val_mse: 0.1951\n",
      "Epoch 886/1000\n",
      "265/265 [==============================] - 0s 237us/sample - loss: 0.1857 - mae: 0.3127 - mse: 0.1857 - val_loss: 0.1938 - val_mae: 0.3415 - val_mse: 0.1938\n",
      "Epoch 887/1000\n",
      "265/265 [==============================] - 0s 214us/sample - loss: 0.1706 - mae: 0.3021 - mse: 0.1706 - val_loss: 0.1946 - val_mae: 0.3364 - val_mse: 0.1946\n",
      "Epoch 888/1000\n",
      "265/265 [==============================] - 0s 179us/sample - loss: 0.1716 - mae: 0.2998 - mse: 0.1716 - val_loss: 0.1997 - val_mae: 0.3361 - val_mse: 0.1997\n",
      "Epoch 889/1000\n",
      "265/265 [==============================] - 0s 191us/sample - loss: 0.1845 - mae: 0.3088 - mse: 0.1845 - val_loss: 0.2516 - val_mae: 0.3713 - val_mse: 0.2516\n",
      "Epoch 890/1000\n",
      "265/265 [==============================] - 0s 233us/sample - loss: 0.1759 - mae: 0.3004 - mse: 0.1759 - val_loss: 0.2442 - val_mae: 0.3716 - val_mse: 0.2442\n",
      "Epoch 891/1000\n",
      "265/265 [==============================] - 0s 180us/sample - loss: 0.1723 - mae: 0.3018 - mse: 0.1723 - val_loss: 0.1923 - val_mae: 0.3380 - val_mse: 0.1923\n",
      "Epoch 892/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.1685 - mae: 0.3041 - mse: 0.1685 - val_loss: 0.1947 - val_mae: 0.3404 - val_mse: 0.1947\n",
      "Epoch 893/1000\n",
      "265/265 [==============================] - 0s 206us/sample - loss: 0.1741 - mae: 0.2936 - mse: 0.1741 - val_loss: 0.2315 - val_mae: 0.3557 - val_mse: 0.2315\n",
      "Epoch 894/1000\n",
      "265/265 [==============================] - 0s 197us/sample - loss: 0.1732 - mae: 0.3028 - mse: 0.1732 - val_loss: 0.2335 - val_mae: 0.3574 - val_mse: 0.2335\n",
      "Epoch 895/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.1751 - mae: 0.2980 - mse: 0.1751 - val_loss: 0.2245 - val_mae: 0.3548 - val_mse: 0.2245\n",
      "Epoch 896/1000\n",
      "265/265 [==============================] - 0s 224us/sample - loss: 0.1740 - mae: 0.2961 - mse: 0.1740 - val_loss: 0.2695 - val_mae: 0.3931 - val_mse: 0.2695\n",
      "Epoch 897/1000\n",
      "265/265 [==============================] - 0s 185us/sample - loss: 0.1814 - mae: 0.3148 - mse: 0.1814 - val_loss: 0.2128 - val_mae: 0.3484 - val_mse: 0.2128\n",
      "Epoch 898/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.1724 - mae: 0.2987 - mse: 0.1724 - val_loss: 0.2256 - val_mae: 0.3506 - val_mse: 0.2256\n",
      "Epoch 899/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.1821 - mae: 0.3098 - mse: 0.1821 - val_loss: 0.1941 - val_mae: 0.3364 - val_mse: 0.1941\n",
      "Epoch 900/1000\n",
      "265/265 [==============================] - 0s 167us/sample - loss: 0.2007 - mae: 0.3468 - mse: 0.2007 - val_loss: 0.2244 - val_mae: 0.3673 - val_mse: 0.2244\n",
      "Epoch 901/1000\n",
      "265/265 [==============================] - 0s 205us/sample - loss: 0.1943 - mae: 0.3366 - mse: 0.1943 - val_loss: 0.1991 - val_mae: 0.3438 - val_mse: 0.1991\n",
      "Epoch 902/1000\n",
      "265/265 [==============================] - 0s 191us/sample - loss: 0.1875 - mae: 0.3160 - mse: 0.1875 - val_loss: 0.2364 - val_mae: 0.3639 - val_mse: 0.2364\n",
      "Epoch 903/1000\n",
      "265/265 [==============================] - 0s 186us/sample - loss: 0.1698 - mae: 0.2944 - mse: 0.1698 - val_loss: 0.2051 - val_mae: 0.3361 - val_mse: 0.2051\n",
      "Epoch 904/1000\n",
      "265/265 [==============================] - 0s 213us/sample - loss: 0.1652 - mae: 0.2909 - mse: 0.1652 - val_loss: 0.1988 - val_mae: 0.3342 - val_mse: 0.1988\n",
      "Epoch 905/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.1652 - mae: 0.2915 - mse: 0.1652 - val_loss: 0.2202 - val_mae: 0.3540 - val_mse: 0.2202\n",
      "Epoch 906/1000\n",
      "265/265 [==============================] - 0s 205us/sample - loss: 0.1683 - mae: 0.2953 - mse: 0.1683 - val_loss: 0.2094 - val_mae: 0.3392 - val_mse: 0.2094\n",
      "Epoch 907/1000\n",
      "265/265 [==============================] - 0s 225us/sample - loss: 0.1690 - mae: 0.2948 - mse: 0.1690 - val_loss: 0.2839 - val_mae: 0.3997 - val_mse: 0.2839\n",
      "Epoch 908/1000\n",
      "265/265 [==============================] - 0s 247us/sample - loss: 0.2180 - mae: 0.3663 - mse: 0.2180 - val_loss: 0.2130 - val_mae: 0.3565 - val_mse: 0.2130\n",
      "Epoch 909/1000\n",
      "265/265 [==============================] - 0s 223us/sample - loss: 0.1956 - mae: 0.3402 - mse: 0.1956 - val_loss: 0.2009 - val_mae: 0.3474 - val_mse: 0.2009\n",
      "Epoch 910/1000\n",
      "265/265 [==============================] - 0s 257us/sample - loss: 0.1883 - mae: 0.3184 - mse: 0.1883 - val_loss: 0.1910 - val_mae: 0.3408 - val_mse: 0.1910\n",
      "Epoch 911/1000\n",
      "265/265 [==============================] - 0s 269us/sample - loss: 0.1769 - mae: 0.3015 - mse: 0.1769 - val_loss: 0.2601 - val_mae: 0.3821 - val_mse: 0.2601\n",
      "Epoch 912/1000\n",
      "265/265 [==============================] - 0s 241us/sample - loss: 0.1710 - mae: 0.3001 - mse: 0.1710 - val_loss: 0.2106 - val_mae: 0.3419 - val_mse: 0.2106\n",
      "Epoch 913/1000\n",
      "265/265 [==============================] - 0s 235us/sample - loss: 0.1644 - mae: 0.2904 - mse: 0.1644 - val_loss: 0.2250 - val_mae: 0.3548 - val_mse: 0.2250\n",
      "Epoch 914/1000\n",
      "265/265 [==============================] - 0s 257us/sample - loss: 0.1791 - mae: 0.3139 - mse: 0.1791 - val_loss: 0.1893 - val_mae: 0.3351 - val_mse: 0.1893\n",
      "Epoch 915/1000\n",
      "265/265 [==============================] - 0s 236us/sample - loss: 0.1887 - mae: 0.3203 - mse: 0.1887 - val_loss: 0.2136 - val_mae: 0.3586 - val_mse: 0.2136\n",
      "Epoch 916/1000\n",
      "265/265 [==============================] - 0s 228us/sample - loss: 0.1886 - mae: 0.3253 - mse: 0.1886 - val_loss: 0.1968 - val_mae: 0.3333 - val_mse: 0.1968\n",
      "Epoch 917/1000\n",
      "265/265 [==============================] - 0s 274us/sample - loss: 0.1688 - mae: 0.2951 - mse: 0.1688 - val_loss: 0.1912 - val_mae: 0.3397 - val_mse: 0.1912\n",
      "Epoch 918/1000\n",
      "265/265 [==============================] - 0s 266us/sample - loss: 0.1659 - mae: 0.2968 - mse: 0.1659 - val_loss: 0.2253 - val_mae: 0.3598 - val_mse: 0.2253\n",
      "Epoch 919/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.1629 - mae: 0.2985 - mse: 0.1629 - val_loss: 0.1907 - val_mae: 0.3361 - val_mse: 0.1907\n",
      "Epoch 920/1000\n",
      "265/265 [==============================] - 0s 225us/sample - loss: 0.1724 - mae: 0.3004 - mse: 0.1724 - val_loss: 0.1977 - val_mae: 0.3371 - val_mse: 0.1977\n",
      "Epoch 921/1000\n",
      "265/265 [==============================] - 0s 226us/sample - loss: 0.1645 - mae: 0.2877 - mse: 0.1645 - val_loss: 0.2421 - val_mae: 0.3617 - val_mse: 0.2421\n",
      "Epoch 922/1000\n",
      "265/265 [==============================] - 0s 232us/sample - loss: 0.1736 - mae: 0.3056 - mse: 0.1736 - val_loss: 0.2495 - val_mae: 0.3706 - val_mse: 0.2495\n",
      "Epoch 923/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.1795 - mae: 0.3170 - mse: 0.1795 - val_loss: 0.1827 - val_mae: 0.3267 - val_mse: 0.1827\n",
      "Epoch 924/1000\n",
      "265/265 [==============================] - 0s 200us/sample - loss: 0.1694 - mae: 0.2914 - mse: 0.1694 - val_loss: 0.1997 - val_mae: 0.3368 - val_mse: 0.1997\n",
      "Epoch 925/1000\n",
      "265/265 [==============================] - 0s 251us/sample - loss: 0.1650 - mae: 0.2924 - mse: 0.1650 - val_loss: 0.1873 - val_mae: 0.3323 - val_mse: 0.1873\n",
      "Epoch 926/1000\n",
      "265/265 [==============================] - 0s 272us/sample - loss: 0.1738 - mae: 0.3078 - mse: 0.1738 - val_loss: 0.1965 - val_mae: 0.3453 - val_mse: 0.1965\n",
      "Epoch 927/1000\n",
      "265/265 [==============================] - 0s 245us/sample - loss: 0.1831 - mae: 0.3163 - mse: 0.1831 - val_loss: 0.1893 - val_mae: 0.3346 - val_mse: 0.1893\n",
      "Epoch 928/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.1752 - mae: 0.2980 - mse: 0.1752 - val_loss: 0.1940 - val_mae: 0.3340 - val_mse: 0.1940\n",
      "Epoch 929/1000\n",
      "265/265 [==============================] - 0s 228us/sample - loss: 0.1628 - mae: 0.2841 - mse: 0.1628 - val_loss: 0.2043 - val_mae: 0.3349 - val_mse: 0.2043\n",
      "Epoch 930/1000\n",
      "265/265 [==============================] - 0s 227us/sample - loss: 0.1619 - mae: 0.2898 - mse: 0.1619 - val_loss: 0.2098 - val_mae: 0.3397 - val_mse: 0.2098\n",
      "Epoch 931/1000\n",
      "265/265 [==============================] - 0s 191us/sample - loss: 0.1622 - mae: 0.2899 - mse: 0.1622 - val_loss: 0.2143 - val_mae: 0.3445 - val_mse: 0.2143\n",
      "Epoch 932/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.1645 - mae: 0.2947 - mse: 0.1645 - val_loss: 0.1943 - val_mae: 0.3321 - val_mse: 0.1943\n",
      "Epoch 933/1000\n",
      "265/265 [==============================] - 0s 260us/sample - loss: 0.1607 - mae: 0.2887 - mse: 0.1607 - val_loss: 0.1919 - val_mae: 0.3291 - val_mse: 0.1919\n",
      "Epoch 934/1000\n",
      "265/265 [==============================] - 0s 233us/sample - loss: 0.1603 - mae: 0.2896 - mse: 0.1603 - val_loss: 0.2040 - val_mae: 0.3377 - val_mse: 0.2040\n",
      "Epoch 935/1000\n",
      "265/265 [==============================] - 0s 244us/sample - loss: 0.1605 - mae: 0.2901 - mse: 0.1605 - val_loss: 0.2108 - val_mae: 0.3570 - val_mse: 0.2108\n",
      "Epoch 936/1000\n",
      "265/265 [==============================] - 0s 227us/sample - loss: 0.1862 - mae: 0.3108 - mse: 0.1862 - val_loss: 0.2064 - val_mae: 0.3436 - val_mse: 0.2064\n",
      "Epoch 937/1000\n",
      "265/265 [==============================] - 0s 227us/sample - loss: 0.1634 - mae: 0.2915 - mse: 0.1634 - val_loss: 0.2076 - val_mae: 0.3398 - val_mse: 0.2076\n",
      "Epoch 938/1000\n",
      "265/265 [==============================] - 0s 260us/sample - loss: 0.1687 - mae: 0.2988 - mse: 0.1687 - val_loss: 0.1905 - val_mae: 0.3290 - val_mse: 0.1905\n",
      "Epoch 939/1000\n",
      "265/265 [==============================] - 0s 196us/sample - loss: 0.1693 - mae: 0.2918 - mse: 0.1693 - val_loss: 0.2163 - val_mae: 0.3474 - val_mse: 0.2163\n",
      "Epoch 940/1000\n",
      "265/265 [==============================] - 0s 205us/sample - loss: 0.1656 - mae: 0.2961 - mse: 0.1656 - val_loss: 0.1906 - val_mae: 0.3322 - val_mse: 0.1906\n",
      "Epoch 941/1000\n",
      "265/265 [==============================] - 0s 170us/sample - loss: 0.1600 - mae: 0.2890 - mse: 0.1600 - val_loss: 0.1884 - val_mae: 0.3274 - val_mse: 0.1884\n",
      "Epoch 942/1000\n",
      "265/265 [==============================] - 0s 242us/sample - loss: 0.1636 - mae: 0.2912 - mse: 0.1636 - val_loss: 0.1893 - val_mae: 0.3285 - val_mse: 0.1893\n",
      "Epoch 943/1000\n",
      "265/265 [==============================] - 0s 228us/sample - loss: 0.1630 - mae: 0.2892 - mse: 0.1630 - val_loss: 0.1889 - val_mae: 0.3329 - val_mse: 0.1889\n",
      "Epoch 944/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.1646 - mae: 0.2884 - mse: 0.1646 - val_loss: 0.2141 - val_mae: 0.3469 - val_mse: 0.2141\n",
      "Epoch 945/1000\n",
      "265/265 [==============================] - 0s 243us/sample - loss: 0.1638 - mae: 0.2935 - mse: 0.1638 - val_loss: 0.1938 - val_mae: 0.3394 - val_mse: 0.1938\n",
      "Epoch 946/1000\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 0.1863 - mae: 0.3161 - mse: 0.1863 - val_loss: 0.1996 - val_mae: 0.3472 - val_mse: 0.1996\n",
      "Epoch 947/1000\n",
      "265/265 [==============================] - 0s 197us/sample - loss: 0.1761 - mae: 0.3047 - mse: 0.1761 - val_loss: 0.1904 - val_mae: 0.3400 - val_mse: 0.1904\n",
      "Epoch 948/1000\n",
      "265/265 [==============================] - 0s 195us/sample - loss: 0.1651 - mae: 0.2994 - mse: 0.1651 - val_loss: 0.2099 - val_mae: 0.3399 - val_mse: 0.2099\n",
      "Epoch 949/1000\n",
      "265/265 [==============================] - 0s 210us/sample - loss: 0.1741 - mae: 0.2995 - mse: 0.1741 - val_loss: 0.1975 - val_mae: 0.3387 - val_mse: 0.1975\n",
      "Epoch 950/1000\n",
      "265/265 [==============================] - 0s 228us/sample - loss: 0.1623 - mae: 0.2889 - mse: 0.1623 - val_loss: 0.2191 - val_mae: 0.3520 - val_mse: 0.2191\n",
      "Epoch 951/1000\n",
      "265/265 [==============================] - 0s 241us/sample - loss: 0.1671 - mae: 0.2939 - mse: 0.1671 - val_loss: 0.2247 - val_mae: 0.3489 - val_mse: 0.2247\n",
      "Epoch 952/1000\n",
      "265/265 [==============================] - 0s 219us/sample - loss: 0.1674 - mae: 0.2960 - mse: 0.1674 - val_loss: 0.2178 - val_mae: 0.3434 - val_mse: 0.2178\n",
      "Epoch 953/1000\n",
      "265/265 [==============================] - 0s 206us/sample - loss: 0.1677 - mae: 0.2957 - mse: 0.1677 - val_loss: 0.2459 - val_mae: 0.3699 - val_mse: 0.2459\n",
      "Epoch 954/1000\n",
      "265/265 [==============================] - 0s 242us/sample - loss: 0.1642 - mae: 0.3070 - mse: 0.1642 - val_loss: 0.1893 - val_mae: 0.3357 - val_mse: 0.1893\n",
      "Epoch 955/1000\n",
      "265/265 [==============================] - 0s 197us/sample - loss: 0.1849 - mae: 0.3257 - mse: 0.1849 - val_loss: 0.2493 - val_mae: 0.3891 - val_mse: 0.2493\n",
      "Epoch 956/1000\n",
      "265/265 [==============================] - 0s 244us/sample - loss: 0.2522 - mae: 0.3788 - mse: 0.2522 - val_loss: 0.2147 - val_mae: 0.3601 - val_mse: 0.2147\n",
      "Epoch 957/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.2349 - mae: 0.3754 - mse: 0.2349 - val_loss: 0.2345 - val_mae: 0.3652 - val_mse: 0.2345\n",
      "Epoch 958/1000\n",
      "265/265 [==============================] - 0s 200us/sample - loss: 0.1712 - mae: 0.2920 - mse: 0.1712 - val_loss: 0.2842 - val_mae: 0.4082 - val_mse: 0.2842\n",
      "Epoch 959/1000\n",
      "265/265 [==============================] - 0s 204us/sample - loss: 0.1742 - mae: 0.3148 - mse: 0.1742 - val_loss: 0.1889 - val_mae: 0.3350 - val_mse: 0.1889\n",
      "Epoch 960/1000\n",
      "265/265 [==============================] - 0s 325us/sample - loss: 0.1646 - mae: 0.2939 - mse: 0.1646 - val_loss: 0.1867 - val_mae: 0.3309 - val_mse: 0.1867\n",
      "Epoch 961/1000\n",
      "265/265 [==============================] - 0s 248us/sample - loss: 0.1758 - mae: 0.3056 - mse: 0.1758 - val_loss: 0.2141 - val_mae: 0.3464 - val_mse: 0.2141\n",
      "Epoch 962/1000\n",
      "265/265 [==============================] - 0s 234us/sample - loss: 0.1619 - mae: 0.2839 - mse: 0.1619 - val_loss: 0.1926 - val_mae: 0.3286 - val_mse: 0.1926\n",
      "Epoch 963/1000\n",
      "265/265 [==============================] - 0s 247us/sample - loss: 0.1595 - mae: 0.2856 - mse: 0.1595 - val_loss: 0.1948 - val_mae: 0.3316 - val_mse: 0.1948\n",
      "Epoch 964/1000\n",
      "265/265 [==============================] - 0s 232us/sample - loss: 0.1739 - mae: 0.3098 - mse: 0.1739 - val_loss: 0.1908 - val_mae: 0.3284 - val_mse: 0.1908\n",
      "Epoch 965/1000\n",
      "265/265 [==============================] - 0s 199us/sample - loss: 0.1600 - mae: 0.2825 - mse: 0.1600 - val_loss: 0.2117 - val_mae: 0.3432 - val_mse: 0.2117\n",
      "Epoch 966/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.1609 - mae: 0.2892 - mse: 0.1609 - val_loss: 0.1976 - val_mae: 0.3362 - val_mse: 0.1976\n",
      "Epoch 967/1000\n",
      "265/265 [==============================] - 0s 240us/sample - loss: 0.1594 - mae: 0.2849 - mse: 0.1594 - val_loss: 0.1901 - val_mae: 0.3300 - val_mse: 0.1901\n",
      "Epoch 968/1000\n",
      "265/265 [==============================] - 0s 226us/sample - loss: 0.1569 - mae: 0.2805 - mse: 0.1569 - val_loss: 0.2052 - val_mae: 0.3391 - val_mse: 0.2052\n",
      "Epoch 969/1000\n",
      "265/265 [==============================] - 0s 275us/sample - loss: 0.1586 - mae: 0.2863 - mse: 0.1586 - val_loss: 0.2088 - val_mae: 0.3398 - val_mse: 0.2088\n",
      "Epoch 970/1000\n",
      "265/265 [==============================] - 0s 262us/sample - loss: 0.1708 - mae: 0.2991 - mse: 0.1708 - val_loss: 0.1948 - val_mae: 0.3364 - val_mse: 0.1948\n",
      "Epoch 971/1000\n",
      "265/265 [==============================] - 0s 223us/sample - loss: 0.1624 - mae: 0.2855 - mse: 0.1624 - val_loss: 0.2192 - val_mae: 0.3486 - val_mse: 0.2192\n",
      "Epoch 972/1000\n",
      "265/265 [==============================] - 0s 219us/sample - loss: 0.1651 - mae: 0.2960 - mse: 0.1651 - val_loss: 0.2284 - val_mae: 0.3559 - val_mse: 0.2284\n",
      "Epoch 973/1000\n",
      "265/265 [==============================] - 0s 217us/sample - loss: 0.1776 - mae: 0.3062 - mse: 0.1776 - val_loss: 0.2555 - val_mae: 0.3805 - val_mse: 0.2555\n",
      "Epoch 974/1000\n",
      "265/265 [==============================] - 0s 239us/sample - loss: 0.1753 - mae: 0.3021 - mse: 0.1753 - val_loss: 0.2306 - val_mae: 0.3588 - val_mse: 0.2306\n",
      "Epoch 975/1000\n",
      "265/265 [==============================] - 0s 201us/sample - loss: 0.1734 - mae: 0.3056 - mse: 0.1734 - val_loss: 0.2396 - val_mae: 0.3648 - val_mse: 0.2396\n",
      "Epoch 976/1000\n",
      "265/265 [==============================] - 0s 183us/sample - loss: 0.1764 - mae: 0.2963 - mse: 0.1764 - val_loss: 0.2100 - val_mae: 0.3403 - val_mse: 0.2100\n",
      "Epoch 977/1000\n",
      "265/265 [==============================] - 0s 199us/sample - loss: 0.1661 - mae: 0.2991 - mse: 0.1661 - val_loss: 0.1868 - val_mae: 0.3247 - val_mse: 0.1868\n",
      "Epoch 978/1000\n",
      "265/265 [==============================] - 0s 208us/sample - loss: 0.1702 - mae: 0.3013 - mse: 0.1702 - val_loss: 0.1833 - val_mae: 0.3295 - val_mse: 0.1833\n",
      "Epoch 979/1000\n",
      "265/265 [==============================] - 0s 205us/sample - loss: 0.1610 - mae: 0.2906 - mse: 0.1610 - val_loss: 0.1823 - val_mae: 0.3259 - val_mse: 0.1823\n",
      "Epoch 980/1000\n",
      "265/265 [==============================] - 0s 232us/sample - loss: 0.1593 - mae: 0.2871 - mse: 0.1593 - val_loss: 0.1983 - val_mae: 0.3332 - val_mse: 0.1983\n",
      "Epoch 981/1000\n",
      "265/265 [==============================] - 0s 189us/sample - loss: 0.1574 - mae: 0.2816 - mse: 0.1574 - val_loss: 0.1825 - val_mae: 0.3264 - val_mse: 0.1825\n",
      "Epoch 982/1000\n",
      "265/265 [==============================] - 0s 227us/sample - loss: 0.1574 - mae: 0.2798 - mse: 0.1574 - val_loss: 0.2295 - val_mae: 0.3593 - val_mse: 0.2295\n",
      "Epoch 983/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.1604 - mae: 0.2920 - mse: 0.1604 - val_loss: 0.1918 - val_mae: 0.3270 - val_mse: 0.1918\n",
      "Epoch 984/1000\n",
      "265/265 [==============================] - 0s 211us/sample - loss: 0.1566 - mae: 0.2847 - mse: 0.1566 - val_loss: 0.1879 - val_mae: 0.3314 - val_mse: 0.1879\n",
      "Epoch 985/1000\n",
      "265/265 [==============================] - 0s 209us/sample - loss: 0.1638 - mae: 0.2911 - mse: 0.1638 - val_loss: 0.1871 - val_mae: 0.3289 - val_mse: 0.1871\n",
      "Epoch 986/1000\n",
      "265/265 [==============================] - 0s 190us/sample - loss: 0.1605 - mae: 0.2882 - mse: 0.1605 - val_loss: 0.1980 - val_mae: 0.3329 - val_mse: 0.1980\n",
      "Epoch 987/1000\n",
      "265/265 [==============================] - 0s 202us/sample - loss: 0.1554 - mae: 0.2809 - mse: 0.1554 - val_loss: 0.2099 - val_mae: 0.3392 - val_mse: 0.2099\n",
      "Epoch 988/1000\n",
      "265/265 [==============================] - 0s 221us/sample - loss: 0.1617 - mae: 0.2838 - mse: 0.1617 - val_loss: 0.1928 - val_mae: 0.3261 - val_mse: 0.1928\n",
      "Epoch 989/1000\n",
      "265/265 [==============================] - 0s 300us/sample - loss: 0.1564 - mae: 0.2838 - mse: 0.1564 - val_loss: 0.1851 - val_mae: 0.3226 - val_mse: 0.1851\n",
      "Epoch 990/1000\n",
      "265/265 [==============================] - 0s 207us/sample - loss: 0.1621 - mae: 0.2860 - mse: 0.1621 - val_loss: 0.2104 - val_mae: 0.3449 - val_mse: 0.2104\n",
      "Epoch 991/1000\n",
      "265/265 [==============================] - 0s 260us/sample - loss: 0.1569 - mae: 0.2840 - mse: 0.1569 - val_loss: 0.2139 - val_mae: 0.3392 - val_mse: 0.2139\n",
      "Epoch 992/1000\n",
      "265/265 [==============================] - 0s 233us/sample - loss: 0.1614 - mae: 0.2905 - mse: 0.1614 - val_loss: 0.2217 - val_mae: 0.3519 - val_mse: 0.2217\n",
      "Epoch 993/1000\n",
      "265/265 [==============================] - 0s 219us/sample - loss: 0.1755 - mae: 0.3070 - mse: 0.1755 - val_loss: 0.1835 - val_mae: 0.3243 - val_mse: 0.1835\n",
      "Epoch 994/1000\n",
      "265/265 [==============================] - 0s 240us/sample - loss: 0.1721 - mae: 0.3083 - mse: 0.1721 - val_loss: 0.1792 - val_mae: 0.3252 - val_mse: 0.1792\n",
      "Epoch 995/1000\n",
      "265/265 [==============================] - 0s 216us/sample - loss: 0.1554 - mae: 0.2823 - mse: 0.1554 - val_loss: 0.1822 - val_mae: 0.3254 - val_mse: 0.1822\n",
      "Epoch 996/1000\n",
      "265/265 [==============================] - 0s 219us/sample - loss: 0.1733 - mae: 0.3024 - mse: 0.1733 - val_loss: 0.1867 - val_mae: 0.3247 - val_mse: 0.1867\n",
      "Epoch 997/1000\n",
      "265/265 [==============================] - 0s 213us/sample - loss: 0.1607 - mae: 0.2887 - mse: 0.1607 - val_loss: 0.2273 - val_mae: 0.3495 - val_mse: 0.2273\n",
      "Epoch 998/1000\n",
      "265/265 [==============================] - 0s 240us/sample - loss: 0.1557 - mae: 0.2817 - mse: 0.1557 - val_loss: 0.1795 - val_mae: 0.3219 - val_mse: 0.1795\n",
      "Epoch 999/1000\n",
      "265/265 [==============================] - 0s 218us/sample - loss: 0.1582 - mae: 0.2895 - mse: 0.1582 - val_loss: 0.1820 - val_mae: 0.3284 - val_mse: 0.1820\n",
      "Epoch 1000/1000\n",
      "265/265 [==============================] - 0s 198us/sample - loss: 0.1639 - mae: 0.2962 - mse: 0.1639 - val_loss: 0.1785 - val_mae: 0.3278 - val_mse: 0.1785\n"
     ]
    }
   ],
   "source": [
    "# Train the constructed model-\n",
    "# Train the model for 1000 epochs, and record the training and validation\n",
    "# accuracy in the 'history' object\n",
    "'''\n",
    "history = nn_model.fit(X_train_n, y_train.values, epochs=1000,\n",
    "        validation_data=(X_test_n, y_test.values), verbose=1,\n",
    "        callbacks = [early_stop])\n",
    "'''\n",
    "history = nn_model.fit(X_train_n, y_train.values, epochs=1000,\n",
    "        validation_data=(X_test_n, y_test.values), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the model's training progress using the stats stored in the\n",
    "# 'history' object-\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df['epoch'] = history.epoch\n",
    "\n",
    "history_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>448.352990</td>\n",
       "      <td>21.131908</td>\n",
       "      <td>448.352966</td>\n",
       "      <td>360.308485</td>\n",
       "      <td>18.962971</td>\n",
       "      <td>360.308502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304.010616</td>\n",
       "      <td>17.391531</td>\n",
       "      <td>304.010651</td>\n",
       "      <td>238.070124</td>\n",
       "      <td>15.407601</td>\n",
       "      <td>238.070114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>196.817112</td>\n",
       "      <td>13.974322</td>\n",
       "      <td>196.817108</td>\n",
       "      <td>143.641853</td>\n",
       "      <td>11.955234</td>\n",
       "      <td>143.641846</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108.279444</td>\n",
       "      <td>10.319079</td>\n",
       "      <td>108.279434</td>\n",
       "      <td>65.826797</td>\n",
       "      <td>8.064235</td>\n",
       "      <td>65.826797</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45.081277</td>\n",
       "      <td>6.582199</td>\n",
       "      <td>45.081276</td>\n",
       "      <td>22.040845</td>\n",
       "      <td>4.597708</td>\n",
       "      <td>22.040844</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.374630</td>\n",
       "      <td>3.446508</td>\n",
       "      <td>13.374631</td>\n",
       "      <td>4.785544</td>\n",
       "      <td>1.975416</td>\n",
       "      <td>4.785544</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.727692</td>\n",
       "      <td>1.452167</td>\n",
       "      <td>2.727692</td>\n",
       "      <td>1.134547</td>\n",
       "      <td>0.893224</td>\n",
       "      <td>1.134547</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.165444</td>\n",
       "      <td>0.837314</td>\n",
       "      <td>1.165444</td>\n",
       "      <td>1.317125</td>\n",
       "      <td>0.880360</td>\n",
       "      <td>1.317125</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.423933</td>\n",
       "      <td>0.877544</td>\n",
       "      <td>1.423933</td>\n",
       "      <td>1.397820</td>\n",
       "      <td>0.896355</td>\n",
       "      <td>1.397820</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.342407</td>\n",
       "      <td>0.843817</td>\n",
       "      <td>1.342407</td>\n",
       "      <td>1.144751</td>\n",
       "      <td>0.814526</td>\n",
       "      <td>1.144751</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss        mae         mse    val_loss    val_mae     val_mse  epoch\n",
       "0  448.352990  21.131908  448.352966  360.308485  18.962971  360.308502      0\n",
       "1  304.010616  17.391531  304.010651  238.070124  15.407601  238.070114      1\n",
       "2  196.817112  13.974322  196.817108  143.641853  11.955234  143.641846      2\n",
       "3  108.279444  10.319079  108.279434   65.826797   8.064235   65.826797      3\n",
       "4   45.081277   6.582199   45.081276   22.040845   4.597708   22.040844      4\n",
       "5   13.374630   3.446508   13.374631    4.785544   1.975416    4.785544      5\n",
       "6    2.727692   1.452167    2.727692    1.134547   0.893224    1.134547      6\n",
       "7    1.165444   0.837314    1.165444    1.317125   0.880360    1.317125      7\n",
       "8    1.423933   0.877544    1.423933    1.397820   0.896355    1.397820      8\n",
       "9    1.342407   0.843817    1.342407    1.144751   0.814526    1.144751      9"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print first 10 rows-\n",
    "history_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x960 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training and testing metrics-\n",
    "\n",
    "fig=plt.figure(figsize=(15, 12), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# For Mean Absolute Error (MAE)-\n",
    "plt.figure()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Absolute Error\")\n",
    "\n",
    "plt.plot(history_df['epoch'], history_df['mae'], label = 'Train Error MAE')\n",
    "plt.plot(history_df['epoch'], history_df['val_mae'], label = 'Test Error MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For Mean Squared Error (MSE)-\n",
    "plt.figure()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "plt.plot(history_df['epoch'], history_df['mse'], label = 'Train Error MSE')\n",
    "plt.plot(history_df['epoch'], history_df['val_mse'], label = 'Test Error MSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model metrics on testing set are:\n",
      "MAE = 0.3278, MSE = 0.1785 & R2-score = 0.8218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using trained model-\n",
    "y_pred = nn_model.predict(X_test_n)\n",
    "\n",
    "mae_model = mean_absolute_error(y_test, y_pred)\n",
    "mse_model = mean_squared_error(y_test, y_pred)\n",
    "r2_score_model = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nModel metrics on testing set are:\")\n",
    "print(\"MAE = {0:.4f}, MSE = {1:.4f} & R2-score = {2:.4f}\\n\".format(mae_model, mse_model, r2_score_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/1 [========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 92us/sample - loss: 0.1338 - mae: 0.3278 - mse: 0.1785\n",
      "\n",
      "Testing set metrics are: \n",
      "loss (MSE) = 0.178525, MAE = 0.3278 & MSE = 0.1785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate trained model-\n",
    "loss, mae, mse = nn_model.evaluate(X_test_n, y_test.values, verbose=1)\n",
    "\n",
    "print(\"\\nTesting set metrics are: \")\n",
    "print(\"loss (MSE) = {0:4f}, MAE = {1:.4f} & MSE = {2:.4f}\\n\".format(loss, mae, mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
